\chapter{Tools\clabel{Tools}}

%\section{Summary}
%\seclabel{Summary}
{\it
The ergodicity question  -- whether time averages are identical to expectation values -- is the key to our redevelopment of formal economics. This is because ergodicity hadn't been established as a concept when the original formalism was developed. The scientific search for stable structures leads to constants in deterministic settings. When randomness is introduced, the role previously played by constants is taken on by ergodic observables. We also introduce the concepts of a random variable, a stochastic process, scalars as representations of transitive preferences, logarithms and exponentials, and dimensional analysis.

In \secref{Brownian_motion} we notice that wealth on logarithmic scales follows a random walk in our game, and we relate this to Brownian motion, as the continuous-time limit of the random walk. This allows us to introduce Brownian motion and its scaling properties that are robust enough to yield insights into more complicated models.

Finally, we ask in \secref{Geometric_Brownian} what wealth in our game is doing in the continuum limit but on linear scales. This takes us to geometric Brownian motion, which will be our starting point for much of the rest of these lectures. We derive ensemble-average and time-average growth rates for geometric Brownian motion, by explicitly taking the continuous-time limit, and then state the key result of \Ito calculus, \eref{Ito_process} and \eref{Ito}, which allows an easier derivation of these growth rates and will be relied on in later chapters.

Some historical perspective is provided to understand the prevalence or absence of key concepts in modern economic theory and other fields. The emphasis is on introducing concepts and useful machinery, with applications in later chapters.
}
\newpage

\section{Random variable}
\seclabel{random_variable}
In economics, as elsewhere, we are often interested in `experiments' whose outcomes we don't yet know. Examples are each coin toss in the game in \secref{The_game} or the result of a football match. We might know something about the experiment, such as the possible outcomes and that some are more plausible than others, but we are ignorant of the actual outcome. Luckily, we can use probability theory, a branch of mathematics, to build models of our ignorance.

Often the experimental outcomes have, or can be mapped to, numerical values. For example, each coin toss in the game has possible outcomes heads and tails, which correspond to wealth multipliers 1.5 and 0.6. In such cases, we model the uncertain numerical value as a \textit{random variable}. We assume you have seen random variables before and we will not give a lengthy technical account. Instead, we recommend the two-page discussion in \cite[p.~2]{vanKampen1992}, whose key points we reproduce here.

A random variable, $\Z$, is defined by:
\bi
\item the set of its possible values, $\{\z_\gj\}$; and
\item a probability distribution, $\prob{}$, over this set.
\ei
The set of possible values of $\Z$ may be continuous, like the interval $(3,12)$ or the real numbers, 
$\Rreal$; discrete, like $\{4, 7.8, 29\}$ or the integers, $\Zint$; or a combination 
of the two. The probability distribution is a function which maps values to probabilities. We define an 
event as ``$\Z$ taking the value $\z$,'' which we write as $\Z=\z$. This is strictly an abuse
of notation because $\Z$ is a random variable, and $\z$ is a scalar (so they can't be identical as 
the ``$=$'' sign suggests). But this is common notation, and we will use it here. Similarly, we may write $\Z<a$
to denote the union of all events where $\Z$ takes a value $\z<a$ \etc
In terms of probabilities, we write
\be
\prob{\Z=\z}=\p
\ee
to mean that the event $\Z=\z$ is associated with the probability $\p$. A specific value, $\z$, 
is sometimes called an `outcome', an `instance' or a `realisation' of the random variable, $\Z$. 
While not obligatory, it is a common convention to denote random variables in upper case and 
realisations in lower case. Later in these lecture notes, where this won't lead to confusion, we will 
often overload notation and use lower case letters for both random variables and realisations. 
This is a little less precise but easier on the eye. 

Forget, for the moment, what probabilities might mean in the context of an experiment. In purely 
mathematical terms, they are just real numbers associated with outcomes. The probability 
distribution has two constraints:
\bi
\item the probability of any outcome must be non-negative; and
\item the probability of an outcome that is certain to happen, \ie that includes all possible outcomes, must be one.
\ei
The latter is a normalisation condition which fixes the scale of the probabilities.

\boldhead{Discrete random variable}
When the set of outcomes is discrete, say $\{\z_1,\dots,\z_M\}$, we assign a probability, 
$\p_\gj$, to each outcome, $\z_\gj$, such that
\be
\prob{\Z=\z} = \begin{cases}
\p_\gj & \text{if } \z=\z_\gj\\
0 & \text{otherwise}
\end{cases}
\ee
and
\be
\sum_{\gj=1}^M\p_\gj = 1.
\ee

\boldhead{Continuous random variable}
Most of the models we will study contain random variables whose possible outcomes form a 
continuous set. In this case, $\Z$ can take uncountably many values, to which we can't assign 
non-zero probabilities while maintaining the normalisation condition. Instead, we assign 
probabilities to intervals. We define a \PDFa, $\PDF_{\Z}(\z)$, such that the probability of $\Z$ 
being in the interval $(a,b)$ is given by
\be
\prob{a \leq \Z \leq b} = \int_a^b \PDF_{\Z}(\z)\gd\z.
\ee
The \PDFa is a non-negative function, $\PDF_{\Z}(\z) \geq 0$, normalised so that the probability of 
the certain outcome, \ie the integral over all possible outcomes, is one:
\be
\int_{-\infty}^{+\infty} \PDF_{\Z}(\z)\gd\z = 1.
\ee
Note the difference between subscript and argument: $\PDF_{\Z}(\z)$ is the probability density of 
the random variable $\Z$ at value $\z$. You might find it helpful to think of $\PDF_{\Z}(\z)\delta\z$ 
as the approximate probability of $\Z$ being in the small interval $(\z,\z+\d\z)$ close to $\z$. 

\boldhead{Interpretation}
So far we have said nothing about the meaning of probabilities: formally, they are simply numbers 
assigned to outcomes of random variables. Giving these numbers meaning is a separate step. 

In these notes, we will use a frequentist interpretation of probabilities, by which we mean the following. 
Imagine many separate experiments. For example, suppose we toss a coin $\N$ times, 
and record the number of times, $\gn$, that a particular outcome occurs. Under the frequentist 
interpretation, the probability associated with this outcome is its relative frequency, $\gn/\N$, in the limit 
$\N\to\infty$. In the coin toss example, the probability assigned to heads would be 0.5 if the coin were 
unbiased; if biased, it would be some other number between 0 and 1.

Note also that time does not appear in the formal definition of a random variable. The moment we
try to illustrate what a random variable might model, we introduce time. For instance by saying ``we toss
a coin, and we model its {\it future} state (heads or tails) as a random variable.'' But the mathematical
object that is the random variable doesn't know about this interpretation. It just knows about possible
values, associated with numbers between zero and one.

Nor do random variables usually depend on time. Of course, there is nothing stopping us from using a 
probability distribution which does depend on time or, indeed, any other variable, like the day of the 
week or the country we are in. Such parameterisations of the random variable do not change 
fundamentally its mathematical structure: a set of outcomes and associated probabilities. When we 
consider probability distributions of random variables that do depend on time, as we do for wealth, $\x(\t)$, 
in the coin tossing game, we will make the time dependence explicit. By default we assume random 
variables are time-independent.

\section{Expectation value}
The \textit{expectation value}\footnote{Also known as expected value, mathematical expectation, 
first moment, and mean.} is a property of a random variable. Denoted by $\eval{\Z}$, it is the 
probability-weighted average of the realisations of $\Z$.

\begin{defn}{Expectation value}
If $\Z$ is a discrete random variable, its expectation value is the sum of the possible realisations, 
$\z_\gj$, weighted by their probabilities, $\p_\gj$:
\be
\eval{\Z}=\sum_\gj \p_\gj \z_\gj.
\elabel{exp_sum}
\ee 
If $\Z$ is a continuous random variable, its expectation value is the integral over the possible 
realisations weighted by the probability density:
\be
\eval{\Z}=\int_{-\infty}^{+\infty} \PDF_{\Z}(\z) \z \gd\z.
\ee 
\end{defn}
The sum or the integral may not exist, in which case the random variable does not have an 
expectation value. One example of this is the payout of the St Petersburg lottery, which 
we will encounter in \secref{SPP}. Another example is any fat-tailed random variable with 
\PDFa of the form:
\be
\PDF_{\Z}(\z) = \begin{cases}
(\tau-1)\z^{-\tau} & \z\geq1 \\
0 & \z<1
\end{cases}
\ee
for $\tau \leq 2$

\section{Ensemble average}
\seclabel{ensemble_average}
The \textit{ensemble average} is conceptually different from the expectation value, although we will see that it always is identical in value. Instead of weighting the average of the possible 
realisations of $\Z$ by their probabilities, we take an unweighted average of a collection of 
generated realisations. For a finite number of realisations, we call this the \textit{\FEA} and denote it by $\ave{\Z}_\N$. 
\begin{defn}{Finite ensemble average}
The \FEA of a random variable, $\Z$, is the average over a finite number, 
$\N$, of generated realisations,
\be
\ave{\Z}_{\N} \equiv \frac{1}{\N}\sum_{\gi=1}^{\N} \z_{\gi},
\elabel{f_ens}
\ee 
where $\z_\gi$ denotes the $\gi^\text{th}$ realisation of $\Z$.
\end{defn}
You may know this as the sample average or sample mean. Note that it is an additive average, 
meaning that $\ave{\Z}_{\N}$ is the value by which each of the $\N$ realisations of $\Z$ must 
be replaced if we want them all to be equal without altering their \textit{sum}.

The \FEA is itself a random variable. Each finite ensemble of size $\N$ can contain different realisations of $\Z$ which sum to a different total. As $\N\to\infty$, this random average may converge with probability one to a unique constant. If it does, we call this limiting value the ensemble average and denote it by $\ave{\Z}$.
\begin{defn}{Ensemble average}
The ensemble average of a random variable, $\Z$, is the $\N\to\infty$ limit of the \FEA,
\be
\ave{\Z} \equiv \lim_{\N\to\infty}\ave{\Z}_{\N} =  \lim_{\N\to\infty} \frac{1}{\N} \sum_{\gi=1}^\N \z_\gi,
\elabel{ens}
\ee
where $\z_\gi$ denotes the $\gi^\text{th}$ realisation of $\Z$.
\end{defn}
The limit is not guaranteed to exist (in which case $\Z$ has no ensemble average) but 
\FEAs can always be computed. We will sometimes refer to the imagined 
infinite collection of realisations as the ``ensemble'' of $\Z$.

Incidentally, if $\ave{\Z}$ is divergent, then 
$\ave{\Z}_\N$ will grow systematically with $\N$. If $\ave{\Z}$ represents an average
taken over $\N$ successive measurements (a time series), then a fat-tailed $\Z$ is often
as good a model as a non-stationary (time-dependent) $\Z$.

Note that, unlike the expectation value, the ensemble average of a random variable is not defined directly 
in terms of probabilities. It is the quantity to which averages over realisations converge as the number of 
realisations grows. The connection to probabilities hides in the procedure of generating realisations. 

Recall how we analysed the coin game. At each round of the gamble, we found \FEAs of 
simulated wealth for increasingly large samples, from one to one million, plotted against time in \fref{1_2}.
It is laborious to compute averages of ever larger finite ensembles, in the hope of discerning their 
convergence to a limit. Fortunately, we can show, under the frequentist interpretation of probability 
introduced in \secref{random_variable}, that the ensemble average of a random variable is equal 
to its expectation value. We do this here for the discrete case, noting that a similar proof can be 
offered for the continuous case.
\begin{proof}
Denote by  $\gn_\gj$ the number of times $\z_\gj$ is observed in $\N$ realisations of $\Z$. The \FEA can be written as
\be
\ave{\Z}_\N =\frac{1}{\N}\sum_{\gi=1}^{\N}  \z_\gi = \sum_\gj \frac{\gn_\gj}{\N} \z_\gj,
\ee
where the subscript $\gi$ indexes a particular realisation of $\z$ and the subscript $\gj$ indexes a possible value of $\z$. 
Circularly -- because we work with frequentist probabilities -- the relative frequency of each possible value, 
$\gn_\gj/\N$, converges almost surely in the limit $\N\to\infty$ to its (frequentist) probability, $\p_\gj$. Thus, we find
\be
\lim_{\N\to\infty}\ave{\Z}_\N = \sum_\gj \p_\gj \z_\gj = \eval{\Z}.
\ee
\end{proof}
This result, commonly known as the \textit{law of large numbers}, is exceedingly useful. It means that we no longer need many realisations of a random variable to compute its ensemble average. Instead, if we know the probability distribution, we can compute its expectation value straightforwardly as a weighted average or integral. It also means that, from now on, we can use ensemble average and expectation value interchangeably.

\begin{history}{The invention of the expectation value}
Coming soon.
%Expectation values 
%were not invented in order to assess whether a gamble is 
%worth taking. Instead, they were developed to settle a  
%moral question that arises in the following somewhat contrived 
%context: imagine playing a game of dice with a 
%group of gamblers. The rules of the game are simple: we 
%roll the dice three times,  and whoever rolls the most points 
%gets the pot to which we've all contributed equal amounts. 
%We've already rolled the dice twice when suddenly the 
%police burst in because they've heard of our illegal gambling ring. 
%We all avoid arrest, most of us escape through the backdoor, 
%and to everyone's great relief you had the presence of mind 
%to grab the pot before jumping out of a conveniently located 
%ground-floor window. Later that day, under the cover of dusk, 
%we meet behind the old oak tree just outside of town to split 
%the pot in a fair way. But hold on -- what does ``fair'' mean here?
%Some of us had acquired more points than others in the first 
%two rolls of the dice. Shouldn't they get more? The game was 
%not concluded, so wouldn't it be fair to return to everyone his 
%wager and thank our lucky stars that we weren't arrested? 
%Should we split the pot in proportion to each player's points? 
%All of these solutions were proposed \cite{Devlin2008}.
%The question is fundamentally moral, and there is no 
%mathematical answer. But \person{Blaise Pascal}, now famous for 
%addressing theological questions using expectation values, put the 
%problem to Pierre de Fermat, and over the course of a few months' 
%correspondence (the two never met in person) \person{Fermat} and 
%\person{Pascal} agreed that fairness is achieved as follows: 
%think of all (equally likely) possible outcomes of the third round 
%of throws of the dice, call the number of all possibilities $\N$. 
%Now count those possibilities that result in player $\gj$ winning, 
%call this $\gn_\gj$. If $\q$ is the amount of money in the pot, then 
%we split the pot fairly by giving each player
% $\frac{\gn_\gj}{\N}\times \q$.
%This is $\ave{\q}$, according to \eref{exp_sum}, 
%because $\frac{\gn_\gj}{\N}=\p_\gj$ is the probability that player $\gj$ wins the
%amount \q. 
%Later researchers called this amount the ``mathematical expectation''  
%or simply ``expectation value''. But this is really an unfortunate choice 
% -- no player ``expected'' to receive $\ave{\q}$. 
%Instead, each player expected to receive either nothing or $\q$. 
\end{history}

\section{Stochastic process}
% Start by illustrating with the coin toss game
In the coin game, the wealth multiplier, $\gr$, at each round is a random variable with outcomes $\{0.6,1.5\}$ and probabilities $\{0.5,0.5\}$. Starting from $x(0)=\$1$, successive realisations of the wealth updating rule,
\be
\x(\t+\dt) = \x(\t)\gr,
\elabel{x_update}
\ee
generate a function of time, $\x(\t)$, which describes one possible evolution of wealth in the game. If we start again, generating fresh realisations of $\gr$ at each time step, we get a second function of time, almost certain to be different from the first. If run the game $\N$ times, we get a set of wealth evolution functions, $\{\x_1(\t),\dots,\x_{\N}(\t)\}$.

%SP as generalisation of RV
This situation reminds us of a random variable, except that each realisation is now a function 
($\x$) of a parameter ($\t$). Each $\x_\gj(\t)$ is an ordered set of real numbers (one for each 
$\t$), rather than a single real number. We call such functions \textit{trajectories}. Many natural 
phenomena are like the coin game, in that they result not in single observations but in ordered 
sequences of connected observations, about whose values we are uncertain. To model such 
phenomena, we need a new type of mathematical object, the \textit{stochastic process}, 
denoted $\Y(\t)$. We define this analogously to the random variable, as:
\bi
\item a set of possible trajectories, $\{\y(\t)\}$; and
\item a probability distribution over this set.
\ei
More technical presentations are available and, again, we point you to \cite[p.~52]{vanKampen1992} 
for greater depth. The set of trajectories may be countable and associated with a discrete 
set of probabilities, or uncountable and associated with a \PDFa. 
%For simplicity, when we 
%index trajectories, $y_\gi(\t)$, we will do so using integers $\gi=1,2,\dots$, even though 
%strictly this is incompatible with the uncountable case. 
We will always interpret 
the parameter, $\t$, as time.

This might seem like lot to take in, but conceptually it's fairly simple. A stochastic process is nothing more than a family of trajectories from which we can select at random, according to a probability distribution. It is the generalisation of the random variable from single numbers to functions of time. If you are already familiar with stochastic processes, this may not be how you think of them. We illustrate our perspective in \fref{sp_grid}.
\begin{figure}[h]
\begin{picture}(200,220)(20,30)
\put(-30,-20){\includegraphics[width=1.2\textwidth]{./chapter_tools/figs/sp_grid.pdf}}
\put(18,195){$y_1(\t)$}
\put(18,164){$y_2(\t)$}
\put(18,133){$y_3(\t)$}
\put(18,102){$y_4(\t)$}
\put(18,71){$y_5(\t)$}
\put(-10,159){\rotatebox{-90}{realisation, $\gi$}}  
\put(200,235){time, $\t$}  
\put(120,220){$y_1(\t^*)$}
\put(130,215){\vector(1,-1){13}}
\end{picture}
\caption{Realisations of a stochastic process, $\Y(\t)$. Each trajectory, $\y_\gi(\t)$ is a function of time, which runs horizontally across the page. Fixing time, $\t=\t^*$, results in a random variable, $\Y(\t^*)$, one realisation of which, $y_1(\t^*)$, is shown.}
\flabel{sp_grid}
\end{figure}

This picture suggests another way of viewing the stochastic process: if we choose a time, $\t=\t^*$, then $\Y(\t^*)$ is a random variable, whose possible realisations form the set of possible function values, $\{\y(\t^*)\}$. So, if you prefer, you can think of a stochastic process as a family of random variables, with probability distributions parametrised by time.

\boldhead{Generating stochastic processes}
Switching to practical considerations, there are two main ways of generating a trajectory of a stochastic processes, say on a computer. The first is consistent with our conceptualisation of selecting one from a family of trajectories. For example, if the possible trajectories were finitely many and equally probable, then we could label them with integers and draw one from a discrete uniform distribution. All the uncertainty is resolved at the start, in a single realisation of a random variable.

The other approach is to generate a trajectory sequentially, by successive application of the rule that tells us which possible values of $\y_\gi(\t+\dt)$ are compatible with a known value of $\y_\gi(\t)$. This is how we generate wealth trajectories in the coin game, where the rule is to multiply current wealth by the realisation of a random variable. Realising a random variable at every time step sounds laborious. However, in many models of natural phenomena, it is easier to write down a realistic updating rule than an expression for all possible trajectories. This is because it is often about the changes of quantities that we can reason `physically'. Moreover, many stochastic processes have uncountably many trajectories, making the single realisation approach difficult, because it requires high numerical precision, \ie very long numbers, to be useful.

Therefore, typically we generate trajectories sequentially, using updating rules which we call the \textit{dynamics} of the stochastic process. Usually, where it creates no ambiguity, we don't distinguish between the stochastic process and a single realisation of it, using lower case to denote both, \eg $\x(\t)$ for wealth. When it's important to identify different trajectories, we include an index, \eg $\x_\gi(\t)$. Sometimes we refer to these different realisations as \textit{systems} and to the index as the system number.

\section{Time average}
Stochastic processes are models of uncertain quantities which vary across systems and over time. 
One example is the wealth of a player of the coin game; another example is the total goals 
scored in a football match. If we fix time in a stochastic process, then we can take the ensemble average of the resulting random variable,
\be
\ave{\Y(\t^*)} = \lim_{\N\to\infty} \frac{1}{\N} \sum_{\gi=1}^{\N} \y_\gi(\t^*).
\ee
The introduction of time in the stochastic process makes another type of average possible, conceptually different from the ensemble average. Instead of choosing a time and averaging across  systems, we can choose a system and average over time. Let's start by doing this for a finite time period, $\Dt$, to generate a \textit{finite-time average}.
\begin{defn}{Finite-time average}
The finite-time average of the trajectory $\y(\t)$ is the average value of the function from $\t$ to $\t+\Dt$,
\be
\bar{\y}_{\Dt}(\t) \equiv \frac{1}{\Dt} \int_{\t}^{\t+\Dt} \y(\gs) \gd\gs.
\elabel{t_ave_f}
\ee
If $\y(\t)$ changes only at discrete times, $\{\t+\dt,\dots,\t+\T\dt\}$, where $\Dt=\T\dt$, then the finite-time average can be written as 
\be
\bar{\y}_{\Dt}(\t)  = \frac{1}{\T\dt} \sum_{\gtau=1}^{\T} \y(\t+\gtau\dt).
\elabel{t_ave_f_disc}
\ee
\end{defn}

In the same way that the ensemble average is the many-systems limit of the \FEA, so the \textit{time average} is the long-time limit of the finite-time average.
\begin{defn}{Time average}
The time average is the long-time limit of the finite-time average,
\be
\bar{\y}(\t) \equiv \lim_{\Dt\to\infty} \bar{\y}_{\Dt}(\t),
\elabel{t_ave}
\ee
where this limit exists.
\end{defn}
Note that both the finite-time average and the time average are strictly functions of the time, 
$\t$, at which the averaging starts. In many of the models we consider, dependence on the 
initial condition vanishes in the limit of long averaging time. If so, we remove it from the 
notation and write $\bar{\y}$.

The two types of average possible in a stochastic process are illustrated in \fref{ergodic_grid}.
\begin{figure}[h]
\begin{picture}(200,260)(0,0)
\put(-40,-30){\includegraphics[width=1.2\textwidth]{./chapter_tools/figs/ergodic_grid.pdf}}
\put(13,190){$\y_1(\t)$}
\put(13,159){$\y_2(\t)$}
\put(13,128){$\y_3(\t)$}
\put(13,97){$\y_4(\t)$}
\put(13,66){$\y_5(\t)$}
\put(-25,22){realisation, $\gi$}  
\put(162,242){$\t=\t^*$}  
\put(344,235){time, $\t$}  
\put(188,207){$\y_1(\t^*)$}
\put(187,208){\vector(-1,-1){15}}
\put(-27,125){$\gi=3$}
\put(362,127){$\bar{\y_3} =$}
\put(340,114){$\lim_{\Dt\to\infty} \frac{1}{\Dt} \int_0^{\Dt} \y_3(\gs) \gd\gs$}
\put(100,10){$\ave{\Y(\t^*)} = \lim_{\N\to\infty} \frac{1}{\N} \sum_{\gi=1}^\N \y_\gi(\t^*)$}  
\end{picture}
\caption{Realisations of a stochastic process, $\Y(\t)$, \cf \fref{sp_grid}. The ensemble average, $\ave{\Y(\t^*)}$, is obtained by choosing a time, $\t=\t^*$, and averaging over realisations (red box). The time average is obtained by choosing a trajectory, $\gi=3$, and averaging over time (green box).}
\flabel{ergodic_grid}
\end{figure}

\section{The game, revisited}
\seclabel{game_revisited}
We have introduced some essential tools for studying models such as that of the coin game, where outcomes are uncertain and vary over time. It makes sense, therefore, to apply these tools to the game to see how they deepen our understanding of it.

We pretended to be mathematically clueless when we ran the simulations in \secref{The_game}, hoping to understand the game simply by looking at many possible outcomes. We can now compute exactly the ensemble average of the stochastic process for wealth, $\x(\t)$, instead of approximating it numerically by a \FEA. We start by taking the ensemble average of the wealth updating equation, \eref{x_update},
\be
\ave{\x(\t+\dt)} = \ave{\x(\t)\gr}.
\elabel{step_1}
\ee
We assume the outcomes of coin tosses are independent of the player's wealth and of time, so that $\ave{\x(\t)\gr} = \ave{\x(\t)}\ave{\gr}$. Thus, \eref{step_1} can be written as
\be
\ave{\x(\t+\dt)}=\ave{\x(\t)}\ave{\gr}.
\ee
We can solve this recursively for the wealth after $\T$ rounds from a known starting wealth, $\x(\t_0)$:
\be
\ave{\x(\t_0+\T\dt)} = \x(\t_0)\ave{\gr}^\T.
\elabel{x_exp_r}
\ee
$\dt$ is the duration of a single round of the game, so the total playing time is $\Dt=\T\dt$.

Recall that $\gr$ is random variable with outcomes $\{0.6,1.5\}$ and probabilities $\{0.5,0.5\}$. The expectation value, $\ave{\gr}$, is, therefore,
\be
\ave{\gr} = (0.5)(0.6) + (0.5)(1.5) = 1.05.
\ee
Since this number is greater than one, the ensemble average wealth, $\ave{\x(\t)}$, grows exponentially over time by a factor 1.05 per round of the game.

It is useful to quantify this growth in a standard way, as a growth rate. We will say much more about this in \secref{Growth_rates}, since it is a central concept in ergodicity economics. For the moment, it is enough to know that the appropriate growth rate for a quantity which grows multiplicatively is the coefficient of time, $\g$, in the exponential function, $\exp(\g\t)$.

We write
\be
\ave{\x(\t_0+\Dt)} = \x(\t_0) \exp(\ggave\Dt)
\elabel{x_exp_g}
\ee
and equate it to $\x(\t_0)\ave{\gr}^\T$ in \eref{x_exp_r} to get the exponential growth rate,
\be
\ggave = \frac{\T\ln\ave{\gr}}{\Dt} = \frac{\ln\ave{\gr}}{\dt}.
\ee
We aren't interested here in whether time is measured in minutes, days, or years, so for simplicity let's just measure it in rounds of the game, \ie $\dt=1\text{ round}$. This choice gives us a growth rate of $\ggave = \ln(1.05)/1 \approx 4.9\%$ per round.

The positive growth rate of ensemble average wealth is what might have led us to conclude that the game is worth playing, when we analysed it numerically in \secref{The_game}. \Fref{cf_exp} compares the analytical result for the ensemble, derived above, to the numerical results of \fref{1_2} for finite ensembles. We could now conclude that the case is closed. Mathematics and simulations agree there is some risk involved, but ``on average'' our wealth grows if we play the game.
\begin{figure}[h]
\begin{picture}(200,200)(0,0)
\put(-100,0){\includegraphics[width=0.8\textwidth]{./chapter_tools/figs/x_of_t_lin_exp.pdf}}
\put(180,0){\includegraphics[width=0.8\textwidth]{./chapter_tools/figs/x_of_t_log_exp.pdf}}
\put(35,160){(A)}
\put(315,160){(B)}  
\end{picture}
\caption{Expectation value (thick light blue line) and \FEAs on (A) linear scales and  (B) logarithmic scales.}
\flabel{cf_exp}
\end{figure}
%In this section we validate \fref{1_3} and compute analytically what happens in the long-time limit. The blue line in \fref{1_3} is not completely smooth, there's still some noise (see panel B). It has some average slope, but that slope will vary from realisation to realisation. The longer we observe the system, \ie the more time is represented in a figure like \fref{1_3}, the smoother the line will be. In the long-time limit, $\Dt\to\infty$, the line will be completely smooth, and the average slope will be a deterministic number -- in any realisation of the process it will come out identical. 

However, we now know that there are two ways in which we can say ``on average'': we have shown how wealth averaged over many systems evolves; let's now consider a single wealth trajectory over long time. Starting from $\x(\t_0)$ and playing for $\T$ rounds leads to wealth,
\be
\x(\t_0+\T\dt) = \x(\t_0) \prod_{\gtau=1}^\T \gr_\gtau,
\elabel{x_prod}
\ee
where $\{\gr_\gtau\}$ are random wealth multipliers, all distributed like $\gr$, and $\gtau$ is the index for the round of the game. We have taken no kind of average yet and $\x(\t_0+\T\dt)$ remains a random variable (it is, after all, the outcome of a stochastic process at a specified time).

We follow our previous strategy of expressing this as exponential growth
\be
\x(\t_0+\Dt) = \x(\t_0) \exp(\g_\T\Dt),
\elabel{x_exp}
\ee
where we have labelled the growth rate as $\g_\T$ to indicate that it is measured after $\T$ rounds of the gamble. Equating \eref{x_prod} and \eref{x_exp} and rearranging for the growth rate, we get
\be
\g_\T = \frac{1}{\Dt} \ln\left(\prod_{\gtau=1}^\T \gr_\gtau\right) = \frac{1}{\dt} \left(\frac{1}{\T} \sum_{\gtau=1}^\T \ln\gr_\gtau\right),
\elabel{g_x_T}
\ee
where we used the unique property of the logarithm that $\ln(\alpha\beta) = \ln\alpha + \ln\beta$ to convert the product into a sum. Individual wealth grows exponentially at a random growth rate, $\g_\T$, whose distribution depends on the number of rounds played, $\T$.

What happens to $\g_\T$ in the long run? There are two ways of evaluating the $\Dt\to\infty$ (or, equivalently, $\T\to\infty$) limit of \eref{g_x_T}. Both are illuminating in different ways. First, we can consider the product of wealth multipliers. The effective multiplier which, if applied at each of the $\T$ rounds would yield the same product, is
\be
%(\greff)^\T =  \prod_{\gtau=1}^\T \gr_\gtau \quad \Rightarrow \quad
\greff = \left(\prod_{\gtau=1}^\T \gr_\gtau\right)^\frac{1}{\T}.
\ee
This is known as the \textit{geometric mean} of $\{\gr_\gtau\}$. Suppose heads appear $\gn_H$ times and tails $\gn_T$ times. Then
\be
\greff = (1.5)^\frac{\gn_H}{\T} (0.6)^\frac{\gn_T}{\T},
\ee
which is simply the product of possible realisations of the wealth multiplier raised to their relative frequencies. We know already what happens to relative frequencies as the number of rounds grows large: they converge almost surely to their corresponding probabilities, in this case both $1/2$. So, over many rounds, the effective per-round multiplier of wealth converges to
\be
\grtime = \lim_{\T\to\infty} \greff = (1.5)^\frac{1}{2} (0.6)^\frac{1}{2} = (0.9)^\frac{1}{2}  \approx 0.95.
\ee
This tells a strikingly different story from that told by ensemble-average wealth. Instead of growing exponentially by a factor 1.05 per round, a single wealth trajectory over long time decays exponentially by an effective factor of 0.95 per round. The corresponding exponential growth rate, from \eref{g_x_T}, is
\be
\ggtime = \frac{\ln\grtime}{\dt} \approx -5.1\% \text{ per round}.
\ee

The alternative route to this result is to take the $\T\to\infty$ limit of the sum in \eref{g_x_T}. Something rather interesting -- and central to ergodicity economics -- happens when we do this:
\be
\ggtime = \lim_{\T\to\infty} \frac{1}{\dt} \left(\frac{1}{\T} \sum_{\gtau=1}^\T \ln\gr_\gtau\right) =  \frac{\ave{\ln\gr}}{\dt}.
\elabel{g_time}
\ee
The first equality says that the growth rate of individual wealth over long time is the time average of the logarithm of the wealth multipliers, $\{\ln\gr_\gtau\}$, divided by the round duration, $\dt$. That's kind of believable. After all, we are trying to quantify how something is growing over time. The second equality, however, contains a remarkable conceptual insight: we can express this time average as the ensemble average of the random variable, $\ln\gr$. In other words, provided we know which observable to average over, we can investigate what happens in our model over long time by averaging over many systems!

To maintain sanity, let's confirm this is the same as the result obtained by finding the effective wealth multiplier:
\be
\frac{\ave{\ln\gr}}{\dt} = \frac{\frac{1}{2}\ln(1.5) + \frac{1}{2}\ln(0.6)}{\dt} = \frac{\ln\left[(1.5)^\frac{1}{2} (0.6)^\frac{1}{2}\right]}{\dt} = \frac{\ln\grtime}{\dt}.
\ee
It checks out.

There are two deep reasons why this works. The first is that the operation we perform on the product of the $\{\gr_\gtau\}$ in \eref{g_x_T} -- taking the logarithm and dividing by the duration -- produces an arithmetic mean of the $\{\ln\gr_\gtau\}$. In effect, by using the logarithm to transform multiplication to addition, we express the evolution of wealth in the form of a sum, specifically a finite-time average. The second reason is that the $\{\ln\gr_\gtau\}$ are independent and identically distributed (``iid'') random variables, with the same distribution as $\ln\gr$ and with no dependence on time. It follows that realisations over time and across systems are statistically indistinguishable, so that the time average in \eref{g_time} can be replaced by the ensemble average. Another way of seeing this is that the summation index, $\gtau$, in \eref{g_time} can just as well index different players as different rounds, because of the iid-ness of the $\{\gr_\gtau\}$.

Observables whose time and ensemble averages are equivalent are very special, and we will say much more about them in the next section.

Our computations of $\ggtime$ are consistent with the simulations in \secref{The_game}. \Fref{ens_v_time} (B) compares the single trajectory generated in \fref{1_3} to exponential decay at rate $\ggtime$. The trajectory in \fref{1_3} was no fluke: {\it every} trajectory will decay in the long run, with a growth rate approaching $-5.1\%$ per round. 
\begin{figure}[h]
\begin{picture}(200,200)(0,0)
\put(-100,0){\includegraphics[width=0.77\textwidth]{./chapter_tools/figs/x_of_t_N1M.pdf}}
\put(180,0){\includegraphics[width=0.8\textwidth]{./chapter_tools/figs/x_of_t_log_10000.pdf}}
\put(-30,120){(A)}
\put(250,120){(B)}  
\end{picture}
\caption{(A) \FEA for $\N=10^6$ and 52 time steps, the light blue line is 
the expectation value. (B) A single system simulated for 10,000 time steps, the light blue 
line decays exponentially with factor $\grtime$ in each time step.}
\flabel{ens_v_time}
\end{figure}

There are two average wealth multipliers -- $\ave{\gr}$ and $\grtime$ -- and corresponding growth rates -- $\ggave$ and $\ggtime$ -- which we have determined numerically and analytically. Neither average is ``wrong'' \textit{per se}. Instead, each average corresponds to a different property of the model and, therefore, answers a different question about the model. A statement like ``wealth goes up, on average'' is meaningless on its own. It should be countered with the question: ``on what type of average?''

%Expectation values are not what you expect.

\boldhead{Finite time and finite ensembles}
So far we have analysed how wealth evolution differs in the many-systems and long-time limits of the game. An observable that neatly captures these two aspects of multiplicative growth is the exponential growth rate, $\g(\ave{\x(\t)}_\N, \Dt)$, observed over finite time, $\Dt$, in a finite ensemble of $\N$ players. Here we make the growth rate an explicit function of the growing quantity and the period over which the growth is measured, because it is a random variable whose distribution depends on both. In this notation, the evolution equation for the \FEA wealth is
\be
\ave{\x(\t+\Dt)}_\N = \ave{\x(\t)}_\N \exp \left[ \g(\ave{\x(\t)}_\N, \Dt) \Dt \right].
\ee
%Exponential growth rates are ubiquitous and may be familiar, but because they are the origin of the logarithmic function, which will be important for us later on, we will intro them properly in a little excursion that will also clarify what a logarithm is.
Rearranging for the growth rate, we get
\be
\g(\ave{\x(\t)}_\N, \Dt) = \frac{\D \ln \ave{\x}_\N}{\Dt},
\elabel{gest}
\ee
where the $\D$ in the numerator corresponds to the change over the period, $\Dt$, in the denominator. For $\N$ and $\Dt$ finite, this is a random variable. Scalar quantities are obtained in two different limits of the same stochastic object. The exponential growth rate of the expectation value is
\be
\ggave = \lim_{\N\to\infty} \g(\ave{\x(\t)}_\N, \Dt),
\ee
which is also $\ln \ave{\gr}/\dt$. The exponential growth rate followed by any single trajectory ($\N=1$) when observed for a long time is 
\be
\ggtime = \lim_{\Dt\to\infty} \g(\x(\t), \Dt),
\elabel{gt}
\ee
which is also $\ln \grtime/\dt$.

We can also write \eref{gest} as a sum of the logarithmic differences in the $\T$ individual rounds of the gamble that make up the time interval $\Dt=\T \dt$
\be
\g(\ave{\x(\t)}_\N, \Dt)=\frac{1}{\T\dt}  \sum_{\gtau=1}^{\T} \D\ln \ave{\x(\t+\gtau\dt)}_\N.
\ee
For a single trajectory, $\N=1$, in the $\Dt\to\infty$ limit, we recover the remarkable result in \eref{g_time}.
%\bea
%\t&=&\lim_{\t\to\infty}\frac{1}{\t} \ln \x(\t)\\
%&=&\lim_{\T\to\infty}\frac{1}{\T\dt} \sum_\t^\T \D \ln \x(\t).
%\eea
%According to this definition, $\ggtime$ is the time average of the observable $\d \ln \x/\dt$.
It can be shown \cite{PetersKlein2013} that the time-average growth rate of a single trajectory is the same as that of a \FEA of trajectories, \ie for any finite \N 
\be
\lim_{\Dt\to\infty}\frac{\D \ln \x}{\Dt}=\lim_{\Dt\to\infty}\frac{\D \ln \ave{\x}_N}{\Dt}.
\ee
In \secref{finite_populations}, we will consider more generally the growth rates of finite ensembles over finite time.

\begin{excursion}{Scalars}
Coming soon.
%$\gr(\t)$ is a random variable, whereas both $\rex$ and $\rt$ are scalars.
%Scalars have the so-called ``transitive property'' that is heavily relied upon in 
%economic theory. Let $\ga_i$ be a set of scalars. Transitivity means that if $\ga_1>\ga_2$ and 
%$\ga_2>\ga_3$ we have $\ga_1>\ga_3$. Notice that we cannot rank random variables 
%in such a way. The ``greater than'' relation, $>$,
%is not defined for a pair of random variables, which is the mathematical
%way of saying that it is difficult to choose between two gambles, and it is why 
%we went to the trouble of removing the randomness from the stochastic process 
%$\x(\t)$. Removing randomness by
%averaging always involves a limiting process, and results are said to hold ``with probability one''. 
%In the case of $\rex$ we considered the infinite-ensemble limit, $\N\to \infty$, and 
%in the case of $\rt$ we considered the infinite-time limit, $\Dt\to\infty$. If we use the scalars 
%$a_i$ to represent preferences, we can test for consistency among preferences. 
%For instance, in such a model world where preferences are represented by scalars, 
%the facts that ``I prefer kangaroos to Beethoven'' and ``I prefer mango chutney to kangaroos'' 
%imply the fact ``I prefer mango chutney to Beethoven''. Translating back to reality, 
%economists like to call individuals who make the first two statements but not the 
%third ``irrational.'' 
%
%Because transitivity makes for a
%nicely ordered world, it is useful to find scalars to represent preferences.
%We are skeptical about the attempt to map all preferences
%into scalars because  the properties of mango chutney are too different, {\it qualitatively},
%from the properties of Beethoven. We will restrict our analysis to money --
%the amount of money we will receive is random and this introduces
%a complication, but at least we know how to compare one amount to 
%another in the limit of no randomness -- there is no qualitative differences 
%between $\$1$ and $\$3$, only a quantitative difference. 
%
%Both $\rex$ and $\rt$ are scalars, and both are therefore potentially powerful 
%representations of preferences. Your decision whether to accept our gamble could
%now be modelled as a choice between the value of the scalar $\rt$ if you do not
%accept our game, namely $\ga_1=1$, and the value of the scalar $\rt$ if you do accept, 
%namely approximately $\ga_2=0.95$. In this model of your decision-making you 
%would prefer not to play because $1>0.95$.
\end{excursion}

\begin{history}{William Allen Whitworth}
Coming soon.
%$\rex$ and $\rt$ are two different properties of the game. $\rex$ is 
%the large-ensemble limit, $\rt$ is the long-time limit, of wealth growth it induces. The Victorian 
%mathematician William Allen Whitworth postulated $\rt$ as the relevant property 
%for an individual deciding whether to take part in a repeated gamble. 
%He used this knowledge to write an appendix  entitled ``The disadvantage 
%of gambling'' to the 1870 edition of his book ``Choice and Chance'' 
%\cite{Whitworth1870}. He phrased his argument in terms of the difference of two squares. 
%Imagine that you either win or lose, with equal probability, 
%an amount $\epsilon \x(\t)$ in each round of a game. In the long run, positive and 
%negative changes will occur equally frequently, and to determine
%the overall effect we just need to consider the effect of one positive and one negative change
%in a row. Over one up and one down-move wealth changes by the factor
%\be
%(1+\epsilon)(1-\epsilon)=1-\epsilon^2.
%\elabel{Whitworth}
%\ee
%This factor is clearly less than one, meaning that what's often called a ``fair gamble'' -- one 
%that does not change the expectation value of the gambler's wealth -- leads to an 
%exponential decay of his wealth over time. Hence the title of the appendix 
%``The disadvantage of gambling.'' We will see in \secref{Geometric_Brownian} that Whitworth's work
%captured the essence of \Ito's famous 1944 discovery \cite{Ito1944} that was to form the basis of
%much of financial mathematics.
%
%Whitworth was arguing against a dogma of expectation values of wealth, that had 
%been established almost immediately following Fermat and Pascal's work. He 
%hoped to show mathematically that gambling may not be a good idea even if 
%the odds are favourable, and was a 
%proponent of the notion that commerce should and does
%consist of mutually beneficial interactions rather than one winner and one loser. 
%In the end his voice was not heard in the economics community. His main career was as a priest at All Saints Church in London's Margaret Street, only a 22 minute stroll away from the (first office of the) London Mathematical Laboratory, according to Google \fref{all_saints}.
\end{history}
%\begin{figure}[h!]
%\begin{picture}(200,270)(0,0)
%  \put(-30,0){\includegraphics[width=1.15\textwidth]{./chapter_tools/figs/all_saints.pdf}}
%\end{picture}
%\caption{Location of All Saints Church and the London Mathematical Laboratory's initial office.}
%\flabel{all_saints}
%\end{figure}
%\FloatBarrier

\begin{excursion}{Compounding growth, exponentials, and the logarithm}
Coming soon.
%The logarithm is a relatively recent mathematical discovery, made some time in the 1590s by John Napier, the 8th Laird of Merchiston in Scotland. Let's say you lend me some money, $\x(\t)$, for one year, $\Dt=1$~year, and I have to pay interest on the loan. We agree a yearly interest rate of $r_{1}=5\%$ per year. 
%
%After one year the amount I have to repay is
%\bea
%\x(\t+\Dt)=\overbrace{\x(\t)}^{\text{Principal}}+ \overbrace{\D \x}^{\text{Interest}}.
%\eea 
%I could convert the interest payment into a rate, $r_1$, so that $r_1 \x(\t)$ is the rate (dollars per time) at which I have to make constant payments to you for one year, so that you end up with the right amount of interest at the end of the year, and
%\be
%\x(\t+\Dt)=\x(\t)r_1 \Dt
%\elabel{deal}
%\ee
%This makes it easy, for instance, to find something to enter into my accounts if I have to say after 6 months how much I owe you at that moment. I would say: only half a year has passed, so I only owe you half the interest, 
%\be
%\x(\t+\Dt/2)=\x(\t)\left(1+r_1 \frac{\Dt}{2}\right). 
%\elabel{deal_2}
%\ee
%But if this is really the amount I owe you after 6 months, surely I should pay interest on this new amount for the following 6 months. We will see that this leads to a problem. Substituting $\x(\t+\Dt/2)$ for $\x(\t)$ on the \RHS of \eref{deal_2} gives
%\bea
%\x(\t+\Dt)&=&\x(t+\Dt/2)\left(1+r_1 \frac{\Dt}{2}\right)\\
%&=&\x(\t)\left(1+r_1\frac{\Dt}{2}\right)^2
%\eea
%Equating to \eref{deal} we find a contradiction
%\bea
%\x(\t) (1+ r_{\Dt} \Dt)&\stackrel{?}{=}&\x(\t)(1+r_{\Dt}/2)^2\\
%\implies 1+ r_{\Dt} \Dt&\stackrel{?}{=}&1+r_{\Dt}\Dt+\left(\frac{r\Dt}{2}\right)^2 \text{{\color{red} \huge \lightning}}.
%\eea
%Strange as it may seem, the solution to this problem is to acknowledge that interest rates depend on the time scale at which they're defined -- the compounding time scale. We can fix the problem by introducing $r_2$ -- the semi-annual interest rate (the subscript 2 indicates that we've split the original interval in 2 equal parts). It is defined by
%insisting that the one-step and two-step computations give the same interest payment $\D\x$, 
%\be
%1+ r_1 \Dt=\left(1+r_2 \frac{\Dt}{2}\right)^2.
%\ee
%There's nothing stopping us from writing down the general expression for any number, $\T$, of intermediate stock-takings
%\bea
%\frac{\x(\t+\Dt)}{\x(\t)}&=&\left(1+r_\T\frac{ \Dt}{\T}\right)^\T\\
%&\text{which implies}&\\
%r_\T&=& \frac{1}{\Dt}\T \left\{\left[\frac{\x(\t+\Dt)}{\x(\t)}\right]^{1/\T}-1\right\}
%\eea
%A common trick to remove the dependence of some quantity ($r_{\T})$ on another ($\T$)  is to let the control variable diverge. The limit no longer depends on the diverging quantity (we've seen this trick before: the expectation value doesn't depend on the ensemble size, which has diverged).
%\be
%r_\infty= \frac{1}{\Dt} \underbrace{\lim_{\T\to\infty}\T \left\{\left[\frac{\x(\t+\Dt)}{\x(\t)}\right]^{1/\T}-1\right\}}_{\ln\left(\frac{\x(\t+\Dt)}{\x(\t)}\right)}.
%\ee
%Note the procedure here: we keep the total time interval, $\Dt$, fixed and split it into $\T$ ever more numerous and shorter sub-intervals $\dt$.
%Because it's tedious to write down the long expression involving the limit, we define it as a new function, called the logarithm, $\ln(\cdot)$, as indicated by the underbrace.
%\be
%\ln \ga :=  \lim_{\T\to\infty}\T \left\{\ga^{1/\T}-1\right\}.
%\ee
%The logarithm has a property that makes it uniquely suited for characterizing multiplicative processes: the logarithm of a ratio is the difference of the logarithms of numerator and denominator, 
%\be
%\ln \frac{\ga_2}{\ga_1}=\ln \ga_2 - \ln \ga_1 = \D \ln \ga.
%\ee
%The inverse function of the logarithm is the exponential (by definition), denoted $\exp(\cdot)$, so that $\exp(\ln \ga)=\ga$, and the limiting growth rate $r_\infty$ is called the logarithmic or exponential growth rate, which we also denote by $\gm$ (subscript ''m'' for ``multiplicative'').
%
%Just to be sure we got this across: these are the definitions of the logarithm and the exponential. You may know many properties of logarithms and exponentials -- all of them can be derived from the definitions we have just presented.
\end{excursion}

\begin{excursion}{Dimensional analysis}
Coming soon.
%We will often and without qualm write the expression $\D \ln \x$. Dimensional
%analysis suggests to think about this expression carefully, at least once. This may
%seem pedantic but the absence of this pedantry has caused sufficient
%confusion in economic theory for us to risk antagonizing you. ``Dimension'' in this context is closely related to the 
%concept of ``unit'' -- for instance, a dollar is a money unit, and the dimension function
%for money tells us how to convert from one currency into another. Similarly, length may 
%have the unit ``meter'', and the dimension function for length tells us how to convert between 
%different systems of units, such as meters and yards. 
%We can only point to the subject here and recommend the book
%by \person{Barenblatt} for a comprehensive treatment \cite{Barenblatt2003}.
%Dimensional analysis is a deeply fascinating and powerful tool that every physicist 
%is drilled to use at all times. \person{Taylor} famously used it to compute the energy
%released by an early nuclear explosion at the Trinity site near Alamogordo, New Mexico, based on 
%some grainy pictures published by Life magazine, at least that's the legend 
%\cite{Taylor1950b,Deakin2011}. Fluid dynamicists in general use it to find meaningful quantities 
%to distinguish different types of flow. In many problems involving random walks dimensional 
%analysis immediately reveals scaling properties, supposed solutions to many problems can
%be seen at a glance to be wrong, and, conversely some complicated-looking 
%problems can be solved as if by magic just by appealing to dimensional analysis.
%
%\person{Barenblatt} shows in his book that the dimension function must be a (scale-free) power-law 
%monomial if there is to be no distinguished system of units. We can all agree that the unit
%of money is physically irrelevant -- I can do exactly the same with the pennies in my 
%bank account as I can do with the pounds those pennies correspond to. Since this is so, 
%for functions of monetary amounts to be physically meaningful we want them to be 
%power-law monomials. An amount of square-dollars, $\$^2$, may be meaningful, but an
%amount of logarithmic or exponential dollars cannot be meaningful. Hence $\ln(\x)$ on its own
%is just some symbol spat on a page by a printer, but it has no physical meaning.
%The reason we're comfortable writing $\D \ln \x$ is the unique property of the logarithmic function
%\be
%\ln \x_1 - \ln \x_2 = \ln\left(\frac{\x_1}{\x_2}\right).
%\ee
%The quantity in brackets on the \RHS is always dimensionless, it's a pure number because
%the dimension functions of two different values of $\x$ always cancel out. So do the units:
%$\$1/\$2=1/2$, which is a pure number without units. We will see that indeed only differences 
%in logarithms of $\x$ will appear in these lecture notes or in any other reasonable lecture notes. 
%Pedantically, we would refuse to write $\D \ln(\x)$ and insist on writing $\ln\left(\frac{\x_1}{\x_2}\right)$.
%Since the first notation is shorter and one can make formal arguments for its validity, we are 
%happy to use it here. 
%
%The issue is related to a result obtained by \person{von Neumann} and 
%\person{Morgenstern} in their 
%famous
%%but quite unhelpful
%book \cite{vonNeumannMorgenstern1944}: 
%only differences in utility functions can have physical meaning. 
%We will have a lot more to say about utility functions (which we call ergodicity mappings) in \cref{Rates}.
\end{excursion}

\section{Ergodicity}
\seclabel{Ergodic_observables}
We have encountered two types of averaging -- the ensemble average and the time average. In our case -- assessing whether it will be good for you to play our game, the time average is the interesting quantity because it tells you what happens to your wealth as time passes. The ensemble average is irrelevant because you do not live your life as an ensemble of many yous who can average over their wealths. Whether you like it or not, you will experience yourself owning your own wealth at future times; whether you like it or not, you will never experience yourself owning the wealth of a different realization of yourself. The different realizations, and therefore the expectation value, are fiction, fantasy, imagined.

We are fully aware that it can be counter-intuitive that with probability one, a different rate is observed for the expectation value than for any trajectory over time. It sounds strange that the expectation value is completely irrelevant to the problem. A reason for the intuitive discomfort is history: since the 1650s we have been trained to compute expectation values, with the implicit belief that they will reflect what happens over time. It may be helpful to point out that all of this trouble has a name that's well-known to certain people, and that an entire field of mathematics is devoted to dealing with precisely this problem. The field of mathematics is called ``ergodic theory.'' It emerged from the question under what circumstances the expectation value is informative 
of what happens over time, first raised in the 1870s by Boltzmann, early in the development of statistical mechanics which began with Maxwell in the 1850s. These lecture notes are our attempt to use precisely the insights of these physicists to re-develop economic theory from the foundations up.

\begin{history}{Randomness and ergodicity in physics}
Coming soon.
%The 1850s were about 200 years after \person{Fermat} and \person{Pascal} introduced expectation 
%values into the study of random systems. Following the success of \person{Newton}'s 
%laws of motion, established around the same time as the expectation value, 
%the notion of ``proper science'' had become synonymous with 
%mechanics. Mechanics had no use for randomness and probability 
%theory, and the success of mechanics was interpreted as a sign that 
%the world was deterministic and that sooner or later we would understand 
%what at the time still seemed random. At that point probability theory would 
%become obsolete. 
%
%When \person{Boltzmann} hit upon the ingenious idea of introducing randomness into 
%physics, to explain the laws of thermodynamics in terms of the underlying 
%dynamics of large numbers of molecules, he was fighting an uphill battle. 
%Neither molecules nor randomness were much liked in the physics 
%community, especially in continental Europe, right up until the publication of 
%\person{Einstein}'s 1905 paper on diffusion \cite{Einstein1905}. \person{Boltzmann} had to be 
%more careful than \person{Fermat} and \person{Pascal}. He had to pre-empt
%predictable objections from his peers, and the question of ergodicity had to be 
%answered -- the usefulness of probability theory relies heavily on expectation 
%values, but as we have seen, they are averages over imagined future states of the 
%universe. \person{Boltzmann}'s critics were aware of this and were not shy to voice their
%concerns. Under what circumstances are expectation values meaningful? 
%\person{Boltzmann} gave two answers. 
%\bi
%\item expectation values are meaningful when 
%the quantity of interest really is an average (or a sum) over many approximately 
%independent systems. An average over a finite ensemble will be close to the 
%expectation value if the ensemble is large enough. 
%\item expectation values 
%are meaningful, even if only a single system exists, if they reflect what happens over time. 
%\ei
%
%\person{Boltzmann} called a system ``ergodic\footnote{The word ``ergodic'' was coined by \person{Boltzmann}. 
%He initially proposed the word ``monodic'', from Greek $\mu o \nu o$ (unique) + $o\delta o \varsigma$ (path) suggesting 
%that a single path when followed for a sufficiently long time will explore all there is to explore and reflect what happens 
%in an ensemble. The term ``ergodic'' refers to the specific system \person{Boltzmann} was considering, namely an energy 
%($\epsilon \rho \gamma o \nu$) shell across which a path is being traced out.}'' if the possible 
%states of the system could be assigned probabilities in such a way that
%the expectation value of any observable with respect to those probabilities would 
%be the same as its time average with probability 1.
%
%Our setup requires us to be more modest, and we will speak of specific ergodic observables (not of ergodic systems) if their ensemble and time averages are the same.
\end{history}

To convey concisely that we cannot use the expectation value and the time average interchangeably in our game, we would say ``the observable $\x$ is not ergodic.'' 

\begin{defn}{Ergodic property}
In these notes, an observable $\A$ is called ergodic if its 
expectation value is constant in time
%, $\frac{\gd\ave{\A}}{\gd\t}=0$, 
and its time average converges to this value with probability one\footnote{Some researchers would call $\A$ ``mean ergodic'' and require further observables derived from it to be (mean) ergodic in order to call $\A$ ``wide-sense ergodic.'' This extra nomenclature is not necessary for our work, but we leave a footnote here to avoid confusion.}

\be
\lim_{\Dt \to\infty}\frac{1}{\Dt } \int_{\t}^{\t+\Dt} \A(\gs) \gd\gs =\lim_{\N\to\infty} \frac{1}{\N}\sum_\gi^\N \A_\gi(\t) .
\elabel{def_ergodic}
\ee
\end{defn}
The \RHS of \eref{def_ergodic} is evaluated at time $\t$, and unlike the \LHS could be a function of time. For now, we restrict our definition of ergodicity to a setup where that is not the case, \ie where the ergodic property holds at all times. In \secref{RGBM_moments} we will discuss transient behavior, where the distribution of $\A$ is time dependent. We then also consider an observable ``ergodic'' if its expectation value only converges to the time-average in the $\t\to\infty$ limit.

In terms of random variables, $\Z$, and stochastic processes, $\Y(\t)$, the ergodic property can be visualized as in \fref{ergodic_grid}. Averaging a stochastic process over time or over the ensemble are completely different operations, and only under very rare circumstances (namely under ergodicity) can the two operations be interchanged. In our coin-tossing game the operations are clearly not interchangeable. An implicit assumption of interchangeability in the early days is the Original Sin of economic theory.

We stress that in a given setup, some observables may have the ergodic property even if others do not. Language therefore must be used carefully. Saying our game is non-ergodic really means that some key observables of interest, most notably wealth $\x$, are not ergodic. Wealth $\x(\t)$, defined by \eref{law}, is clearly not ergodic -- with $\A=\x$ the \LHS of \eref{def_ergodic} is zero, and the \RHS is not constant in time but grows. The expectation value $\ave{\x}(\t)$ simply doesn't give us the relevant information about the temporal behavior of $\x(\t)$.
 
This does not mean that no ergodic observables exist that are related to $\x$. Such observables do exist, and we have already encountered two of them. In fact, we will encounter a particular type
of them frequently -- in our quest for an observable that tells us what happens over time in a stochastic system we will find them automatically. However, again, the issue is subtle: an ergodic observable may or may not tell  us what we're interested in. It may be ergodic but not indicate what happens to $\x$. For example, the multiplicative factor $\gr(\t)$ is an ergodic observable that reflects what happens to the expectation value of $\x$, whereas per-round changes in the logarithm of wealth, $\d \ln \x = \ln \gr$, are also ergodic and reflect what happens to $\x$ over time. \textbf{This reflects the importance of the observable appearing meaningfully in a sum.}

\vspace{.3cm}
\underline{Proposition:} $\gr(\t)$ and $\d \ln \x$  are ergodic for the wealth dynamic defined by \eref{law} and \eref{gamble}.

\begin{proof}
According to \eref{ens} and \eref{f_ens}, the expectation value of $r(t)$ is
\be
\ave{\gr}=\lim_{\N\to\infty} \frac{1}{\N} \sum_\gi^\N \gr_\gi,
\elabel{e_r}
\ee
and, according to \eref{t_ave_f_disc}, the time average of $\gr(\t)$ is
\be
\tave{\gr}=\lim_{\T\to\infty} \frac{1}{\T} \sum_\gtau^\T \gr_\gtau,
\elabel{t_r}
\ee
where we have written $\gr_\gtau = \gr(\t+\gtau\dt)$ to make clear the equivalence between the two expressions. The only difference is between the labels we have chosen for the dummy variable ($\gi$ in \eref{e_r} and $\gtau$ in \eref{t_r}). Clearly, the expressions yield the same value. 

The same argument holds for $\d \ln \x$.
\end{proof}

Whether we consider \eref{t_r} an average over time or over an ensemble is only a matter of our interpretation. The mathematics is unchanged.

The expectation value $\ave{\d \ln \x}$ is important, historically. \person{Daniel Bernoulli} noticed in 1738 \cite{Bernoulli1738} that people tend to optimize $\ave{\d \ln \x}$, whereas it had been assumed that they should optimize $\ave{\d \x}$. Unaware of the issue of ergodicity (150 years before the concept was discovered and the word was coined), \person{Bernoulli} had no good explanation for this empirical fact and simply stated that people tend to behave as though they valued money non-linearly. We now know what is actually going on: multiplicative dynamics are a fairly realistic model for real wealth, and under those dynamics $\d \x$ is not ergodic, and $\ave{\d \x}$ is of no interest -- it doesn't tell us what happens over time. However, $\d \ln \x$ {\it is} ergodic, and $\ave{\d \ln \x}$ does tell us what happens to $\x$ over time, wherefore seeing people optimise $\ave{\d \ln \x}$ just means seeing them optimise wealth over the one trajectory that describes a financial life, rather than across the ensemble of possibilities.

% repetition
%When the foundations of economic theory were laid, specifically in \person{Bernoulli}'s 
%seminal paper of 1738 \cite{Bernoulli1738}, the distinction between ergodic and
%non-ergodic observables was unknown. Researchers thought that the expectation value
%of $\d \x$ reflected what happens over time but observed that real people behaved
%according to what the expectation value of $\d \ln \x$ would suggest. While the origin of the 
%discrepancy remained mysterious and numerous puzzles and paradoxes in 
%economic theory arose as a result. The paradigm we outline here resolves these puzzles.

Ergodicity is not the same concept as stationarity. As an illustration of the difference, consider the following process: $\gf(\t)=\z_{\gi}$, where $\z_{\gi}$ is an instance of a random variable $\Z$. Explicitly, this means a realisation of the stochastic process $\gf(\t)$ is generated as follows: we generate the random instance $\z_{\gi}$ once, and then fix $\gf(\t)$ at that value for all time. The distribution of $\gf(\t)$ is independent of $\t$ and in that sense $\gf(\t)$ is stationary. But it is not ergodic: averaging over the ensemble, we obtain $\ave{\gf(\t)}=\ave{\z}$, whereas averaging over time in the $\gi^{\text{th}}$ trajectory gives $\overline{\gf}=\z_{\gi}$. Thus the process is stationary but not ergodic.

%\section{Changes and stability}
%\seclabel{Rates}
%In this section we discuss the role of changes and stability in science in general and then say a few words about the coin toss in particular. 
%If the latter part is hard to understand right now -- don't worry, we'll get back to it in much more detail in \cref{decisions}.
%The ergodic observable $\d \ln \x$, identified in the previous section, is almost a rate.
%Dividing it by the duration of the gamble, we obtain exactly the exponential 
%growth rate of $\x$, namely $\frac{\d \ln \x}{\dt}$. Finding good growth rates 
%will be important, wherefore we now discuss the notion of a rate and 
%the notion of time independence. 
%To do this properly let's think about the basic task of science. This may be described as the 
%search for stable structure. Science attempts to build models of the world 
%whose applicability does not vary over time. This doesn't mean that the world doesn't change, 
%but the way in which the models describe change does not change. ``Children grow up to be adults'' is a 
%description of a change that has been true for a long time. The model identifies
%something stable. This is implied by the fact that we can write 
%equations (or English sentences) in ink on paper, with the equation (or sentence) remaining 
%useful over time. The ink won't change over time, so
%if an article written in 1905 is useful today then it must describe something that hasn't 
%changed in the meantime. These ``somethings'' are often somewhat 
%grandiosely called laws of nature.
%
%\person{Newton}'s laws are a good illustration of this. They are part of mechanics, meaning that they
%are an idealized mathematical model of the behavior of positions, time, and masses (by the way, this definition of 
%mechanics in terms of base quantities is a neat application of dimensional analysis \cite{Barenblatt2003}). 
%For instance, \person{Newton}'s second law, $\NF=\Nm \frac{\gd^2 \Nx}{\gd\t^2}$, states that the mass multiplied by 
%the rate of change of the rate of change of its position equals the force. The law is an unchanging 
%law about positions, time, and masses, but it does not say that positions don't change, it doesn't even say 
%that rates of change of positions don't change. It does say that the rate of change of the 
%rate of change of a position remains unchanged so long as the force and the mass 
%remain unchanged. \person{Newton}'s deep insight was to transform an unstable thing -- the position of a mass --
%until it became stable: he fixed the force and considered rates of changes of rates of changes, et 
%voil\'a!, a useful equation could be written down in ink, remaining useful for 350 years so far.
%
%Like \person{Newton}'s laws (a mathematical model of the world), our game is a prescription of changes. 
%Unlike \person{Newton}'s laws it's stochastic, but it's a prescription of changes nonetheless. 
%The multiplicative aspect of our game makes it also a powerful mathematical model of the world, as we shall see in subsequent lectures. 
%
%We're very much interested in changes of $\x$ -- we want to know 
%whether we're winning or losing -- but changes in $\x$ are not stable. 
%Under the rules of the game the rate of change of wealth, $\frac{\d \x(\t)}{\dt}$, is a different 
%random variable for each $\t$ because it is proportional to $\x(\t)$. But not to worry, 
%in \person{Newton}'s case changes in the position are not stable either, even in a 
%constant force field. Nonetheless \person{Newton} found a useful stable property. 
%Maybe we can do something similar. We're looking for a function $\gv(\x)$ that satisfies two conditions: 
%it should indicate what happens to $\x$ itself, and its random changes should be instances of a time-independent random variable.
%
%The first condition is that $\gv(\x)$ must tell us 
%whether $\x(\t)$ is growing or shrinking -- this just means that $\gv(\x)$ has to 
%be monotonic in $\x$. We know that there is something time-independent
%about $\x$ because we were able to write down in ink how $\x$ changes. So we only need 
%to find the monotonic function of $\x$ whose additive changes inherit the time-independence 
%of the ink in \eref{law}. The game is defined by a set of factors of change in $\x(\t)$, \eref{gamble}. 
%Therefore, the fractional change in $\x$, namely
%$\gr(\t) = \frac{\x(\t+\dt)}{\x(\t)}$, comes from a time-independent distribution. Which 
%function responds additively to a multiplicative change in its argument? 
%The answer is the logarithm, \ie
%only the logarithm satisfies
%\be
%\gv[\x(\t+\dt)]-\gv[\x(\t)]=\gv \left(\frac{\x(\t+\dt)}{\x(\t)}\right)
%\ee
%and we conclude that for our game $\gv(\x)=\ln\x$.
%For multiplicative dynamics, \ie if $\frac{\x(\t+\dt)}{\x(\t)}$ is ergodic, the expectation 
%value of the rate of change of the logarithm of $\x(\t)$ determines whether the game is long-term profitable 
%for an individual.
%
%More generally, when evaluating a gamble that is represented as a stochastic process, it seems that people's intuitive
%choices roughly maximise appropriate long-time growth rates.
%Mathematically speaking, they
%\begin{enumerate}
%\item
%find a monotonically increasing function $\gv[\x(\t)]$ such that $\frac{\d \gv[\x(\t)]}{\dt}$ 
%is ergodic, over time taking values of instances of a random variable.
%\item
%compute the expectation value of $\frac{\d \gv[\x(\t)]}{\dt}$. If this is positive then $\x(\t)$
%grows in the long run, if it is negative then $\x(\t)$ decays.
%\end{enumerate}
%
%The mathematics of this procedure is discussed in detail in \cref{Rates}, and an
%experiment testing whether people behave as predicted is described in \secref{Copenhagen}.

%\section{Normal distribution}
%\seclabel{Normal_distribution}
%Coming soon.

\section{Brownian motion}
\seclabel{Brownian_motion}
\boldhead{Random walks}
We motivate the model called \BM as the continuous-time limit of a random walk. In \secref{game_revisited}, we established that the discrete increments of the logarithm of wealth, $\x$, are instances of a time-independent random variable in our game. A quantity, call it $\gv$, making such random steps over time is said to perform a \textit{random walk}. Indeed, the blue line for a single system in \fref{1_2} (B) shows a 52-step trajectory of a random walk.

Random walks come in many forms. In all of them, $\gv$ changes discontinuously by an amount, $\d\gv$, which is the realisation of a random variable with a stationary (\ie time-independent) distribution. Each change takes places over a time interval, $\dt$, which may be regular or which may itself be drawn from a stationary distribution.

We are interested in the simple case where $\gv$ changes at regular intervals, \ie at times $t+\dt$, $t+2\dt$, and so on. For the distribution of increments, $\d\gv$, we insist only on the existence of the variance, \ie that 
\be
\var[\d \gv]=\ave{\d \gv^2}-\ave{\d \gv}^2
\ee
be finite. Note that this means the expectation value, $\ave{\d\gv}$, must also be finite. Increments with heavier-tailed distributions do not produce \BM in the continuous-time limit. (\BM has continuous paths and that continuity is broken by such increments.)

The change in $\gv$ after a long time is the sum of many independent increments, 
\be
\gv(\t+\T\dt)-\gv(\t)=\sum_{\gtau=1}^\T \d\gv_\gtau.
\ee
The \CLT (strictly speaking, the Lindeberg-L\'{e}vy version of it) tells us that this sum, a random variable, will become normally distributed as we add more terms and rescale appropriately. Specifically, the rescaling keeps the width of the distribution finite and removes systematic drift in the mean:
\be
\lim_{\T\to\infty} \overbrace{\frac{1}{\sqrt{\T}}}^{\mathclap{\text{keep width finite}}}\sum_{\gtau=1}^\T ( \d \gv_\gtau \underbrace{-\ave{\d \gv}}_{\mathclap{\text{remove systematic drift}}}) \sim \mN(0 , \var[\d \gv]).
\elabel{CLT}
\ee
%where we call $\frac{\ave{\d \gv}}{\dt}$ the ``drift term.'' 
The notation $\sim \mN(0, \var[\d \gv])$ is shorthand for ``follows a normal distribution with mean $0$ and variance $\var[\d \gv]$.'' The logarithmic change in the long-time limit that was of interest to us in the analysis of the coin game is, therefore, normally distributed. 

We can gain a partial intuition for this result from the mean,
\be
\ave{\sum_{\gtau=1}^\T \d\gv_\gtau} = \sum_{\gtau=1}^\T \ave{\d\gv_\gtau} = \T \ave{\d\gv},
\ee
and variance,
\be
\var\left[\sum_{\gtau=1}^\T \d\gv_\gtau\right] = \sum_{\gtau=1}^\T \var\left[\d\gv_\gtau\right] = \T \var[\d\gv],
\ee
of the sum of the $\T$ independent random increments. These are easily derived and are a worthwhile exercise. They tell us the scaling of the mean and variance of the sum: both grow linearly with the number of terms, $\T$. One way of writing the sum, then, is
\bea
\sum_{\gtau=1}^\T \d\gv_\gtau &\sim& \mathcal{D}(\T \ave{\d\gv},\T \var[\d\gv]) \\
&\sim& \T \ave{\d\gv} + \sqrt{\T}\,\mathcal{D}(0,\var[\d\gv]),
\eea
where $\mathcal{D}$ represents some unknown distribution with the mean and variance shown. We can rearrange this to get an identical expression for the finite sum to that in \eref{CLT}, except that the distribution unspecified.

So, it's fairly simple to deduce the rescaling used in \eref{CLT}. The magic of the \CLT is that, in the limit $\T\to\infty$, the distribution converges to a normal, \ie $\mathcal{D}$ becomes $\mN$. This is regardless of the distribution of the individual increments: we need only know that their mean and variance exist. We won't spoil the magic here, as our main interest is in using the result, but deriving the \CLT (and there are many ways to do it) is another illuminating exercise for the reader.

%We work with the simplest setup: at time zero we start at zero, $\gv(0)=0$, and in each time step, we either increase or decrease $\gv$ by 1, with probability 1/2. To avoid notation clutter, we'll set the duration of a time step to $\dt=1$, so that $\T$ is both the number of steps and the time.
%
%We are interested in the variance of the distribution of $\gv$ as $\T$ increases, which we obtain by computing the first and second moments of the distribution. 
%
%The first moment (the expectation value) of $\gv$ is $\ave{\gv}(\T)=0$, by symmetry for all times. 
%
%We obtain the second moment by induction\footnote{The argument is nicely illustrated in \cite[Volume 1, Chapter 6-4]{Feynman1963}, where we first came across it.}:
%Whatever the second moment, $\ave{\gv(\T)^2}$, is at time $\T$, we can write down its value at time $\T+1$ as 
%\bea
%\ave{\gv(\T+1)^2}&=&\frac{1}{2}\left[\ave{(\gv(\T)+1)^2}+\ave{(\gv(\T)-1)^2}\right]\\
%&=&\frac{1}{2}\left[\ave{\gv(\T)^2+1+2\gv(\T)}+\ave{(\gv(\T)^2+1-2\gv(\T))}\right]\\
%&=&\ave{\gv(\T)^2}+1.
%\eea
%In addition, we know the initial value of $\gv(0)=0$. By induction it follows that the second moment is
%\be
%\ave{\gv(\T)^2}=T
%\ee
%and, since the first moment is zero, the variance is
%\be
%\var(\gv(\T))=T.
%\elabel{BM_var}
%\ee

The standard deviation -- the width of the distribution -- of changes in a quantity following a random walk thus scales as the square-root of the number of steps that have been taken, $\sqrt{\T}$. This square-root scaling leads to many interesting results. It can make averages stable, because $\sqrt{\T}/\T$ converges to zero for large $\T$, and sums unstable, because $\sqrt{\T}$ diverges for large $\T$. Consequently, we may expect that, as the size of some system or the length of some time interval increase, some properties become stable and others unstable.

Imagine simulating a single long trajectory of $\gv$ and plotting it on paper.\footnote{This argument is inspired by a colloquium presented by Wendelin Werner in the mathematics department of Imperial College London in January 2012. Werner started the colloquium with a slide that showed a straight horizontal line and asked: what is this? Then answered that it was the trajectory of a random walk, with the vertical and horizontal axes scaled equally.} The amount of time that has to be represented by a fixed length of paper increases linearly with the simulated time because the paper has a finite width to accommodate the horizontal axis. If $\ave{\d \gv} \neq 0$ then the amount of variation in $\gv$ that has to be represented by a fixed amount of paper also increases linearly with the simulated time. However, the departures of $\D\gv$ from its expectation value $\T \ave{\d \gv}$ only increase as the square-root of $\T$. Thus, the amount of paper-space given to these departures scales as $\T^{-1/2}$, and for very long simulated times the trajectory will look like a straight line on paper.

In an intermediate regime, fluctuations will still be visible and they will be approximately normally distributed. In this regime it is often easier to replace the discrete random walk model with the corresponding continuous process. That process -- finally -- is \BM. 

\boldhead{Brownian motion}
We think of \BM as the limit of a random walk where we shorten the duration of a step, $\dt \to 0$, so that changes happen continuously in time and not just at discrete times. As we will see, this creates a model which is more tractable than the random walk, and this will allow us to focus on economics. How realistic it is to think of wealth changing continuously is debatable. We certainly do not measure real-world observables, such as individual wealth, in continuous time. We only ever get to glimpse them between discrete time intervals and, if we are lucky, we will collect enough glimpses to do some science. So, we are replacing the discrete-time model with a continuous-time model to use the latter to describe discrete-time glimpses in the real world. If continuity seems an unnecessary assumption to you, we don't blame you! The value in this approach is in the powerful analytical tools it will put at our disposal.

Subtracting any drift, or assuming zero drift, we know from our treatment of the random walk that the net change in $\gv$ over $\T$ steps scales like $\sqrt{\T}$. If the intervals are regular, \ie their duration is fixed, then this means the net change scales like the square-root of the time spent walking. In \BM we maintain this scaling \textit{over all time intervals} and, in particular, over small intervals, $\dt \to 0$. This requires $\d\gv = O(\sqrt{\dt})$. In the limit $\dt\to0$, the local slope of a trajectory diverges, $\d\gv/\dt\to\infty$. Thus, trajectories of \BM are infinitely jagged, or -- in mathematical terms -- they are not differentiable. The way in which they become non-differentiable, through the $\sqrt{\dt}$ factor, just leaves the trajectories continuous. (This isn't the case for $\d\gv = O(\dt^\alpha)$, where $\alpha < 0.5$.) 

Continuity of $\gv$ means that it is possible to make the difference $|\gv(\t)-\gv(\t+\epsilon)|$ as small as we like by choosing $\epsilon$ small enough. Trajectories without this property contain what are appropriately called ``jumps.'' Therefore, continuity means that there are no jumps. These subtleties make \BM a topic of great mathematical interest, and many books have been written about it. We will pick from these books only what is immediately useful to us. To convey the universality of \BM, we define it formally as follows:
\begin{defn}{Brownian motion i}
If a stochastic process has continuous paths, stationary independent increments, and is distributed according to $\mN(\gmu \t, \gsigma^2 \t)$ then it is a Brownian motion.
\end{defn}
The process can be defined in different ways. A particularly compact definition is this:
\begin{defn}{Brownian motion ii}
If a stochastic process is continuous, with stationary independent increments, then the process is a Brownian motion.
\end{defn}
We quote from \cite{Harrison2013}: {\it ``This beautiful theorem shows that Brownian motion can actually be defined by stationary independent increments and path continuity alone, with normality following as a consequence of these assumptions. This may do more than any other characterization to explain the significance of Brownian motion for probabilistic modeling.''}

Indeed, \BM is not just a mathematically rich model but also -- due to its emergence through the \CLT -- a model that represents a large universality class, \ie it is a good description of what happens over long times in many other models that produce random trajectories. 

\boldhead{Stochastic differential equation}
The power of \BM lies in its simplicity and analytic tractability, involving only two parameters, $\gmu$ and $\gsigma$. We will often work with its representation as a \SDE:
\be
\gd\gv=\gmu \gd\t + \gsigma \gd\gW.
\elabel{BM_dx}
\ee
We had better say what this means. Firstly, it should have quotation marks around it because all of the terms in it are infinitesimal. Really it is a shorthand for the more formal integral equation,
\be
\gv(\t) = \gv(\t_0) + \gmu\int_{t_0}^t\gd\gs + \gsigma\int_{t_0}^t \gd\gW(\gs),
\ee
where each term is either a scalar quantity or a random variable. The formal study of such equations, often called ``stochastic calculus'', is a huge field in its own right. We will not examine it in detail, although we will borrow some useful results from it in \secref{Ito}. Instead, we will work with the informal version, \eref{BM_dx}, and we will interpret the terms in very simple ways:
\bi
\item $\gd\gv$ is a small (infinitesimal) increment in $\gv$;
\item $\gmu$ is a constant parameter, known as the ``drift'';
\item $\gd\t$ is a small increment in time, $\t$;
\item $\gsigma$ is a constant parameter, known as the ``volatility''; and
\item $\gd\gW$ is the so-called ``Wiener increment'', a random variable whose properties we describe below.
\ei
The name ``Brownian motion'' is sometimes reserved for the stochastic process generated by \eref{BM_dx} when $\gmu=0$, with ``Brownian motion with drift'' used for the case when $\gmu\neq0$. When $\gmu=0$ and $\gsigma=1$, then it is sometimes called ``standard Brownian motion'' or the ``Wiener process''.

\boldhead{Wiener process}
Let's look at the random Wiener increment, $\gd\gW$. This is where all the action is: without it, \eref{BM_dx} would be an \ODE with easy-to-find deterministic solutions, perfectly inadequate for modelling uncertainty across systems and over time. $\gd\gW$ is the beating heart of many \SDEs. The Wiener increment can be defined by two properties, its distribution and its auto-covariance,\footnote{Physicists often write $\gd\gW=\geta \gd\t$, where $\ave{\geta}=0$ and $\ave{\geta(\t) \geta(\t')}=\gdelta(\t-\t')$, in which case $\gdelta(\t-\t')$ is the Dirac delta function, defined by the integral $\int_{-\infty}^{\infty} \gf(\t) \gdelta(\t-\t') \gd\t=\gf(\t')$. Because of its singular nature -- $\geta(\t)$ does not exist, only its integral exists -- it can be difficult to develop an intuition for this object, and we prefer the $dW$ notation.}
\bea
\gd\gW &\sim& \mN(0,\gd\t)\\
\ave{\gd\gW_\t \gd\gW_\gs}&=& \gdelta(\t,\gs)\gd\t,
\eea
where the subscripts denote the starting time of each increment, \ie $\gd\gW_\t$ is modelled as occurring at the end of the interval from $\t$ to $\t+\gd\t$. $\gdelta(\t,\gs)$ is the Kronecker delta,
\be
\gdelta(\t,\gs) = \begin{cases}
1 & \text{if } \t=\gs\\
0 & \text{otherwise.}
\end{cases}
\ee
Put very simply, the $\{\gd\gW_\t\}$ are independent and identically distributed normal random variables with mean zero and variance $\gd\t$. That's all we really need to know.

\boldhead{Solution}
Rather like an \ODE, we can integrate \eref{BM_dx}, say from time $0$ to $\t$, to express its solution as
\bea
\gv(\t) &=& \gv(0) + \gmu\t + \gsigma\int_0^\t \gd\gW_\gs \\
&=& \gv(0) + \gmu\t + \gsigma\gW(\t),
\elabel{BM_v}
\eea
where $\gW(\t) = \int_0^\t \gd\gW_\gs$ is the Wiener process with $\gW(0)=0$. Since the integral is the limiting case of a sum over normally distributed Wiener increments, $\gW(\t)$ is itself normally distributed with mean zero and variance $\t$, \ie $\gW(\t)\sim\mN(0,\t)$. In other words, its width has the same $\sqrt{\t}$ scaling as the random walk.

$\gW(\t)$ is a stochastic process which, for fixed time, generates a time-dependent normal random variable. However, this does not mean we can generate a trajectory by drawing successive independent realisations from $\mN(0,\t)$. While the Wiener increments, $\gd\gW_\t$, are independent, the Wiener process is autocorrelated. In simple terms, this is because its value at the end of a time period depends on where it started, because it must take a continuous path to travel from there. It's a standard result in stochastic calculus that the autocovariance of the values of a Wiener process at two different times is simply the earlier time, \ie for $\t<\t'$, $\ave{\gW(\t)\gW(\t')} = \t$. This is because 
\bea
\ave{\gW(\t)\gW(\t')} &=& \ave{\gW(\t)[\gW(\t)+\gW(\t')-\gW(\t)]}\\
&=& \ave{\gW(\t)^2} + \ave{\gW(\t)[\gW(\t')-\gW(\t)]}\\
&=& \t + 0,
\eea
where the final term follows by independence of increments over non-overlapping time intervals.

Let's return to the stochastic process, $\gv(\t)$, in \eref{BM_v}. If we fix the time, $\t$, this is a time-dependent normal random variable,
\be
\gv(\t) \sim \mN(\gv(0)+\gmu\t, \gsigma^2\t),
\ee
whose location moves linearly in time and whose width grows like the square-root of time. 

\boldhead{Numerics} 
In simulations, \BM paths can be constructed from a discretised version of \eref{BM_dx}
\be
\gv(\t+\dt)=\gv(\t)+ \gmu \dt + \gsigma \sqrt{\dt} \gxi_\t,
\elabel{BM_d}
\ee
where $\gxi_\t$ are iid standard normal variates, $\gxi_\t\sim\mN(0,1)$.

\boldhead{Non-ergodicity of Brownian motion}
\BM is a non-ergodic stochastic process. This is easily seen by comparing expectation value and time average. We start with the expressions (stated without proof here) for the \FEA and the finite-time average of \BM. The \FEA (easy to derive) is distributed as
\be
\ave{\gv(\t)}_\N \sim \gmu \t+\mN(0, \t/\N),
\elabel{fin_ens_BM}
\ee
and the finite-time average (harder to derive) of a single \BM trajectory is distributed as
\be
\bar{\gv}_\t \sim \gmu \t + \gsigma \mN(0, \t/3).
\elabel{fin_tim_BM}
\ee

The expectation value, \ie the limit $\N\to\infty$ of \eref{fin_ens_BM}, converges to $\gmu \t$ with probability  one, so it depends on time. It's unclear how to compare that to a time average, which cannot depend on time. Its limit $\t\to\infty$ does not exist.

The time average, the limit $\t\to\infty $ of \eref{fin_tim_BM} diverges unless $\gmu=0$, but even with $\gmu=0$ the limit is a random variable with diverging variance -- something whose density 
is zero everywhere. In no meaningful sense do the two expressions converge to the same scalar in the relevant limits.

Clearly, \BM, whose increments are ergodic, is itself not ergodic. However, that doesn't make it unmanageable or unpredictable -- we know the distribution of \BM at any moment in time. But the non-ergodicity has surprising consequences of which we mention one now. We already mentioned that if we plot a Brownian trajectory with non-zero drift on a piece of paper it will turn into a straight line for long enough simulation times. This suggests that the randomness of a Brownian trajectory becomes irrelevant under a very natural rescaling. Inspired by this insight let's hazard a guess as to what the time average of zero-drift \BM might be. 

The simplest form of zero-drift \BM is the Wiener process. It starts at zero, $\gW(0)=0$ and has variance $\var[\gW(\t)]=\t$. The process is known to be recurrent: it returns to zero, arbitrarily many times, with probability one in the long-time limit. We might guess that the time average of the Wiener process,
\be
\bar{\gv}=\lim_{\t\to\infty} \frac{1}{\t}\int_0^\t \gd\gs \gv(\gs),
\ee
will converge to zero with probability one. We would be wrong. Yes, the process has no drift, and yes it returns to zero infinitely many times, but its time average is not a delta function at zero.
It is, instead normally distributed with infinite variance according to the following limit (the limit of \eref{fin_tim_BM} with \gmu=0)
\be
\bar{\gv}\sim \lim_{\t\to\infty} \mN(0,\t/3).
\ee
Averaging over time, in this case, does not remove the randomness. A sample trajectory of the finite-time average (not of \BM but of the average over a \BM) is shown in \fref{1_6}. In the literature this process, $\frac{1}{\t}\int_0^\t \gd\t' \gv(\t')$, is known as the ``random acceleration process'' \cite{Burkhardt2007}.
\begin{figure}[h!]
\begin{picture}(200,220)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_tools/figs/BM_time_ave.pdf}}
\end{picture}
\caption{Trajectory of the finite-time average of a zero-drift \BM. The process is not ergodic: the time average does not converge to a number, but is instead distributed according to 
$\mN(0,\t/3)$ for all times, while the expectation value is zero. It is the result of integrating a \BM; integration is a smoothing operation, and as a consequence the trajectories are smoother than \BM (unlike a \BM trajectory, they are differentiable).}
\flabel{1_6}
\end{figure}
\FloatBarrier

\section{Geometric Brownian motion}
\seclabel{Geometric_Brownian}
\begin{defn}{Geometric Brownian motion}
If the logarithm of an observable performs \BM, then the observable itself performs ``\GBM''.
\end{defn}
In \secref{Brownian_motion}, the observable $\gv$ performs \BM. If we imagine $\gv$ as a function of another observable, $\x$, specifically if $\gv(\x)=\ln\x$, then $\x$ performs \GBM. The change of variable from $\x$ to $\gv(\x)=\ln(\x)$ is trivial to state, but it has interesting consequences. It implies, for instance, that 
\begin{itemize}
\item $\x(\t)$ is lognormally distributed;
\item increments in $\x$ are neither stationary nor independent;
\item $\x(\t)$ cannot change sign;
\item the most likely value of $\x$ (the mode) does not coincide with the expectation value of $\x$. 
\end{itemize}
These and other properties of the lognormal distribution will be discussed in detail in \cref{People}.

\boldhead{\SDE}
Again, it is informative to write \GBM as a stochastic differential equation. Our definition suggests we should do this as follows,
\be
\gd\ln\x = \tilde{\gmu}\gd\t + \tilde{\gsigma}\gd\gW.
\ee
However, it's somewhat unintuitive to start by examining the changes of an already-transformed variable. Let's start by modelling the real phenomenon instead. We are looking for the continuous-time version of the coin game, where the change in our wealth at the end of each round is a random multiple of our wealth at the start of the round. This suggests we want an \SDE like
\be
\gd\x=\x(\gmu \gd\t+ \gsigma \gd\gW).
\elabel{GBM_c}
\ee
We will show in \secref{Ito} that these two \SDEs define the same stochastic process, subject to a mapping between the drift and volatility parameters.

\boldhead{Discrete time}
Similarly to \BM, trajectories for \GBM can be simulated using the discretised form (\cf \eref{BM_d})
\be
\d \x=\x(\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t),
\elabel{GBM_d}
\ee
where $\gxi_\t \sim \mN(0,1)$ are instances of a standard normal variable. In such simulations we must pay attention that the discretization does not lead to negative values of $\x$. This 
happens if the expression in brackets in \eref{GBM_d} is smaller than $-1$ (in which case $\x$ changes negatively by more than itself). To avoid negative values we must have $\gmu \dt + \gsigma \sqrt{\dt} \gxi_\t>-1$, or  $\gxi_\t <\frac{1+\gmu\dt}{\gsigma \sqrt{\dt}}$. As $\dt$ becomes large it becomes more likely for $\gxi_\t$ to exceed this value, in which case the simulation fails. But $\gxi_\t$ is Gaussian distributed, meaning it has thin tails, and choosing a sufficiently small value of $\dt$ makes these failures essentially impossible.

\GBM on logarithmic vertical scales looks like \BM on linear vertical scales. \Fref{1_2} is, in fact, an example of a very coarse discretisation of \GBM. But it's useful to look at a more finely discretised trajectory of \GBM on linear scales to develop an intuition for this important process.
\begin{figure}[h!]
\begin{picture}(200,230)(0,0)
    \put(0,-5){\includegraphics[width=\textwidth]{./chapter_tools/figs/GBM_trajectory.pdf}}
\end{picture}
\caption{Trajectory of a \GBM. What happens to the trajectory tomorrow depends strongly on where it is today. For instance, unlike for \BM, it is difficult to recover from a low value of $\x$, and trajectories are likely to get stuck near zero (indeed, certain in the long run when $\gmu-\gsigma^2/2<0$). Occasional excursions are characterised by large fluctuations. Parameters are $\gmu=0.05$ per time unit and $\gsigma=\sqrt{2 \gmu}$, corresponding to zero growth rate in the long run. It would be easy to invent a story to go with this (completely random) trajectory, something like  ``things were going well in the beginning but then a massive crash occurred that destroyed morale.''}
\flabel{1_7}
\end{figure}

The basic message of the game from \secref{The_game} is that we may obtain different values for growth rates, depending on how we average: an expectation value is one average; a time average is quite another. The game itself is sometimes called the multiplicative binomial process \cite{Redner1990}, we thank \person{S. Redner} for pointing this out to us. \GBM is the continuous version of the multiplicative binomial process, and it shares the basic feature of a difference between the growth rate of the expectation value and time-average growth.

\boldhead{Ensemble average}
The expectation value is easily computed -- the process is not ergodic, but that does not mean we cannot compute its expectation value. We simply take the expectation values of both sides of \eref{GBM_c} to get
\bea
\ave{\gd\x}&=&\ave{\x(\gmu \gd\t+ \gsigma \gd\gW)}\\
=\gd\ave{\x}&=&\ave{\x} \gmu \gd\t.
\eea
This differential equation has the solution 
\be
\ave{\x(\t)}=\x(\tn)\exp(\gmu \t),
\ee
which determines the growth rate of the expectation value as 
\be
\gm(\ave{x})=\gmu.
\elabel{expectation_g}
\ee

\boldhead{Time average}
As we know, this growth rate is different from the growth rate that materializes with probability one in the long run. Computing the time-average growth rate is only slightly more complicated, and it will get even simpler once we've introduced \Ito calculus in \secref{Ito}. But for now we will follow this plan: consider the discrete process \eref{GBM_d} and compute the changes in the logarithm of $\x$, then we will let $\dt$ become infinitesimal and arrive at the result for the continuous process. We know $\d \ln(\x(\t))$ to be ergodic and reflective of performance over time, wherefore we will proceed to take its expectation value to compute the time average of the exponential growth rate of the process. 

The change in the logarithm of $\x$ in a time interval $\dt$ is
\bea
\ln \x(\t+\dt) - \ln \x(\t) &=&\ln [\x(1+\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t)] - \ln \x(\t)\\
&=&\ln \x + \ln (1+\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t) - \ln \x(\t)\\
&=&\ln (1+\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t),
\eea
which we Taylor-expand as $\ln(1+ \text{something small})$ because we will let $\dt$ become small. Expanding to second order,
\be
\ln \x(\t+\dt) - \ln \x(\t) =\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t - \frac{1}{2} \left(
\gsigma^2\dt 
\gxi_\t^2+2\gmu \gsigma \dt^{3/2}\gxi_\t \right)+\go(\dt^2),
\ee
using ``little-o notation'' to denote terms that are of order $\dt^2$ or smaller. Finally, because $\d \ln \x(\t)$ is ergodic, by taking the expectation value of this equation we find the time average of $\d \ln \x(\t)$
\be
\ave{\ln \x(\t+\dt) - \ln \x(\t)} =\gmu \dt- \frac{1}{2} \left(\gmu^2\dt^2+\gsigma^2\dt \right)+o(\dt^2).
\ee
Letting $\dt$ become infinitesimal the higher-order terms in $\dt$ vanish, and we find
\be
\ave{\ln \x(\t+\gd\t) - \ln \x(\t)} =\gmu \gd\t- \frac{1}{2} \gsigma^2 \gd\t,
\ee
so that the time-average growth rate is
\be
\gt=\frac{\gd \ave{\ln \x}}{\gd\t}=\gmu - \frac{\gsigma^2}{2}.
\elabel{time_g}
\ee
The non-ergodicity of \GBM leads to a difference between the behaviour of the expectation value (which grows at $\gm(\ave{\x})$) and the long-time behaviour of any given trajectory (which grows at $\gt$). Because people experience their wealth over time (which may be described by \GBM) and have no access to the ensemble of other possible trajectories, they quite reasonably behave closer to optimising $\gt$ than to $\gm(\ave{\x})$.

%We could have guessed the result by combining Whitworth's argument on the disadvantage of gambling with the scaling of \BM. Let's re-write the factor $1-\epsilon$ in \eref{Whitworth} as
%$1-\gsigma \sqrt{\dt}$. According to the scaling of the variance in a random walk, \eref{BM_var}, this would be a good coarse-graining of some faster process (with shorter time step) underlying Whitworth's game. To find out what happens over one single time step we take the square root of \eref{Whitworth},
%\be
%[(1+\gsigma \sqrt{\dt})(1-\gsigma \sqrt{\dt})]^{1/2}=[1-\gsigma^2 \dt]^{1/2}.
%\ee 
%Letting $\dt$ become infinitesimally small, we replace $\dt$ by $\gd \t$, and the first-order term of a Taylor-expansion becomes exact,
%\be
%[(1+\gsigma \sqrt{\dt})(1-\gsigma \sqrt{\dt})]^{1/2}\to 1-\frac{\gsigma^2}{2} \gd\t,
%\ee 
%in agreement with \eref{time_g} if the drift term $\mu=0$, as assumed by Whitworth.

\section{\Ito calculus}
\seclabel{Ito}
The results we obtained by analysing the discrete difference equation for \GBM, \eref{GBM_d}, are more commonly shown by applying \Ito's formula directly to the \SDE in \eref{GBM_c}. We will not discuss \Ito calculus in depth here, but we will use some of its results. The key insight of \Ito was that the non-differentiability of so-called \Ito processes leads to a new form of calculus. In particular, a new chain rule is needed. An \Ito process is an \SDE of the following form,
\be
\gd\x = a(\x, \t) \gd\t + b(\x, \t) \gd\gW.
\elabel{Ito_process}
\ee
If we are interested in the behaviour of some other quantity that is a function of $\x$, let's say $\gv(\x)$, then \Ito's formula says that $\gv$ obeys the \SDE,
\be
\gd\gv =  \left(\frac{\partial \gv}{\partial\t} +a(\x, \t)\frac{\partial \gv}{\partial\x} + \frac{b(\x,\t)^2}{2} \frac{\partial^2 \gv}{\partial\x^2}\right) \gd\t + b(\x, \t) \frac{\partial \gv}{\partial\x} \gd\gW.
\elabel{Ito}
\ee

Many derivations of this formula can be found online. Intuitive derivations, such as \cite{Hull2006}, use the scaling of the variance of the Wiener process. More formal derivations, along the lines of \cite{Harrison2013}, rely on integrals. We provide an informal derivation here, with three basic steps, which we find easier to remember than \eref{Ito}. The steps are:
\bi
\item Taylor expand the total derivative, $\gd\gv$, in terms of $\gd\t$ and $\gd\x$;
\item substitute for $\gd\x$ using the known \SDE for $\x$;
\item replace $\gd\gW^2=\gd\t$ and keep only $O(\gd\gW$) and $O(\gd\t)$ terms.
\ei
The key insight remains that the variance of the Wiener increment has linear (\ie random walk) scaling in time. Replacing $\gd\gW^2$ by $\gd\t$ in the final step incorporates this insight. \Ito showed how to do this formally for integral equations, for which our \SDEs are shorthand.

Let's follow these steps for general $\gv(\x)$ and then specifically for $\gv(\x)=\ln\x$. Firstly, the general case. Taylor expanding for $\gd\gv$, we get
\be
\gd\gv =  \frac{\partial \gv}{\partial\t} \gd\t + \frac{\partial \gv}{\partial\x} \gd\x + \frac{1}{2} \frac{\partial^2 \gv}{\partial\x^2} \gd\x^2 + O(\gd\t^2).
\ee
Substituting \eref{Ito_process} for $\gd\x$ gives us
\be
\gd\gv =  \frac{\partial \gv}{\partial\t} \gd\t + \frac{\partial \gv}{\partial\x} (a\,\gd\t + b\,\gd\gW) + \frac{1}{2} \frac{\partial^2\gv}{\partial\x^2} (a^2 \gd\t^2 + 2ab\,\gd\t\,\gd\gW + b^2\gd\gW^2) + O(\gd\t^2),
\ee
where we have supressed the $\x$ and $\t$ dependence of $a(\x, \t)$ and $b(\x, \t)$ for compactness. Finally, we replace $\gd\gW^2$ by $\gd\t$ and throw away terms which tend to zero faster than $\gd\t$. This gives
\bea
\gd\gv &=&  \frac{\partial \gv}{\partial\t} \gd\t + \frac{\partial \gv}{\partial\x} (a\,\gd\t + b\,\gd\gW) + \frac{1}{2} \frac{\partial^2 \gv}{\partial\x^2} (a^2 \cancel{\gd\t^2} + 2ab\,\cancel{\gd\t\,\gd\gW} + b^2\gd\t) + \cancel{O(\gd\t^2)} \nn\\
&=&  \left( \frac{\partial \gv}{\partial\t} + a \frac{\partial \gv}{\partial\x} + \frac{b^2}{2} \frac{\partial^2 \gv}{\partial\x^2} \right) + b \frac{\partial \gv}{\partial\x} \gd\gW,
\eea
which is \Ito's formula in \eref{Ito}.

For the specific case $\gv(\x)=\ln\x$, and setting $a(\x,\t)=\gmu\x$ and $b(\x,\t)=\gsigma\x$, we find that the \SDE for \GBM
\be
\gd\x = \x(\gmu\gd\t+\gsigma\gd\gW)
\ee
is transformed to
\be
\gd\ln\x = \left(\gmu-\frac{\gsigma^2}{2}\right)\gd\t + \gsigma\gd\gW,
\elabel{GBM_SDE_log}
\ee
which is a \BM in $\ln\x$. Indeed, this is how we defined \GBM at the start of \secref{Geometric_Brownian}.

Once we have the \SDE for $\ln(\x)$, we can integrate it, say from time $0$ to $\t$, as we did for \BM in \eref{BM_v}. This gives a stochastic process,
\be
\ln\x(\t) = \ln\x(0) + \left(\gmu-\frac{\gsigma^2}{2}\right)\t + \gsigma\gW(\t),
\elabel{GBM_sol_log}
\ee
which, for fixed $\t$, is a normally distributed random variable,
\be
\ln\x(\t) \sim \mN\left(\ln\x(0) + \left(\gmu-\frac{\gsigma^2}{2}\right)\t, \gsigma^2\t\right).
\elabel{GBM_dist}
\ee
Thus $\x(\t)$ is lognormally distributed.

We can rewrite \eref{GBM_sol_log} as a solution for $\x(\t)$,
\be
\x(\t) = \x(0)\exp\left[\left(\gmu-\frac{\gsigma^2}{2}\right)\t + \gsigma\gW(\t)\right].
\elabel{GBM_sol}
\ee
which shows neatly that $\x(\t)$ follows noisy exponential growth. If we write $\x(\t)=\x(0)\exp(g(t)t)$, we can express the noise as a random growth rate,
\be
\g(t) = \gmu-\frac{\gsigma^2}{2} + \frac{\gsigma\gW(\t)}{\t}.
\ee
Since $\gW(\t)/\t \sim 1/\sqrt{t}$, it's easy to see that the long time limit of this noisy growth rate is $\gt=\gmu-\gsigma^2/2$, which is also its expectation value, $\ave{g}$.

The above computations are intended to give the reader intuitive confidence that \Ito calculus can be trusted.\footnote{\Ito calculus is one way of interpreting the non-differentiability of \gd\gW. Another interpretation is due to Stratonovich, which is not strictly equivalent. However, the key property of \GBM that we make extensive use of is the difference between the growth rate of the expectation value, $\gm(\ave{\x})$, and the time-average growth rate, $\gt$. This difference is the same in the Stratonovich and the \Ito interpretation, and all our results hold in both cases.} We find that, though phrased in different words, our key insight -- that {\it the growth rate of the expectation value is not the time-average growth rate} -- has appeared in the literature not only in 1870 \cite{Whitworth1867} but also in 1944 \cite{Ito1944}. And in 1956 \cite{Kelly1956}, and in 1966 \cite{Thorp1966}, and in 1991 \cite{CoverThomas1991}, and at many other times! And yet the depth of this insight remained unprobed.

\Eref{time_g}, which agrees with \Ito calculus, may be surprising. Consider the case of no noise $\gd\x=\x \gmu \gd\t$. Here we can identify $\gmu=\frac{1}{\x}\frac{\gd\x}{\gd\t}$ as the infinitesimal increment in the logarithm, $\frac{\gd \ln(\x)}{\gd\t}$, using the chain rule of ordinary calculus. A na\"ive application of the chain rule to \eref{GBM_c} would therefore also yield $\frac{\gd \ave{\ln(\x)}}{\gd\x}=\gmu$, but the fluctuations in \GBM have a non-linear effect, and it turns out that the usual chain rule does not apply. \Ito calculus is a modified chain rule, \eref{Ito}, which leads to the difference $-\frac{\gsigma^2}{2}$ between the expectation-value growth rate and the time-average growth rate. 

This difference is sometimes called the ``spurious drift'', but at the \LML we call it the ``Weltschmerz'' because it is the difference between the many worlds of our dreams and fantasies, and the one cruel reality that the passage of time imposes on us.

\section*{Summary of \cref{Tools}}
Coming soon.
%In this chapter we have introduced the following key concepts:
%\bi
%\item[\bf Random variable]
%A random variable $Y$ is a set of pairs of possible values and  corresponding probabilities, $Y=\{(y_1, p_1),(y_2, p_2)...\}$.
%The sets may be discrete or continuous. We stressed that a random variable is an a-temporal concept. It's just a bunch of possible values and their weights (probabilities). In real life we often this of generating instances of random variables as time passes, but this is not part of the formal definition of a random variable.
%
%\item[{\bf Expectation value}]
%The expectation value of a random variable is the weighted sum $\ave{Y}=\int y \mathcal{P}_Y(y) dy$, where $\mathcal{P}_Y$ has atomic point masses in the discrete case, which means we can express the integral as $\ave{Y}=\sum_i y_i p_i$.
%
%The expectation value is also called the ensemble average, which reflects a physical interpretation: imagine (infinitely) many possible worlds, identical safe for the value taken by the the random variable $Y$. Those values are represented in the superverse of many worlds in proportion to their probabilties. Averaging $y$ over the ensemble of universes then gives the expectation value.
%
%\item[\bf Logarithms and exponentials]
%The logarithmic function is defined as $\ln \ga :=  \lim_{\T\to\infty}\T \left\{\ga^{1/\T}-1\right\}$. It was motivated by computing interest on loans. Its property of turning products into sums makes it a key function in many branches of science where quantities that combine multiplicatively are to be represented by a quantity that combines additively. 
%
%The exponential function is the inverse of the logarithm.
%
%\item[\bf Stochastic process]
%A stochastic process $Y_Z(z;t)$ is a family of random numbers, indexed by a parameter, $t$, that we call ``time.'' At each moment in time, a realization of a stochastic process takes a random value. We often think of stochastic processes as being generated through stochastic differential equations, where the initial value of some quantity is iteratively transformed -- for example by adding a random value to it.
%Examples include
%\bi
%\item[{\bf Coin-toss game}]
%Start with initial value \$1 and multiply it in each round with an instance of the random variable $[(0.6,1/2),(1.5,1/2)].$
%
%\item[\bf Brownian motion]
%Brownian motion can be defined as a stochastic process with continuous paths (no jumps) whose distribution is Gaussian $\mN(\mu t, \sigma^2 t)$. 
%
%\item[\bf Geometric Brownian motion]
%If the logarithm of a quantity is a Brownian motion, the quantity itself is geometric Brownian motion. Wealth in the coin-toss game is a discrete form of geometric Brownian motion.
%\ei
%
%\item[\bf Time average]
%Any function of time, including any stochastic process can time averaged. For an observable $\ga(\t)$, the finite-time average is $\frac{1}{\T}\int_0^\T \ga(\t) \dt$. This quantity is a random variable. One way to get rid of the randomness is to keep averaging, namely to let $\T$ diverge. The time average (without the qualifier ``finite-time'') is the limit  $\lim_{\T\to\infty} \frac{1}{\T}\int_0^\T \ga(\t) \dt$.
%
%\item[\bf Scalars]
%Scalars are just numbers. We say ``scalar'' when we want to emphasize that we're not talking about higher-dimensional mathematical objects, like vectors or random variables. The neat thing about scalars is their transitivity property: if $a>b$ and $b>c$ then $a>c$. This allows us to rank scalars. As a consequence, anything that can be mapped, or collapsed onto a scalar can be ranked. For this reason, much of decision theory consists of coming up with ways of collapsing models of wealth onto scalars. These can then be ranked, yielding transitive preferences.
%
%\item[\bf Dimensional analysis]
%Dimensional analysis is a branch of mathematics that places constraints on what can be a physical quantity. This is done by insisting that the quantity cannot depend on human conventions. It is often used to derive scaling relations and as a sanity check for supposed mathematical relationships between physical quantities.
%
%\item[\bf \Ito process]
%\Ito processes are stochastic processes that can be written down as particularly simple stochastic differential equations. Their form allows a deep mathematical analysis of their properties.
%
%\item[\bf \Ito's formula]
%We will make use of \Ito's formula, which is a tool to find the increment in a function of an \Ito process.
%
%\item[\bf Ergodic observable]
%An ergodic observable is a stochastic process with the following ergodic property: its time average is identical to its expectation value. If the process includes transients, one has to consider the expectation value far away from any transients.
%\ei