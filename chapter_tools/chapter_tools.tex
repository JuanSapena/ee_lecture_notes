\chapter{Tools\clabel{Tools}}

%\section{Summary}
%\seclabel{Summary}
{\it In this chapter we motivate and introduce the basic mathematical tools we will use. In \secref{The_game} we play a simple coin-toss game and analyze it 
numerically, by Monte Carlo simulation, and analytically, with pen and paper. The game motivates the 
introduction of the expectation value and the time average, which in turn 
lead to a discussion of ergodic properties. As we have seen, the ergodicity question  -- whether time averages are identical to expectation values -- is the key to our redevelopment of formal economics. This is because ergodicity hadn't been established as a concept when the original formalism was developed. The scientific search for stable structures leads to constants in deterministic settings. When randomness is introduced, the role previously played by constants is taken on by ergodic observables.
We also introduce the concepts of a random variable, a stochastic process, scalars as representations of transitive preferences, logarithms and exponentials, and dimensional analysis.

In \secref{Brownian_motion} we notice that wealth on logarithmic scales follows a random walk in our game, and we relate this to Brownian motion, as the continuous-time limit of the random walk. This allows us to introduce Brownian motion and its scaling properties that are robust enough to yield insights into more complicated models.

Finally, we ask in \secref{Geometric_Brownian} what wealth in our game is doing in the continuum limit but on linear scales. This takes us to geometric Brownian motion, which will be our starting point for much of the rest of these lectures. We derive ensemble-average and time-average growth rates for geometric Brownian motion, by explicitly taking the continuous-time limit, and then state the key result of \Ito calculus, \eref{Ito_process} and \eref{Ito}, which allows an easier derivation of these growth rates and will be relied on in later chapters.

Some historical perspective is provided to understand the prevalence or
absence of key concepts in modern economic theory and other fields.
The emphasis is on introducing concepts and useful machinery, with applications in later chapters.}
\newpage

\section{Random variable}
\seclabel{random_variable}
In economics, as elsewhere, we are often interested in `experiments' whose outcomes we don't yet know. Examples are each coin toss in the game in \secref{The_game} and the result of a football match. We might know something about the experiment, such as the possible outcomes and that some are more plausible than others, but we are ignorant of the actual outcome. Luckily, we can use probability theory, a branch of mathematics, to build models of our ignorance.

Often the experimental outcomes have, or can be mapped to, numerical values. For example, each coin toss in the game has possible outcomes heads and tails, which correspond to wealth multipliers 1.5 and 0.6. In such cases, we model the uncertain numerical value as a \textit{random variable}. We assume you have seen random variables before and we will not give a lengthy technical account. Instead, we recommend the two-page discussion in \cite[p.~2]{vanKampen1992}, whose key points we reproduce here.

A random variable, $\Z$, is defined by:
\bi
\item the set of its possible values; and
\item a probability distribution over this set.
\ei
The set of values of $\Z$ may be continuous, like the interval $(3,12)$ or the real numbers, $\mathbb{R}$; discrete, like $\{4, 7.8, 29\}$ or the integers, $\mathbb{Z}$; or a combination of the two. The probability distribution is a function which maps values to probabilities. So
\be
\prob{\Z=\z}=\p
\ee
means that the outcome $\Z=\z$ is associated with the probability $\p$. A specific value, $\z$, is sometimes called an `instance' or `realisation' of the random variable, $\Z$. While not obligatory, it is a common convention to denote random variables in upper case and realisations in lower case.

Forget, for the moment, what probabilities might mean in the context of an experiment. In purely mathematical terms, they are just real numbers associated with outcomes. The probability distribution has two constraints:
\bi
\item the probability of any outcome must be non-negative; and
\item the probability of an outcome that is certain to happen, \ie that includes all possible outcomes, must be one.
\ei
The latter is a normalisation condition which fixes the scale of the probabilities.

\boldhead{Discrete random variable}
When the set of outcomes is discrete, say $\{\z_1,\dots,\z_M\}$, we assign a probability, $\p_\gj$, to each outcome, $\z_\gj$, such that
\be
\prob{\Z=\z} = \begin{cases}
\p_\gj & \text{if } \z=\z_\gj\\
0 & \text{otherwise}
\end{cases}
\ee
and
\be
\sum_{\gj=1}^M\p_\gj = 1.
\ee

\boldhead{Continuous random variable}
Most of the models we will study contain random variables whose possible outcomes form a continuous set. In this case, $\Z$ can take uncountably many values, to which we can't assign non-zero probabilities while maintaining the normalisation condition. Instead, we assign probabilities to intervals. We define a \PDFa, $\PDF_{\Z}(\z)$, such that the probability of $\Z$ being in the interval $(a,b)$ is given by
\be
\prob{a \leq \Z \leq b} = \int_a^b \PDF_{\Z}(\z)\gd\z.
\ee
The \PDFa is a non-negative function, $\PDF_{\Z}(\z) \geq 0$, normalised so that the probability of the certain outcome, \ie the integral over all possible outcomes, is one:
\be
\int_{-\infty}^{+\infty} \PDF_{\Z}(\z)\gd\z = 1.
\ee
Note the difference between subscript and argument: $\PDF_{\Z}(\z)$ is the probability density of the random variable $\Z$ at value $\z$. You might find it helpful to think of $\PDF_{\Z}(\z)\delta\z$ as the approximate probability of $\Z$ being in the small interval $(\z,\z+\delta\z)$ close to $\z$. 

\boldhead{Interpretation}
So far we have said nothing of the meaning of probabilities: they are simply numbers assigned to outcomes of random variables. One way to interpret probabilities is to imagine many separate experiments, of whose outcomes are identically ignorant. For example, we can imagine tossing the same coin many different times, assuming that each coin toss is equally likely to result in heads. Suppose we perform $\N$ experiments and record the number of times, $n$, that a particular outcome occurs. Under the so-called `frequentist' interpretation, the appropriate probability to assign to this outcome is its relative frequency, $n/\N$, in the limit $\N\to\infty$. In the coin toss example, the probability assigned to heads would be 0.5 if the coin were unbiased; if biased, it would be some other number between 0 and 1.

Note also that time does not appear in the random variable setup. Of course, there is nothing stopping us from using a probability distribution which depends on time or, indeed, any other variable, like the day of the week or the country we are in. Such parametrisations of the random variable do not change fundamentally its mathematical structure: a set of outcomes and associated probabilities. When we consider probability distributions of random variables that do depend on time, such as wealth, $\x(\t)$, in the coin tossing game, we will make the time dependence explicit. By default we assume random variables are time-independent

\section{Expectation value}
The \textit{expectation value}\footnote{Also known as expected value, mathematical expectation, first moment, and mean.} is a property of a random variable. Denoted by $\eval{\Z}$, it is the probability-weighted average of the realisations of $\Z$.

\begin{defn}{Expectation value}
If $\Z$ is a discrete random variable, its expectation value is the sum of the possible realisations, $\z_\gj$, weighted by their probabilities, $\p_\gj$:
\be
\eval{\Z}=\sum_\gj \p_\gj \z_\gj.
\elabel{exp_sum}
\ee 
If $\Z$ is a continuous random variable, its expectation value is the integral over the possible realisations weighted by the probability density:
\be
\eval{\Z}=\int_{-\infty}^{+\infty} \PDF_{\Z}(\z) \z \gd\z.
\ee 
\end{defn}
The sum or the integral may not exist, in which case the random variable does not have an expectation value. Examples of this are the payout of the St Petersburg lottery in \secref{SPP} and the power-law distributed random variable with \PDFa:
\be
\PDF_{\Z}(\z) = \begin{cases}
\z^{-2} & \z\geq1 \\
0 & \z<1.
\end{cases}
\ee

\section{Ensemble average}
A conceptually different quantity from the expectation value is the \textit{ensemble average}. Instead of weighting the average of the possible realisations of $\Z$ by their probabilities, we take an unweighted average of a collection of realisations. For a finite number of realisations, we call this the \textit{finite-ensemble average} and denote it by $\ave{\Z}_\N$. 
\begin{defn}{Finite-ensemble average}
The finite-ensemble average of a random variable, $\Z$, is the average over a finite number, $\N$, of realisations,
\be
\ave{\Z}_{\N} \equiv \frac{1}{\N}\sum_{\gi=1}^{\N} \z_{\gi},
\elabel{f_ens}
\ee 
where $\z_\gi$ denotes the $\gi^\text{th}$ realisation of $\Z$.
\end{defn}
You may know this as the sample average or sample mean. Note that it is an additive average: $\ave{\Z}_{\N}$ is, in effect, the value which each of the $\N$ realisations of $\Z$ can be replaced by if we want their \textit{sum} to be the same.

The finite-ensemble average is itself a random variable: each finite ensemble of size $\N$ can contain different realisations of $\Z$ which sum to a different total. As $\N\to\infty$, this random average may converge with probability one to a unique constant. If it does, we call this limiting value the ensemble average and denote it by $\ave{\Z}$.
\begin{defn}{Ensemble average}
The ensemble average of a random variable, $\Z$, is the $\N\to\infty$ limit of the finite-ensemble average,
\be
\ave{\Z} \equiv \lim_{\N\to\infty}\ave{\Z}_{\N} =  \lim_{\N\to\infty} \frac{1}{\N} \sum_{\gi=1}^\N \z_\gi,
\elabel{ens}
\ee
where $\z_\gi$ denotes the $\gi^\text{th}$ realisation of $\Z$.
\end{defn}
The limit is not guaranteed to exist, in which case $\Z$ has no ensemble average, although  finite-ensemble averages can always be computed. We will sometimes refer to the imagined infinite collection of realisations as the ``ensemble'' of $\Z$.

Note that, unlike the expectation value, the ensemble average of a random variable is not defined in terms of probabilities. It is the quantity to which averages over realisations converge as the number of realisations grows. Recall that we followed this procedure when analysing the coin game. At each round of the gamble, we found finite-ensemble averages of simulated wealth for increasingly large samples, from one to one million, plotted against time in \fref{1_2}.

It is laborious to compute averages of ever larger finite ensembles, in the hope of discerning their convergence to a limit. Fortunately, we can show, under the frequentist interpretation of probability introduced in \secref{random_variable}, that the ensemble average of a random variable is equal to its expectation value. We do this here for the discrete case, noting that a similar proof can be offered for the continuous case.
\begin{proof}
Denote by  $\gn_\gj$ the number of times $\z_\gj$ is observed in $\N$ realisations of $\Z$. The finite-ensemble average can be written as
\be
\ave{\Z}_\N =\frac{1}{\N}\sum_{\gi=1}^{\N}  \z_\gi = \sum_\gj \frac{\gn_\gj}{\N} \z_\gj,
\ee
where the subscript $\gi$ indexes a particular realisation of $\z$ and the subscript $\gj$ indexes a possible value of $\z$. Under the frequentist interpretation, the relative frequency of each possible value, $\gn_\gj/\N$, converges almost surely in the limit $\N\to\infty$ to its probability, $\p_\gj$. Thus, we find
\be
\lim_{\N\to\infty}\ave{\Z}_\N = \sum_\gj \p_\gj \z_\gj = \eval{\Z}.
\ee
\end{proof}
This result, commonly known as the \textit{law of large numbers}, is exceedingly useful. It means that we no longer need many realisations of a random variable to compute its ensemble average. Instead, if we know the probability distribution, we can compute its expectation value straightforwardly as a weighted average or integral. It also means that, from now on, we can use ensemble average and expectation value interchangeably.

\begin{history}{The invention of the expectation value}
Suppressed.
%Expectation values 
%were not invented in order to assess whether a gamble is 
%worth taking. Instead, they were developed to settle a  
%moral question that arises in the following somewhat contrived 
%context: imagine playing a game of dice with a 
%group of gamblers. The rules of the game are simple: we 
%roll the dice three times,  and whoever rolls the most points 
%gets the pot to which we've all contributed equal amounts. 
%We've already rolled the dice twice when suddenly the 
%police burst in because they've heard of our illegal gambling ring. 
%We all avoid arrest, most of us escape through the backdoor, 
%and to everyone's great relief you had the presence of mind 
%to grab the pot before jumping out of a conveniently located 
%ground-floor window. Later that day, under the cover of dusk, 
%we meet behind the old oak tree just outside of town to split 
%the pot in a fair way. But hold on -- what does ``fair'' mean here?
%Some of us had acquired more points than others in the first 
%two rolls of the dice. Shouldn't they get more? The game was 
%not concluded, so wouldn't it be fair to return to everyone his 
%wager and thank our lucky stars that we weren't arrested? 
%Should we split the pot in proportion to each player's points? 
%All of these solutions were proposed \cite{Devlin2008}.
%The question is fundamentally moral, and there is no 
%mathematical answer. But \person{Blaise Pascal}, now famous for 
%addressing theological questions using expectation values, put the 
%problem to Pierre de Fermat, and over the course of a few months' 
%correspondence (the two never met in person) \person{Fermat} and 
%\person{Pascal} agreed that fairness is achieved as follows: 
%think of all (equally likely) possible outcomes of the third round 
%of throws of the dice, call the number of all possibilities $\N$. 
%Now count those possibilities that result in player $\gj$ winning, 
%call this $\gn_\gj$. If $\q$ is the amount of money in the pot, then 
%we split the pot fairly by giving each player
% $\frac{\gn_\gj}{\N}\times \q$.
%This is $\ave{\q}$, according to \eref{exp_sum}, 
%because $\frac{\gn_\gj}{\N}=\p_\gj$ is the probability that player $\gj$ wins the
%amount \q. 
%Later researchers called this amount the ``mathematical expectation''  
%or simply ``expectation value''. But this is really an unfortunate choice 
% -- no player ``expected'' to receive $\ave{\q}$. 
%Instead, each player expected to receive either nothing or $\q$. 
\end{history}

\section{Stochastic process}
% Start by illustrating with the coin toss game
In the coin game, the wealth multiplier, $\gr$, at each round is a random variable with outcomes $\{0.6,1.5\}$ and probabilities $\{0.5,0.5\}$. Starting from $x(0)=\$1$, successive realisations of the wealth updating rule,
\be
\x(\t+\dt) = \x(\t)\gr,
\elabel{x_update}
\ee
generate a function of time, $\x(\t)$, which describes one possible evolution of wealth in the game. If we start again, generating fresh realisations of $\gr$ at each time step, we get a second function of time, almost certain to be different from the first. If run the game $\N$ times, we get a set of wealth evolution functions, $\{\x_1(\t),\dots,\x_{\N}(\t)\}$.

%SP as generalisation of RV
This situation reminds us of a random variable, except that each realisation is now a function of a parameter -- in effect, a set of real numbers -- rather than a single real number. We call such functions \textit{trajectories}. Many natural phenomena are like the coin game, in that they result not in single observations but in ordered sequences of connected observations, about whose values we are uncertain. To model such phenomena, we need a new type of mathematical object, the \textit{stochastic process}, denoted $\Y(\t)$. We define this analogously to the random variable, as:
\bi
\item a set of possible trajectories, $\{y(\t)\}$; and
\item a probability distribution over this set.
\ei
More technical presentations are available and, again, we point you to \cite[p.~52]{vanKampen1992} for greater depth. The set of trajectories may be countable and associated with a discrete set of probabilities, or uncountable and associated with a \PDFa. For simplicity, when we index trajectories, we will do so using integers, $y_\gi(\t)$ for $\gi=1,2,\dots$, even though strictly this is incompatible with the uncountable case. Furthermore, we will always interpret the parameter, $\t$, as time.

This might seem like lot to take in, but conceptually it's fairly simple. A stochastic process is nothing more than a family of trajectories from which we can select at random, according to a probability distribution. It is the generalisation of the random variable to from single numbers to functions of time. If you are already familiar with stochastic processes, this may not be how you think of them. We illustrate our perspective in \fref{sp_grid}.
\begin{figure}[h]
\begin{picture}(200,220)(20,30)
\put(-30,-20){\includegraphics[width=1.2\textwidth]{./chapter_tools/figs/sp_grid.pdf}}
\put(18,195){$y_1(\t)$}
\put(18,164){$y_2(\t)$}
\put(18,133){$y_3(\t)$}
\put(18,102){$y_4(\t)$}
\put(18,71){$y_5(\t)$}
\put(-10,159){\rotatebox{-90}{realisation, $\gi$}}  
\put(200,235){time, $\t$}  
\put(120,220){$y_1(\t^*)$}
\put(130,215){\vector(1,-1){13}}
\end{picture}
\caption{Realisations of a stochastic process, $\Y(\t)$. Each trajectory, $y_\gi(\t)$ is a function of time, which runs horizontally across the page. Fixing time, $\t=\t^*$, results in a random variable, $\Y(\t^*)$, one realisation of which, $y_1(\t^*)$, is shown.}
\flabel{sp_grid}
\end{figure}

This picture suggests another way of viewing the stochastic process: if we choose a time, $\t=\t^*$, then $\Y(\t^*)$ is a random variable, whose possible realisations form the set of possible function values, $\{y(\t^*)\}$. So, if you prefer, you can think of a stochastic process as a family of random variables, with probability distributions parametrised by time.

\boldhead{Generating stochastic processes}
Switching to practical considerations, there are two main ways of generating a trajectory of a stochastic processes, say on a computer. The first is consistent with our conceptualisation of selecting one from a family of trajectories. For example, if the possible trajectories were finitely many and equally probable, then we could label them with integers and draw one from a discrete uniform distribution. All the uncertainty is resolved at the start, in a single realisation of a random variable.

The other approach is to generate a trajectory sequentially, by successive application of the rule that tells us which possible values of $y_\gi(\t+\dt)$ are compatible with a known value of $y_\gi(\t)$. This is how we generate wealth trajectories in the coin game, where the rule is to multiply current wealth by the realisation of a random variable. Realising a random variable at every time step sounds laborious. However, in many models of natural phenomena, it is easier to write down a realistic updating rule than an expression for all possible trajectories. This is because it is often about the changes of quantities that we can reason `physically'. Moreover, many stochastic processes have uncountably many trajectories, making the single realisation approach difficult, because it requires high numerical precision, \ie very long numbers, to be useful.

Therefore, typically we generate trajectories sequentially, using updating rules which we call the \textit{dynamics} of the stochastic process. Usually, where it creates no ambiguity, we don't distinguish between the stochastic process and a single realisation of it, using lower case to denote both, \eg $\x(\t)$ for wealth. When it's important to identify different trajectories, we include an index, \eg $\x_\gi(\t)$. Sometimes we refer to these different realisations as \textit{systems} and to the index as the system number.

\section{Time average}
Stochastic processes are models of uncertain quantities which vary across systems and over time. Examples are the wealth of a player of the coin game and the total goals scored in a football match. If we fix time in a stochastic process, then we can take the ensemble average of the resulting random variable,
\be
\ave{\Y(\t^*)} = \lim_{\N\to\infty} \frac{1}{\N} \sum_{\gi=1}^{\N} y_\gi(\t^*).
\ee
The introduction of time in the stochastic process makes another type of average possible, conceptually different from the ensemble average. Instead of choosing a time and averaging across  systems, we can choose a system and average over time. Let's start by doing this for a finite time period, $\Dt$, to generate a \textit{finite-time average}.
\begin{defn}{Finite-time average}
The finite-time average of the trajectory $y(\t)$ is the average value of the function from $\t$ to $\t+\Dt$,
\be
\bar{y}_{\Dt}(\t) \equiv \frac{1}{\Dt} \int_{\t}^{\t+\Dt} y(\gs) \gd\gs.
\elabel{t_ave_f}
\ee
If $y(\t)$ changes only at discrete times, $\{\t+\dt,\dots,\t+\T\dt\}$, where $\Dt=\T\dt$, then the finite-time average can be written as 
\be
\bar{y}_{\Dt}(\t)  = \frac{1}{\T\dt} \sum_{\gtau=1}^{\T} y(\t+\gtau\dt).
\elabel{t_ave_f_disc}
\ee
\end{defn}

In the same way that the ensemble average is the many-systems limit of the finite-ensemble average, so the \textit{time average} is the long-time limit of the finite-time average.
\begin{defn}{Time average}
The time average is the long-time limit of the finite-time average,
\be
\bar{y}(t) \equiv \lim_{\Dt\to\infty} \bar{y}_{\Dt}(\t),
\elabel{t_ave}
\ee
where this limit exists.
\end{defn}
Note that both the finite-time and time averages are strictly functions of the time, $\t$, at which the averaging starts. In many of the models we consider, dependence on the initial condition vanishes in the limit of long averaging time. If so, we remove it from the notation and write $\bar{y}$.

The two types of average possible in a stochastic process are illustrated in \fref{ergodic_grid}.
\begin{figure}[h]
\begin{picture}(200,260)(0,0)
\put(-40,-30){\includegraphics[width=1.2\textwidth]{./chapter_tools/figs/ergodic_grid.pdf}}
\put(13,190){$y_1(\t)$}
\put(13,159){$y_2(\t)$}
\put(13,128){$y_3(\t)$}
\put(13,97){$y_4(\t)$}
\put(13,66){$y_5(\t)$}
\put(-25,22){realisation, $\gi$}  
\put(162,242){$\t=\t^*$}  
\put(344,235){time, $\t$}  
\put(188,207){$y_1(\t^*)$}
\put(187,208){\vector(-1,-1){15}}
\put(-27,125){$\gi=3$}
\put(362,127){$\bar{y_3} =$}
\put(340,114){$\lim_{\Dt\to\infty} \frac{1}{\Dt} \int_0^{\Dt} y_3(\gs) \gd\gs$}
\put(100,10){$\ave{\Y(\t^*)} = \lim_{\N\to\infty} \frac{1}{\N} \sum_{\gi=1}^\N y_\gi(\t^*)$}  
\end{picture}
\caption{Realisations of a stochastic process, $\Y(\t)$, \cf \fref{sp_grid}. The ensemble average, $\ave{\Y(\t^*)}$, is obtained by choosing a time, $\t=\t^*$, and averaging over trajectories (red box). The time average is obtained by choosing a trajectory, $\gi=3$, and averaging over time (green box).}
\flabel{ergodic_grid}
\end{figure}

\section{The game, revisited}
We have introduced some essential tools for studying models such as that of the coin game, where outcomes are uncertain and vary over time. It makes sense, therefore, to apply these tools to the game to see how they deepen our understanding of it.

We pretended to be mathematically clueless when we ran the simulations in \secref{The_game}, hoping to gain an understanding of the game by simply looking at many possible outcomes. We can now compute exactly the ensemble average of the stochastic process for wealth, $\x(\t)$, instead of approximating it numerically by a finite-ensemble average. We start by taking the ensemble average of the wealth updating rule in \eref{x_update},
\be
\ave{\x(\t+\dt)} = \ave{\x(\t)\gr}.
\elabel{step_1}
\ee
We assume the outcomes of coin tosses are independent of the player's wealth and of time, so that $\ave{\x(\t)\gr} = \ave{\x(\t)}\ave{\gr}$. Thus, \eref{step_1} can be written as
\be
\ave{\x(\t+\dt)}=\ave{\x(\t)}\ave{\gr}.
\ee
We can solve this recursively for the wealth after $\T$ rounds from a known starting wealth, $\x(\t_0)$:
\be
\ave{\x(\t_0+\T\dt)} = \x(\t_0)\ave{\gr}^\T.
\elabel{x_exp_r}
\ee
$\dt$ is the duration of a single round of the game, so the total playing time is $\Dt=\T\dt$.

Recall that $\gr$ is random variable with outcomes $\{0.6,1.5\}$ and probabilities $\{0.5,0.5\}$. The expectation value, $\ave{\gr}$, is, therefore,
\be
\ave{\gr} = (0.5)(0.6) + (0.5)(1.5) = 1.05.
\ee
Since this number is greater than one, the ensemble average wealth, $\ave{\x(\t)}$, grows exponentially over time by a factor 1.05 per round of the game.

It is useful to quantify this growth in a standard way, as a growth rate. We will say much more about this in \secref{Growth_rates}, since it is a central concept in ergodicity economics. For the moment, it is enough to know that the appropriate growth rate is the coefficient of time, $\g$, in the exponential function, $\exp(\g\t)$.

Writing
\be
\ave{\x(\t_0+\Dt)} = \x(\t_0) \exp(\g\Dt)
\elabel{x_exp_g}
\ee
and equating it to $\x(\t_0)\ave{\gr}^\T$ from \eref{x_exp_r}, we get the exponential growth rate
\be
\g = \frac{\T\ln\ave{\gr}}{\Dt} = \frac{\ln\ave{\gr}}{\dt}.
\ee
We aren't interested here in whether time is measured in minutes, days, or years, so for simplicity let's just measure it in rounds of the game, \ie $\dt=1\text{ round}$. This choice gives us a growth rate of $\g = \ln(1.05)/1 \approx 4.9\%$ per round.

The positive growth rate of ensemble average wealth is what might have led us to conclude that the game is worth playing, when we analysed it numerically in \secref{The_game}. \Fref{cf_exp} compares the analytical result for the ensemble, derived above, to the numerical results of \fref{1_2} for finite ensembles. We could now conclude that the case is closed. Mathematics and simulations agree there is some risk involved, but ``on average'' our wealth grows if we play the game. However, we now know that there are two ways in which we can say ``on average'': we have shown how wealth averaged over many systems evolves; let's now consider a single wealth trajectory over long time.

\begin{figure}[h]
\begin{picture}(200,200)(0,0)
\put(-100,0){\includegraphics[width=0.8\textwidth]{./chapter_tools/figs/x_of_t_lin_exp.pdf}}
\put(180,0){\includegraphics[width=0.8\textwidth]{./chapter_tools/figs/x_of_t_log_exp.pdf}}
\put(35,160){(A)}
\put(315,160){(B)}  
\end{picture}
\caption{Expectation value (thick light blue line) and finite-ensemble averages on (A) linear scales and  (B) logarithmic scales.}
\flabel{cf_exp}
\end{figure}

%In this section we validate \fref{1_3} and compute analytically what happens in the long-time limit. The blue line in \fref{1_3} is not completely smooth, there's still some noise (see panel B). It has some average slope, but that slope will vary from realisation to realisation. The longer we observe the system, \ie the more time is represented in a figure like \fref{1_3}, the smoother the line will be. In the long-time limit, $\Dt\to\infty$, the line will be completely smooth, and the average slope will be a deterministic number -- in any realisation of the process it will come out identical. 

Starting from $\x(\t_0)$ and playing for $\T$ rounds leads to wealth,
\be
\x(\t_0+\T\Dt) = \x(\t_0) \prod_{\gtau=1}^\T \gr_\gtau,
\ee
where $\{\gr_\gtau\}$ are random wealth multipliers, all distributed like $r$, and $\gtau$ indexes the round of the game. We have taken no kind of average yet and $\x(\t_0+\T\Dt)$ remains a random variable, consistent with fixing time in a stochastic process.

Let's follows our previous strategy and write this as an exponential function of time,
\be
\x(\t_0+\Dt) = \x(\t_0) \exp(\g\Dt).
\ee
Equating the two expressions and rearranging for the growth rate, we get
\be
\g = \frac{1}{\Dt} \ln\left(\prod_{\gtau=1}^\T \gr_\gtau\right) = \frac{1}{\dt} \left(\frac{1}{\T} \sum_{\gtau=1}^\T \ln\gr_\gtau\right),
\elabel{g_x_T}
\ee
where we used the unique property of the logarithm that $\ln(\alpha\beta) = \ln\alpha + \ln\beta$ to convert the product into a sum. So, wealth grows exponentially at a random growth rate, $\g$, whose distribution depends on the number of rounds played, $\T$.

What happens to $\g$ in the long run? There are two ways of evaluating the $\Dt\to\infty$ -- or, equivalently, $\T\to\infty$ -- limit of \eref{g_x_T}, both illuminating in different ways. First, we can look at the product of wealth multipliers. Consider the equivalent single multiplier that, if applied at each round, would have the same product, \ie
\be
(\reff)^\T =  \prod_{\gtau=1}^\T \gr_\gtau \quad \Rightarrow \quad \reff = \left(\prod_{\gtau=1}^\T \gr_\gtau\right)^\frac{1}{\T},
\ee
known as the \textit{geometric mean} of $\{\gr_\gtau\}$. Suppose heads appear $m$ times and tails $n$ times. Then
\be
\reff = (1.5)^\frac{m}{\T} (0.6)^\frac{n}{\T}
\ee
This is just the product of possible realisations of the wealth multiplier raised to their relative frequencies. We know what happens to relative frequencies the number of rounds grows large: they converge almost surely to their corresponding probabilities, in this case both $1/2$. So, over many rounds, the effective per round multiplier of wealth converges to
\be
\lim_{\T\to\infty} \reff = (1.5\times0.6)^\frac{1}{2} \approx 0.95.
\ee

This is a very different story from that told by ensemble average wealth. Instead of growing exponentially by a factor 1.05 per round, a single wealth trajectory over long time decays exponentially by an effective factor of approximately 0.95 per round. The corresponding growth rate, from \eref{g_x_T}, is
\be
\g = \frac{\ln\left(\reff^\T\right)}{\Dt}  = \frac{\ln\reff}{\dt} \approx -5.3\% \text{ per round}
\ee
in the $\T\to\infty$ limit.

This is consistent with our simulations in \secref{The_game}. \Fref{1_4} (B) compares the trajectory generated in \fref{1_3} to a trajectory decaying exactly at rate $\g$. The trajectory in \fref{1_3} was not a fluke: {\it every} trajectory will decay in the long run at a rate of $(\gr_1 \gr_2)^{1/2}$ per round. 
\begin{figure}[h]
\begin{picture}(200,200)(0,0)
\put(-100,0){\includegraphics[width=0.77\textwidth]{./chapter_tools/figs/x_of_t_N1M.pdf}}
\put(180,0){\includegraphics[width=0.8\textwidth]{./chapter_tools/figs/x_of_t_log_10000.pdf}}
\put(-30,120){(A)}
\put(250,120){(B)}  
\end{picture}
\caption{(A) Finite-ensemble average for $N=10^6$ and 52 time steps, the light blue line is 
the expectation value.
 (B) A single system simulated for 10,000 time steps, the light blue 
line decays exponentially with the time-average decay factor $\rt$ in each time step.}
\flabel{1_4}
\end{figure}

There are two averages, $\rex$ and $\rt$ that we have determined numerically and analytically. 
Neither average is ``wrong'' in itself; instead each average corresponds to a different property
of the system. Each average is the answer to a different question. Saying that ``wealth 
goes up, on average'' is clearly meaningless and should be countered with the question 
``on what type of average?'' 

\textbf{Expectation values are not what you expect.}

An observable that neatly captures the two different 
aspects of multiplicative growth we have illustrated is the exponential growth rate, $\gm(\ave{\x(\t)}_\N, \Dt)$ 
observed over finite time $\Dt$, in a finite ensemble
of $\N$ realisations. Exponential growth rates are ubiquitous and may be familiar, but because they are the origin of the logarithmic function, which will be important for us later on, we will intro them properly in a little excursion that will also clarify what a logarithm is.

The exponential growth rate of average wealth in an ensemble of $\N$ systems, observed over time $\Dt$ is
\be
\gm(\ave{\x(\t)}_\N, \Dt)=\frac{\D \ln \ave{\x}_\N}{\Dt},
\elabel{gest}
\ee
where the $\D$ in the numerator corresponds to the change over the $\Dt$ in the denominator. For $\N$ and $\Dt$ finite this is a random variable. The relevant scalars arise as two different limits
of the same stochastic object. The exponential growth rate of the expectation value 
(that's also $\frac{1}{\dt} \ln \ave{\gr}$) is
\be
\gm(\ave{\x})=\lim_{\N\to\infty}\gm,
\ee
and the exponential growth rate followed by every trajectory when
observed for a long time (that's also $\frac{1}{\dt} \ln \rt$) is 
\be
\gt=\lim_{\Dt\to\infty}\gm.
\elabel{gt}
\ee
We can also write \eref{gest} as a sum of the logarithmic differences in  the 
$\T$ individual rounds of the gamble that make up the time interval 
$\Dt=\T \dt$
\be
\gm(\ave{\x(\t)}_\N, \Dt)=\frac{1}{\T\dt}  \sum_{\gtau=1}^{\T} \D\ln \ave{\x(\t+\gtau\dt)}_\N.
\ee
%For a single trajectory, $\N=1$, \eref{gt} becomes
%\bea
%\t&=&\lim_{\t\to\infty}\frac{1}{\t} \ln \x(\t)\\
%&=&\lim_{\T\to\infty}\frac{1}{\T\dt} \sum_\t^\T \D \ln \x(\t).
%\eea

According to this definition, $\gt$ is the time average of 
the observable $\frac{\d \ln \x}{\dt}$. It can be shown that
the time-average growth rate of a single trajectory is the same as that
of a finite-ensemble average of trajectories,
$\lim_{\Dt\to\infty}\frac{\D \ln \x}{\Dt}=\lim_{\Dt\to\infty}\frac{\D \ln \ave{\x}_N}{\Dt}$, \cite{PetersKlein2013}. In \secref{finite_populations} we will derive this result as well as growth rates in finite ensembles and finite time.

\begin{excursion}{Scalars}
Suppressed.
%$\gr(\t)$ is a random variable, whereas both $\rex$ and $\rt$ are scalars.
%Scalars have the so-called ``transitive property'' that is heavily relied upon in 
%economic theory. Let $\ga_i$ be a set of scalars. Transitivity means that if $\ga_1>\ga_2$ and 
%$\ga_2>\ga_3$ we have $\ga_1>\ga_3$. Notice that we cannot rank random variables 
%in such a way. The ``greater than'' relation, $>$,
%is not defined for a pair of random variables, which is the mathematical
%way of saying that it is difficult to choose between two gambles, and it is why 
%we went to the trouble of removing the randomness from the stochastic process 
%$\x(\t)$. Removing randomness by
%averaging always involves a limiting process, and results are said to hold ``with probability one''. 
%In the case of $\rex$ we considered the infinite-ensemble limit, $\N\to \infty$, and 
%in the case of $\rt$ we considered the infinite-time limit, $\Dt\to\infty$. If we use the scalars 
%$a_i$ to represent preferences, we can test for consistency among preferences. 
%For instance, in such a model world where preferences are represented by scalars, 
%the facts that ``I prefer kangaroos to Beethoven'' and ``I prefer mango chutney to kangaroos'' 
%imply the fact ``I prefer mango chutney to Beethoven''. Translating back to reality, 
%economists like to call individuals who make the first two statements but not the 
%third ``irrational.'' 
%
%Because transitivity makes for a
%nicely ordered world, it is useful to find scalars to represent preferences.
%We are skeptical about the attempt to map all preferences
%into scalars because  the properties of mango chutney are too different, {\it qualitatively},
%from the properties of Beethoven. We will restrict our analysis to money --
%the amount of money we will receive is random and this introduces
%a complication, but at least we know how to compare one amount to 
%another in the limit of no randomness -- there is no qualitative differences 
%between $\$1$ and $\$3$, only a quantitative difference. 
%
%Both $\rex$ and $\rt$ are scalars, and both are therefore potentially powerful 
%representations of preferences. Your decision whether to accept our gamble could
%now be modelled as a choice between the value of the scalar $\rt$ if you do not
%accept our game, namely $\ga_1=1$, and the value of the scalar $\rt$ if you do accept, 
%namely approximately $\ga_2=0.95$. In this model of your decision-making you 
%would prefer not to play because $1>0.95$.
\end{excursion}

\begin{history}{William Allen Whitworth}
Suppressed.
%$\rex$ and $\rt$ are two different properties of the game. $\rex$ is 
%the large-ensemble limit, $\rt$ is the long-time limit, of wealth growth it induces. The Victorian 
%mathematician William Allen Whitworth postulated $\rt$ as the relevant property 
%for an individual deciding whether to take part in a repeated gamble. 
%He used this knowledge to write an appendix  entitled ``The disadvantage 
%of gambling'' to the 1870 edition of his book ``Choice and Chance'' 
%\cite{Whitworth1870}. He phrased his argument in terms of the difference of two squares. 
%Imagine that you either win or lose, with equal probability, 
%an amount $\epsilon \x(\t)$ in each round of a game. In the long run, positive and 
%negative changes will occur equally frequently, and to determine
%the overall effect we just need to consider the effect of one positive and one negative change
%in a row. Over one up and one down-move wealth changes by the factor
%\be
%(1+\epsilon)(1-\epsilon)=1-\epsilon^2.
%\elabel{Whitworth}
%\ee
%This factor is clearly less than one, meaning that what's often called a ``fair gamble'' -- one 
%that does not change the expectation value of the gambler's wealth -- leads to an 
%exponential decay of his wealth over time. Hence the title of the appendix 
%``The disadvantage of gambling.'' We will see in \secref{Geometric_Brownian} that Whitworth's work
%captured the essence of \Ito's famous 1944 discovery \cite{Ito1944} that was to form the basis of
%much of financial mathematics.
%
%Whitworth was arguing against a dogma of expectation values of wealth, that had 
%been established almost immediately following Fermat and Pascal's work. He 
%hoped to show mathematically that gambling may not be a good idea even if 
%the odds are favourable, and was a 
%proponent of the notion that commerce should and does
%consist of mutually beneficial interactions rather than one winner and one loser. 
%In the end his voice was not heard in the economics community. His main career was as a priest at All Saints Church in London's Margaret Street, only a 22 minute stroll away from the (first office of the) London Mathematical Laboratory, according to Google \fref{all_saints}.
\end{history}
%\begin{figure}[h!]
%\begin{picture}(200,270)(0,0)
%  \put(-30,0){\includegraphics[width=1.15\textwidth]{./chapter_tools/figs/all_saints.pdf}}
%\end{picture}
%\caption{Location of All Saints Church and the London Mathematical Laboratory's initial office.}
%\flabel{all_saints}
%\end{figure}
%\FloatBarrier

\begin{excursion}{Compounding growth, exponentials, and the logarithm}
Suppressed.
%The logarithm is a relatively recent mathematical discovery, made some time in the 1590s by John Napier, the 8th Laird of Merchiston in Scotland. Let's say you lend me some money, $\x(\t)$, for one year, $\Dt=1$~year, and I have to pay interest on the loan. We agree a yearly interest rate of $r_{1}=5\%$ per year. 
%
%After one year the amount I have to repay is
%\bea
%\x(\t+\Dt)=\overbrace{\x(\t)}^{\text{Principal}}+ \overbrace{\D \x}^{\text{Interest}}.
%\eea 
%I could convert the interest payment into a rate, $r_1$, so that $r_1 \x(\t)$ is the rate (dollars per time) at which I have to make constant payments to you for one year, so that you end up with the right amount of interest at the end of the year, and
%\be
%\x(\t+\Dt)=\x(\t)r_1 \Dt
%\elabel{deal}
%\ee
%This makes it easy, for instance, to find something to enter into my accounts if I have to say after 6 months how much I owe you at that moment. I would say: only half a year has passed, so I only owe you half the interest, 
%\be
%\x(\t+\Dt/2)=\x(\t)\left(1+r_1 \frac{\Dt}{2}\right). 
%\elabel{deal_2}
%\ee
%But if this is really the amount I owe you after 6 months, surely I should pay interest on this new amount for the following 6 months. We will see that this leads to a problem. Substituting $\x(\t+\Dt/2)$ for $\x(\t)$ on the \RHS of \eref{deal_2} gives
%\bea
%\x(\t+\Dt)&=&\x(t+\Dt/2)\left(1+r_1 \frac{\Dt}{2}\right)\\
%&=&\x(\t)\left(1+r_1\frac{\Dt}{2}\right)^2
%\eea
%Equating to \eref{deal} we find a contradiction
%\bea
%\x(\t) (1+ r_{\Dt} \Dt)&\stackrel{?}{=}&\x(\t)(1+r_{\Dt}/2)^2\\
%\implies 1+ r_{\Dt} \Dt&\stackrel{?}{=}&1+r_{\Dt}\Dt+\left(\frac{r\Dt}{2}\right)^2 \text{{\color{red} \huge \lightning}}.
%\eea
%Strange as it may seem, the solution to this problem is to acknowledge that interest rates depend on the time scale at which they're defined -- the compounding time scale. We can fix the problem by introducing $r_2$ -- the semi-annual interest rate (the subscript 2 indicates that we've split the original interval in 2 equal parts). It is defined by
%insisting that the one-step and two-step computations give the same interest payment $\D\x$, 
%\be
%1+ r_1 \Dt=\left(1+r_2 \frac{\Dt}{2}\right)^2.
%\ee
%There's nothing stopping us from writing down the general expression for any number, $\T$, of intermediate stock-takings
%\bea
%\frac{\x(\t+\Dt)}{\x(\t)}&=&\left(1+r_\T\frac{ \Dt}{\T}\right)^\T\\
%&\text{which implies}&\\
%r_\T&=& \frac{1}{\Dt}\T \left\{\left[\frac{\x(\t+\Dt)}{\x(\t)}\right]^{1/\T}-1\right\}
%\eea
%A common trick to remove the dependence of some quantity ($r_{\T})$ on another ($\T$)  is to let the control variable diverge. The limit no longer depends on the diverging quantity (we've seen this trick before: the expectation value doesn't depend on the ensemble size, which has diverged).
%\be
%r_\infty= \frac{1}{\Dt} \underbrace{\lim_{\T\to\infty}\T \left\{\left[\frac{\x(\t+\Dt)}{\x(\t)}\right]^{1/\T}-1\right\}}_{\ln\left(\frac{\x(\t+\Dt)}{\x(\t)}\right)}.
%\ee
%Note the procedure here: we keep the total time interval, $\Dt$, fixed and split it into $\T$ ever more numerous and shorter sub-intervals $\dt$.
%Because it's tedious to write down the long expression involving the limit, we define it as a new function, called the logarithm, $\ln(\cdot)$, as indicated by the underbrace.
%\be
%\ln \ga :=  \lim_{\T\to\infty}\T \left\{\ga^{1/\T}-1\right\}.
%\ee
%The logarithm has a property that makes it uniquely suited for characterizing multiplicative processes: the logarithm of a ratio is the difference of the logarithms of numerator and denominator, 
%\be
%\ln \frac{\ga_2}{\ga_1}=\ln \ga_2 - \ln \ga_1 = \D \ln \ga.
%\ee
%The inverse function of the logarithm is the exponential (by definition), denoted $\exp(\cdot)$, so that $\exp(\ln \ga)=\ga$, and the limiting growth rate $r_\infty$ is called the logarithmic or exponential growth rate, which we also denote by $\gm$ (subscript ''m'' for ``multiplicative'').
%
%Just to be sure we got this across: these are the definitions of the logarithm and the exponential. You may know many properties of logarithms and exponentials -- all of them can be derived from the definitions we have just presented.
\end{excursion}

\begin{excursion}{Dimensional analysis}
Suppressed.
%We will often and without qualm write the expression $\D \ln \x$. Dimensional
%analysis suggests to think about this expression carefully, at least once. This may
%seem pedantic but the absence of this pedantry has caused sufficient
%confusion in economic theory for us to risk antagonizing you. ``Dimension'' in this context is closely related to the 
%concept of ``unit'' -- for instance, a dollar is a money unit, and the dimension function
%for money tells us how to convert from one currency into another. Similarly, length may 
%have the unit ``meter'', and the dimension function for length tells us how to convert between 
%different systems of units, such as meters and yards. 
%We can only point to the subject here and recommend the book
%by \person{Barenblatt} for a comprehensive treatment \cite{Barenblatt2003}.
%Dimensional analysis is a deeply fascinating and powerful tool that every physicist 
%is drilled to use at all times. \person{Taylor} famously used it to compute the energy
%released by an early nuclear explosion at the Trinity site near Alamogordo, New Mexico, based on 
%some grainy pictures published by Life magazine, at least that's the legend 
%\cite{Taylor1950b,Deakin2011}. Fluid dynamicists in general use it to find meaningful quantities 
%to distinguish different types of flow. In many problems involving random walks dimensional 
%analysis immediately reveals scaling properties, supposed solutions to many problems can
%be seen at a glance to be wrong, and, conversely some complicated-looking 
%problems can be solved as if by magic just by appealing to dimensional analysis.
%
%\person{Barenblatt} shows in his book that the dimension function must be a (scale-free) power-law 
%monomial if there is to be no distinguished system of units. We can all agree that the unit
%of money is physically irrelevant -- I can do exactly the same with the pennies in my 
%bank account as I can do with the pounds those pennies correspond to. Since this is so, 
%for functions of monetary amounts to be physically meaningful we want them to be 
%power-law monomials. An amount of square-dollars, $\$^2$, may be meaningful, but an
%amount of logarithmic or exponential dollars cannot be meaningful. Hence $\ln(\x)$ on its own
%is just some symbol spat on a page by a printer, but it has no physical meaning.
%The reason we're comfortable writing $\D \ln \x$ is the unique property of the logarithmic function
%\be
%\ln \x_1 - \ln \x_2 = \ln\left(\frac{\x_1}{\x_2}\right).
%\ee
%The quantity in brackets on the \RHS is always dimensionless, it's a pure number because
%the dimension functions of two different values of $\x$ always cancel out. So do the units:
%$\$1/\$2=1/2$, which is a pure number without units. We will see that indeed only differences 
%in logarithms of $\x$ will appear in these lecture notes or in any other reasonable lecture notes. 
%Pedantically, we would refuse to write $\D \ln(\x)$ and insist on writing $\ln\left(\frac{\x_1}{\x_2}\right)$.
%Since the first notation is shorter and one can make formal arguments for its validity, we are 
%happy to use it here. 
%
%The issue is related to a result obtained by \person{von Neumann} and 
%\person{Morgenstern} in their 
%famous
%%but quite unhelpful
%book \cite{vonNeumannMorgenstern1944}: 
%only differences in utility functions can have physical meaning. 
%We will have a lot more to say about utility functions (which we call ergodicity mappings) in \cref{Rates}.
\end{excursion}

\section{Ergodicity}
\seclabel{Ergodic_observables}

We have encountered two types of averaging -- the ensemble average and the
time average. In our case -- assessing whether it will be good for you to play our 
game, the time average is the interesting quantity because it tells you what happens
to your wealth as time passes. The ensemble average is irrelevant 
because you do not live your life as an ensemble of many yous who can average
over their wealths. Whether you like it or not, you will experience yourself owning 
your own wealth at future times; whether you like it not, you will never experience
yourself owning the wealth of a different realization of yourself. The different realizations,
and therefore the expectation value, are fiction, fantasy, imagined.

We are fully aware that it can be counter-intuitive that with probability one, a different
rate is observed for the expectation value than for any trajectory over time. It sounds
strange that the expectation value is completely irrelevant to the problem. A reason
for the intuitive discomfort is history: since the 1650s we have been trained to
compute expectation values, with the implicit belief that they will reflect what happens
over time. It may be helpful to point out that all of this trouble has a name that's well-known
to certain people, and that an entire field of mathematics is devoted to dealing with
precisely this problem. The field of mathematics is called ``ergodic theory.'' It emerged
from the question under what circumstances the expectation value is informative 
of what happens over time, first raised in the development of statistical mechanics by Maxwell and 
Boltzmann starting in the 1850s. These lecture notes are our attempt to use precisely the insights of these physicists to re-develop economic theory from the foundations up.

\begin{history}{Randomness and ergodicity in physics}
Suppressed.
%The 1850s were about 200 years after \person{Fermat} and \person{Pascal} introduced expectation 
%values into the study of random systems. Following the success of \person{Newton}'s 
%laws of motion, established around the same time as the expectation value, 
%the notion of ``proper science'' had become synonymous with 
%mechanics. Mechanics had no use for randomness and probability 
%theory, and the success of mechanics was interpreted as a sign that 
%the world was deterministic and that sooner or later we would understand 
%what at the time still seemed random. At that point probability theory would 
%become obsolete. 
%
%When \person{Boltzmann} hit upon the ingenious idea of introducing randomness into 
%physics, to explain the laws of thermodynamics in terms of the underlying 
%dynamics of large numbers of molecules, he was fighting an uphill battle. 
%Neither molecules nor randomness were much liked in the physics 
%community, especially in continental Europe, right up until the publication of 
%\person{Einstein}'s 1905 paper on diffusion \cite{Einstein1905}. \person{Boltzmann} had to be 
%more careful than \person{Fermat} and \person{Pascal}. He had to pre-empt
%predictable objections from his peers, and the question of ergodicity had to be 
%answered -- the usefulness of probability theory relies heavily on expectation 
%values, but as we have seen, they are averages over imagined future states of the 
%universe. \person{Boltzmann}'s critics were aware of this and were not shy to voice their
%concerns. Under what circumstances are expectation values meaningful? 
%\person{Boltzmann} gave two answers. 
%\bi
%\item expectation values are meaningful when 
%the quantity of interest really is an average (or a sum) over many approximately 
%independent systems. An average over a finite ensemble will be close to the 
%expectation value if the ensemble is large enough. 
%\item expectation values 
%are meaningful, even if only a single system exists, if they reflect what happens over time. 
%\ei
%
%\person{Boltzmann} called a system ``ergodic\footnote{The word ``ergodic'' was coined by \person{Boltzmann}. 
%He initially proposed the word ``monodic'', from Greek $\mu o \nu o$ (unique) + $o\delta o \varsigma$ (path) suggesting 
%that a single path when followed for a sufficiently long time will explore all there is to explore and reflect what happens 
%in an ensemble. The term ``ergodic'' refers to the specific system \person{Boltzmann} was considering, namely an energy 
%($\epsilon \rho \gamma o \nu$) shell across which a path is being traced out.}'' if the possible 
%states of the system could be assigned probabilities in such a way that
%the expectation value of any observable with respect to those probabilities would 
%be the same as its time average with probability 1.
%
%Our setup requires us to be more modest, and we will speak of specific ergodic observables (not of ergodic systems) if their ensemble and time averages are the same.
\end{history}

To convey concisely that we cannot use the expectation value and the 
time average interchangeably in our game, we would say ``the observable $\x$ is not ergodic.'' 

\begin{defn}{Ergodic property}
In these notes, an observable $\A$ is called ergodic if its 
expectation value is constant in time
%, $\frac{\gd\ave{\A}}{\gd\t}=0$, 
and its time average converges to this value with probability one\footnote{Some researchers would call $\A$ ``mean ergodic'' and require further observables derived from it to be (mean) ergodic in order to call $\A$ ``wide-sense ergodic.'' This extra nomenclature is not necessary for our work, but we leave a footnote here to avoid confusion.}

\be
\lim_{\Dt \to\infty}\frac{1}{\Dt } \int_{\t}^{\t+\Dt} \A(\gs) \gd\gs =\lim_{\N\to\infty} \frac{1}{\N}\sum_\gi^\N \A_\gi(\t) .
\elabel{def_ergodic}
\ee
\end{defn}
The \RHS of \eref{def_ergodic} is evaluated at time $\t$, and unlike the \LHS could be a function of time. For now, we restrict our definition of ergodicity to a setup where that is not the case, \ie where the ergodic property holds at all times. In \secref{RGBM_moments} we will discuss transient behavior, where the distribution of $\A$ is time dependent. We then also consider an observable ``ergodic'' if its expectation value only converges to the time-average in the $\t\to\infty$ limit.

In terms of random variables, $\Z$, and stochastic processes, $\Y_{\Z}(\t)$, the ergodic property can be
visualized as in \fref{ergodic_grid}. Averaging a stochastic process over time or over the ensemble
are completely different operations, and only under very rare circumstances (namely under ergodicity) can the two operations be interchanged. In our coin-tossing game the operations are clearly not interchangeable. An implicit assumption of interchangeability in the early days is the Original Sin of economic theory.

We stress that in a given setup, some observables may have the ergodic property even
if others do not. Language therefore must be used carefully. Saying our game is non-ergodic
really means that some key observables of interest, most notably wealth $\x$, are
not ergodic. Wealth $\x(\t)$, defined by \eref{law}, is clearly not ergodic -- with $\A=\x$ the \LHS of \eref{def_ergodic} 
is zero, and the \RHS is not constant in time but grows. The expectation value $\ave{\x}(\t)$
simply doesn't give us the relevant information about the temporal behavior of $\x(\t)$.
 
This does not mean that no ergodic observables exist that are related
to $\x$. Such observables do exist, and
we have already encountered two of them. In fact, we will encounter a particular type
of them frequently -- in our quest for an observable that tells us what happens over
time in a stochastic system we will find them automatically. However, again, the issue
is subtle: an ergodic observable may or may not tell  us what we're interested in.
It may be ergodic but not indicate what happens to $\x$. For example, 
the multiplicative factor $\gr(\t)$ is an 
ergodic observable that reflects what happens to the expectation value of $\x$, 
whereas per-round changes in the logarithm of wealth, $\d \ln \x = \ln \gr$, are also ergodic 
and reflect what happens to $\x$ over time.

\vspace{.3cm}
\underline{Proposition:} $\gr(\t)$ and $\d \ln \x$  are ergodic for the wealth dynamic defined by \eref{law} and \eref{gamble}.

\begin{proof}

According to \eref{ens} and \eref{f_ens}, the expectation value of $r(t)$ is
\be
\ave{\gr}=\lim_{\N\to\infty} \frac{1}{\N} \sum_\gi^\N \gr_\gi,
\elabel{e_r}
\ee
and, according to \eref{t_ave_f_disc}, the time average of $\gr(\t)$ is
\be
\tave{\gr}=\lim_{\T\to\infty} \frac{1}{\T} \sum_\gtau^\T \gr_\gtau,
\elabel{t_r}
\ee
where we have written $\gr_\gtau = \gr(\t+\gtau\dt)$ to make clear the equivalence between the two expressions. The only difference is between the labels we have chosen
for the dummy variable ($\gi$ in \eref{e_r} and $\gtau$ in \eref{t_r}). Clearly, the 
expressions yield the same value. 

The same argument holds for $\d \ln \x$.
\end{proof}

Whether we consider \eref{t_r} an average over 
time or over an ensemble is only a matter of our choice of words. 

The expectation value $\ave{\d \ln \x}$ is important, historically. \person{Daniel Bernoulli} noticed in 
1738 \cite{Bernoulli1738} that people tend to
optimize $\ave{\d \ln \x}$, whereas it had been assumed that they should optimize $\ave{\d \x}$. 
Unaware of the issue of ergodicity (200 years before the concept was discovered 
and the word was coined), \person{Bernoulli} had no good explanation for this 
empirical fact and simply stated that people tend to behave as though they valued
money non-linearly. We now know what is actually going on: multiplicative dynamics
are a fairly realistic model for real wealth, and under those dynamics
$\d \x$ is not ergodic, 
and $\ave{\d \x}$ is of no interest -- it doesn't tell us what happens over time. However,
$\d \ln \x$ {\it is} ergodic, and $\ave{\d \ln \x}$ does tell us what happens to $\x$ over time, 
wherefore seeing people optimise $\ave{\d \ln \x}$ just means seeing them optimise wealth 
over the one trajectory that describes a financial life, rather than across the ensemble of possibilities.

% repetition
%When the foundations of economic theory were laid, specifically in \person{Bernoulli}'s 
%seminal paper of 1738 \cite{Bernoulli1738}, the distinction between ergodic and
%non-ergodic observables was unknown. Researchers thought that the expectation value
%of $\d \x$ reflected what happens over time but observed that real people behaved
%according to what the expectation value of $\d \ln \x$ would suggest. While the origin of the 
%discrepancy remained mysterious and numerous puzzles and paradoxes in 
%economic theory arose as a result. The paradigm we outline here resolves these puzzles.

Ergodicity is not the same concept as stationarity. As an illustration of the difference, consider the following process: $\gf(\t)=\z_{\gi}$, where $\z_{\gi}$ is an instance of a random variable $\Z$. Explicitly, this means a realisation of the stochastic process $\gf(\t)$ is generated as follows: we generate the random instance $\z_{\gi}$ once, and then fix $\gf(\t)$ at that value for all time. The distribution of $\gf(\t)$ is independent of $\t$ and in that sense $\gf(\t)$ is stationary. But it is not ergodic: averaging over the ensemble, we obtain $\ave{\gf(\t)}=\ave{\z}$, whereas averaging over time in the $\gi^{\text{th}}$ trajectory gives $\overline{\gf}=\z_{\gi}$. Thus the process is stationary but not ergodic.

\section{Changes and stability}
\seclabel{Rates}
Deleted -- the material now appears in the next chapter. However, we must say enough about rates to consider the growth rates of \BM and \GBM.
%In this section we discuss the role of changes and stability in science in general and then say a few words about the coin toss in particular. 
%If the latter part is hard to understand right now -- don't worry, we'll get back to it in much more detail in \cref{decisions}.
%The ergodic observable $\d \ln \x$, identified in the previous section, is almost a rate.
%Dividing it by the duration of the gamble, we obtain exactly the exponential 
%growth rate of $\x$, namely $\frac{\d \ln \x}{\dt}$. Finding good growth rates 
%will be important, wherefore we now discuss the notion of a rate and 
%the notion of time independence. 
%To do this properly let's think about the basic task of science. This may be described as the 
%search for stable structure. Science attempts to build models of the world 
%whose applicability does not vary over time. This doesn't mean that the world doesn't change, 
%but the way in which the models describe change does not change. ``Children grow up to be adults'' is a 
%description of a change that has been true for a long time. The model identifies
%something stable. This is implied by the fact that we can write 
%equations (or English sentences) in ink on paper, with the equation (or sentence) remaining 
%useful over time. The ink won't change over time, so
%if an article written in 1905 is useful today then it must describe something that hasn't 
%changed in the meantime. These ``somethings'' are often somewhat 
%grandiosely called laws of nature.
%
%\person{Newton}'s laws are a good illustration of this. They are part of mechanics, meaning that they
%are an idealized mathematical model of the behavior of positions, time, and masses (by the way, this definition of 
%mechanics in terms of base quantities is a neat application of dimensional analysis \cite{Barenblatt2003}). 
%For instance, \person{Newton}'s second law, $\NF=\Nm \frac{\gd^2 \Nx}{\gd\t^2}$, states that the mass multiplied by 
%the rate of change of the rate of change of its position equals the force. The law is an unchanging 
%law about positions, time, and masses, but it does not say that positions don't change, it doesn't even say 
%that rates of change of positions don't change. It does say that the rate of change of the 
%rate of change of a position remains unchanged so long as the force and the mass 
%remain unchanged. \person{Newton}'s deep insight was to transform an unstable thing -- the position of a mass --
%until it became stable: he fixed the force and considered rates of changes of rates of changes, et 
%voil\'a!, a useful equation could be written down in ink, remaining useful for 350 years so far.
%
%Like \person{Newton}'s laws (a mathematical model of the world), our game is a prescription of changes. 
%Unlike \person{Newton}'s laws it's stochastic, but it's a prescription of changes nonetheless. 
%The multiplicative aspect of our game makes it also a powerful mathematical model of the world, as we shall see in subsequent lectures. 
%
%We're very much interested in changes of $\x$ -- we want to know 
%whether we're winning or losing -- but changes in $\x$ are not stable. 
%Under the rules of the game the rate of change of wealth, $\frac{\d \x(\t)}{\dt}$, is a different 
%random variable for each $\t$ because it is proportional to $\x(\t)$. But not to worry, 
%in \person{Newton}'s case changes in the position are not stable either, even in a 
%constant force field. Nonetheless \person{Newton} found a useful stable property. 
%Maybe we can do something similar. We're looking for a function $\gv(\x)$ that satisfies two conditions: 
%it should indicate what happens to $\x$ itself, and its random changes should be instances of a time-independent random variable.
%
%The first condition is that $\gv(\x)$ must tell us 
%whether $\x(\t)$ is growing or shrinking -- this just means that $\gv(\x)$ has to 
%be monotonic in $\x$. We know that there is something time-independent
%about $\x$ because we were able to write down in ink how $\x$ changes. So we only need 
%to find the monotonic function of $\x$ whose additive changes inherit the time-independence 
%of the ink in \eref{law}. The game is defined by a set of factors of change in $\x(\t)$, \eref{gamble}. 
%Therefore, the fractional change in $\x$, namely
%$\gr(\t) = \frac{\x(\t+\dt)}{\x(\t)}$, comes from a time-independent distribution. Which 
%function responds additively to a multiplicative change in its argument? 
%The answer is the logarithm, \ie
%only the logarithm satisfies
%\be
%\gv[\x(\t+\dt)]-\gv[\x(\t)]=\gv \left(\frac{\x(\t+\dt)}{\x(\t)}\right)
%\ee
%and we conclude that for our game $\gv(\x)=\ln\x$.
%For multiplicative dynamics, \ie if $\frac{\x(\t+\dt)}{\x(\t)}$ is ergodic, the expectation 
%value of the rate of change of the logarithm of $\x(\t)$ determines whether the game is long-term profitable 
%for an individual.
%
%More generally, when evaluating a gamble that is represented as a stochastic process, it seems that people's intuitive
%choices roughly maximise appropriate long-time growth rates.
%Mathematically speaking, they
%\begin{enumerate}
%\item
%find a monotonically increasing function $\gv[\x(\t)]$ such that $\frac{\d \gv[\x(\t)]}{\dt}$ 
%is ergodic, over time taking values of instances of a random variable.
%\item
%compute the expectation value of $\frac{\d \gv[\x(\t)]}{\dt}$. If this is positive then $\x(\t)$
%grows in the long run, if it is negative then $\x(\t)$ decays.
%\end{enumerate}
%
%The mathematics of this procedure is discussed in detail in \cref{Rates}, and an
%experiment testing whether people behave as predicted is described in \secref{Copenhagen}.

\section{Normal distribution}
\seclabel{Normal_distribution}

\section{Brownian motion}
\seclabel{Brownian_motion}
We motivate the model called \BM as a limiting process, the continuous-time limit, that arises from random walks.
In the previous section we established that the discrete increments of the logarithm of 
$\x$, which we called $\gv$, are instances of a time-independent random variable in our game. A quantity 
making such random steps over time is said to perform a ``random walk.'' 
Indeed, the blue line for a single system in \fref{1_2} (B) shows 52 steps of a random walk trajectory.
Random walks come in many forms -- in all of them $\gv$ changes discontinuously by an amount 
$\d \gv$ drawn from a time-independent distribution, over time intervals which may be regular or which may be drawn from a time-independent distribution themselves.

We are interested only in the simple case where $\gv$ changes at regular intervals, $\dt, 2\dt, \dots$. For 
the distribution of increments we only insist on the existence of the variance, meaning we insist that 
$\var(\d \gv)=\ave{\d \gv^2}-\ave{\d \gv}^2$ be finite. Increments whose distributions are heavier-tailed do 
not lead to \BM (\BM has continuous paths, and that continuity 
is broken by such increments).

The change in $\gv$ after a long time is the sum 
of many independent increments, 
\be
\gv(\t+\T\dt)-\gv(\t)=\sum_\gi^\T \d \gv_\gi.
\ee
The Gaussian central limit theorem tells us that such a sum will become 
Gaussian-distributed as we add more terms to the sum and re-scale it appropriately, namely so as to keep the width of the distribution finite and  remove any systematic drift,
\be
\lim_{\T\to\infty} \overbrace{\frac{1}{\sqrt{\T}}}^{\mathclap{\text{keep width finite}}}\sum_\gi^\T ( \d \gv_\gi \underbrace{-\ave{\d \gv}}_{\mathclap{\text{remove systematic drift}}}) \sim \mathcal{\N}(0 , \var(\d \gv)),
\elabel{CLT}
\ee
where we call $\frac{\ave{\d \gv}}{\dt}$ the ``drift term.'' The notation $\sim \mathcal{N}(0, \var(\d \gv))$ is short-hand for ``is Gaussian distributed, with mean $0$ and variance $\var(\d \gv)$.''
The logarithmic change in the 
long-time limit that was of interest to us in the analysis of the coin toss game is thus 
Gaussian distributed. 

Let's also ask about the re-scaling that was applied in \eref{CLT}. 
Scaling properties are very robust, and especially the scaling  
of random walks for long times will be useful to us. 

We work with the
simplest setup: at time zero we start at zero, $\gv(0)=0$, and in each time step, we either increase or 
decrease $\gv$ by 1, with probability 1/2. To avoid notation clutter, we'll set the duration of a time step to $\dt=1$, so that $\T$ is both the number of steps and the time.

We are interested in the variance of the distribution of $\gv$ as $\T$ increases, which we obtain by
computing the first and second moments of the distribution. 

The first moment (the expectation value) of $\gv$ is $\ave{\gv}(\T)=0$, by symmetry for all times. 

We obtain the second moment by induction\footnote{The argument is nicely illustrated in \cite[Volume 1, Chapter 6-4]{Feynman1963}, 
where we first came across it.}:
Whatever the second moment, $\ave{\gv(\T)^2}$, is at time $\T$, we can write down its value at
time $\T+1$ as 
\bea
\ave{\gv(\T+1)^2}&=&\frac{1}{2}\left[\ave{(\gv(\T)+1)^2}+\ave{(\gv(\T)-1)^2}\right]\\
&=&\frac{1}{2}\left[\ave{\gv(\T)^2+1+2\gv(\T)}+\ave{(\gv(\T)^2+1-2\gv(\T))}\right]\\
&=&\ave{\gv(\T)^2}+1.
\eea
In addition, we know the initial value of $\gv(0)=0$. By induction it follows that the second moment is
\be
\ave{\gv(\T)^2}=T
\ee
and, since the first moment is zero, the variance is
\be
\var(\gv(\T))=T.
\elabel{BM_var}
\ee
The standard deviation -- the width of the distribution -- of changes in a quantity 
following a random walk thus scales as the square-root of the number of steps 
that have been taken, $\sqrt{\T}$. 

This square-root behaviour leads to many interesting
results. It can make averages stable (because $\sqrt{\T}/\T$ converges to zero for large $\T$), 
and sums unstable (because $\sqrt{\T}$ diverges for large $\T$). Consequently, we may expect that as the size of some system increases, some properties become stable and others unstable.

Imagine simulating a single long trajectory of $\gv$ and plotting it on paper\footnote{This argument is
inspired by a colloquium presented by Wendelin Werner in the mathematics department of Imperial 
College London in January 2012. 
Werner started the colloquium with a slide that showed a straight horizontal line and asked: what is this? 
Then answered that it was the trajectory of a random walk, with the vertical and horizontal axes scaled equally.}. 
The amount of 
time that has to be represented by a fixed length of paper increases linearly with the simulated time
because the paper has a finite width to accommodate the horizontal axis. 
If $\ave{\d \gv} \neq 0$ then the amount of variation in $\gv$ that has to be represented by a fixed
amount of paper also increases linearly with the simulated time. However, the departures of $\D\gv$ from
its expectation value $\T \ave{\d \gv}$ only increase as the square-root of $\T$. Thus, the 
amount of paper-space given to these departures scales as $\T^{-1/2}$, and for very long simulated
times the trajectory will look like a straight line on paper.

In an intermediate regime, fluctuations will still be visible but they will also be approximately 
Gaussian distributed. In this regime it is often easier to replace the random walk model 
with the corresponding continuous process. That process -- finally -- is \BM. 

We think of \BM
as the limit of a random walk where we shorten the duration of a step $\dt \to 0$, and 
scale the width of an individual step so as to maintain the random-walk scaling of the variance, meaning
$|\d\gv|=\sqrt{\dt}$. In the limit $\dt\to0$, this implies that the local slope of a \BM trajectory diverges, 
$\frac{\d\gv}{\dt}\to\infty$. This means that \BM trajectories 
are infinitely jagged, or -- in mathematical terms -- they are not differentiable. However, the way in
which they become non-differentiable, through the $\sqrt{\dt}$ factor, just leaves the 
trajectories continuous (this isn't the case for $|\d\gv|=\dt^\alpha$, where $\alpha$ is less than 0.5). 

Continuity of $\gv$ 
means that it is possible to make the difference $|\gv(\t)-\gv(\t+\epsilon)|$ arbitrarily small by choosing
$\epsilon$ sufficiently small. Trajectories (of non-\BM processes) that don't have this property contain what 
are appropriately called ``jumps.'' Continuity therefore means that there are no jumps. These subtleties make \BM 
a topic of great mathematical interest, and many books have been written about it. We will pick from 
these books only what is immediately useful to us. To convey the universality of \BM we define it formally as follows:

\begin{defn}{Brownian motion i}
If a stochastic process has continuous paths, stationary independent increments, and is distributed according to 
$\mathcal{\N}(\gmu \t, \gsigma^2 \t)$ then it is a Brownian motion.
\end{defn}

The process can be defined in different ways. Another illuminating definition is this:

\begin{defn}{Brownian motion ii}
If a stochastic process is continuous, with stationary independent increments, then the process is a Brownian 
motion.
\end{defn}

We quote from \cite{Harrison2013}: {\it ``This beautiful theorem shows that Brownian motion can actually be defined
by stationary independent increments and path continuity alone, with normality following as a consequence 
of these assumptions. This may do more than any other characterization to explain the significance of 
Brownian motion for probabilistic modeling.''}

Indeed, \BM is not just a mathematically rich model but also -- due to its emergence through the Gaussian 
central limit theorem -- a model that represents a large universality class, 
\ie it is a good description of what happens over long times 
in many other models that produce random trajectories. 

The power of \BM lies in its simplicity and analytic tractability, involving only two parameters, $\gmu$ and $\gsigma$.
We will often work with its representation as a \SDE
\be
\gd\gv=\gmu \gd\t + \gsigma \gd\gW
\elabel{BM_dx}
\ee
where $\gd\gW$ is the so-called ``Wiener increment,'' the beating heart of many \SDEs. The Wiener increment can be defined by two properties: its distribution and its auto-correlation, 
\bea
\gd\gW &\sim& \mathcal{\N}(0,\gd\t)\\
\ave{\gd\gW(\t) \gd\gW(\t')}&=&\gd\t~ \gdelta(\t,\t'),
\eea
where $\gdelta(\t,\t')$ is the Kronecker delta -- zero if its two arguments differ ($\t\neq \t'$), and one if 
they are identical ($\t=\t'$).\footnote{Physicists often write $\gd\gW=\geta \gd\t$, where $\ave{\geta}=0$ and 
$\ave{\geta(\t) \geta(\t')}=\gdelta(\t-\t')$, in which case $\gdelta(\t-\t')$ is the Dirac 
delta function, defined by the integral $\int_{-\infty}^{\infty} \gf(\t) \gdelta(\t-\t') \gd\t=\gf(\t')$. Because of its singular
nature ($\geta(t)$ does not exist (``is infinite''), only its integral exists) it can be difficult to develop
an intuition for this object, and we prefer the $dW$ notation.} 
In simulations \BM paths can be constructed from a discretized version of \eref{BM_dx}
\be
\gv(\t+\dt)=\gv(\t)+ \gmu \dt + \gsigma \sqrt{\dt} \gxi_\t,
\elabel{BM_d}
\ee
where $\gxi_\t$ are instances of a standard normal distribution ($\mathcal{\N}(0,1)$).

\BM itself is not a time-independent random variable -- it is a non-ergodic stochastic process. This is easily seen
by comparing expectation value and time average. We start with the expressions (stated without proof here) for the finite-ensemble average and the finite-time average of \BM. The finite-ensemble average (easy to derive) is distributed as
\be
\ave{\gv}_\N \sim \gmu \t+\mN(0, \t/\N),
\elabel{fin_ens_BM}
\ee
and the finite-time average (a little harder to derive) of a single \BM trajectory is distributed as
\be
\bar{\gv}_\t \sim \gmu \t/2 + \gsigma \mN(0, \t/3).
\elabel{fin_tim_BM}
\ee

The expectation value, \ie the limit $\N\to\infty$ of \eref{fin_ens_BM}, converges to $\gmu \t$ with probability 
one, so it depends on time, and it's unclear how to compare that to a time average (which cannot depend on time). Its limit $\t\to\infty$ does not exist.

The time average, the limit $\t\to\infty $ of \eref{fin_tim_BM} diverges unless $\gmu=0$, but even with 
$\gmu=0$ the limit is a random variable with diverging variance -- something whose density 
is zero everywhere. In no meaningful sense do the two expressions converge to the same 
scalar in the relevant limits.

Clearly, \BM, whose increments are ergodic, is itself not ergodic. However, that doesn't make it
unmanageable or unpredictable -- we know the distribution of \BM at any moment in time. But the non-ergodicity
has surprising consequences of which we mention one now. We already mentioned
that if we plot a Brownian trajectory with non-zero drift on a piece of paper it will turn into a straight line for long enough
simulation times. This suggests that the randomness of a Brownian trajectory becomes irrelevant
under a very natural rescaling. Inspired by this insight let's hazard a guess as to what 
the time-average of zero-drift \BM might be. 

The simplest form of zero-drift \BM starts at zero, $\gv(0)=0$
and has variance $\var(\gv(\t))=\t$ (this process is also known as the ``Wiener process''). The process is 
known to be recurrent -- it returns to zero, arbitrarily many times, with probability one in the 
long-time limit. We would not be mad to guess that the time average of zero-drift \BM,
\be
\bar{\gv}=\lim_{\t\to\infty} \frac{1}{\t}\int_0^\t \gd\t' \gv(\t'),
\ee
will converge to zero with probability one. But we would be wrong. Yes, the process has no drift, and
yes it returns to zero infinitely many times, but its time average is not a delta function at zero.
It is, instead normally distributed with infinite variance according to the following limit
\be
\bar{\gv}\sim \lim_{\t\to\infty} \mN(0,\t/3).
\ee
Averaging over time, in this case, does not remove the randomness. A sample 
trajectory of the finite-time average (not of \BM but of the average over a \BM) is shown in \fref{1_6}. In the literature this process, 
$\frac{1}{\t}\int_0^\t \gd\t' \gv(\t')$, is known as the ``random acceleration process'' \cite{Burkhardt2007}.

\begin{figure}[h!]
\begin{picture}(200,200)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_tools/figs/BM_time_ave.pdf}}
\end{picture}
\caption{Trajectory of the finite-time average of a zero-drift \BM. The process is not ergodic: the time average 
does not converge to a number, but is instead distributed according to 
$\mN(0,\t/3)$ for all times, while the expectation value is zero. It is the result of integrating a \BM; 
integration is a smoothing operation, and as a consequence the trajectories are smoother than \BM (unlike a 
\BM trajectory, they are differentiable).}
\flabel{1_6}
\end{figure}
\FloatBarrier

\section{Geometric Brownian motion}
\seclabel{Geometric_Brownian}
\begin{defn}{Geometric Brownian motion}
If the logarithm of a quantity performs Brownian motion, the quantity itself performs ``geometric Brownian 
motion.''
\end{defn}

While in \secref{Brownian_motion} $\gv(\x)=\ln(\x)$ performed \BM, $\x$ 
itself performed \GBM. The change of variable from $\x$ 
to $\gv(\x)=\ln(\x)$ is trivial in a sense but it has interesting consequences. 
It implies, for instance, that 
\begin{itemize}
\item $\x(\t)$ is log-normally distributed
\item increments in $\x$ are neither stationary nor independent
\item $\x(\t)$ cannot become negative 
\item the most likely value of $\x$ (the mode) does not coincide with the 
expectation value of $\x$. 
\end{itemize}
These and other properties of the log-normal distribution will be discussed in detail in \secref{Log-normal_wealth}.

Again, it is informative to write \GBM as a stochastic differential equation. 
\be
\gd\x=\x(\gmu \gd\t+ \gsigma \gd\gW).
\elabel{GBM_c}
\ee
Similarly to \BM, trajectories for \GBM can be simulated using the discretized form (\cf \eref{BM_d})
\be
\d \x=\x(\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t),
\elabel{GBM_d}
\ee
where $\gxi_\t \sim \mN(0,1)$ are instances of a standard normal variable. In such simulations
we must pay attention that the discretization does not lead to negative values of $\x$. This 
happens if the expression in brackets in \eref{GBM_d} is smaller than $-1$ (in which case $\x$ changes negatively by more than itself).
To avoid negative values we must have $\gmu \dt + \gsigma \sqrt{\dt} \gxi_\t>-1$, or 
$\gxi_\t <\frac{1+\gmu\dt}{\gsigma \sqrt{\dt}}$. As $\dt$ becomes large it becomes more likely for
$\gxi_\t$ to exceed this value, in which case the simulation fails. But $\gxi_\t$ is Gaussian distributed, meaning
it has thin tails, and choosing a sufficiently small value of $\dt$ makes these failures essentially impossible.

 \GBM on logarithmic vertical scales looks like \BM on linear vertical scales. \Fref{1_2} is, in fact, and example of a very coarse discretisation of \GBM. 
But it's useful to look at a more finely discretised trajectory of \GBM on linear scales to develop an intuition for this important process.
\begin{figure}[h!]
\begin{picture}(200,230)(0,0)
    \put(0,-5){\includegraphics[width=\textwidth]{./chapter_tools/figs/GBM_trajectory.pdf}}
\end{picture}
\caption{Trajectory of a \GBM. What happens to the trajectory tomorrow depends strongly on where it is today -- for 
instance, unlike for \BM, it is difficult to recover
from a low value of $\x$, and trajectories are likely to get stuck near zero. Occasional excursions are characterised
by large fluctuations. Parameters are $\gmu=0.05$ per time unit and $\gsigma=\sqrt{2 \gmu}$, corresponding to zero 
growth rate in the long run. It would be easy to invent a story to go with this (completely random) trajectory --
perhaps something like  ``things were going well
in the beginning but then a massive crash occurred that destroyed morale.''}
\flabel{1_7}
\end{figure}

The basic message of the game from \secref{The_game} is that we may obtain different values for growth rates, depending on
how we average -- an expectation value is one average, a time average is quite another. The game 
itself is sometimes called the multiplicative binomial process \cite{Redner1990}, we thank \person{S. Redner} for 
pointing this out to us. \GBM is the continuous version of the multiplicative binomial process, and it shares the
basic feature of a difference between the growth rate of the expectation value and time-average growth.

The expectation value is easily computed -- the process is not ergodic, but that does not mean we cannot
compute its expectation value. We simply take the expectations values of both sides of \eref{GBM_c} to get
\bea
\ave{\gd\x}&=&\ave{\x(\gmu \gd\t+ \gsigma \gd\gW)}\\
=\gd\ave{\x}&=&\ave{\x} \gmu \gd\t.
\eea
This differential equation has the solution 
\be
\ave{\x(\t)}=\x(\tn)\exp(\gmu \t),
\ee
which determines the growth rate of the expectation value as 
\be
\gm(\ave{x})=\gmu.
\elabel{expectation_g}
\ee

As we know, this growth rate is different from the growth rate that materializes with probability 1 in the long run. 
Computing the time-average growth rate is only slightly more complicated, and it will get even simpler once we've introduced \Ito calculus in \secref{Ito}. 
But for now we will follow this plan: consider the discrete process \eref{GBM_d} and compute the changes in the logarithm of $\x$, 
then we will let
$\dt$ become infinitesimal and arrive at the result for the continuous process. We know $\d \ln(\x(\t))$
to be ergodic and reflective of performance over time, wherefore we will proceed to take its expectation value to compute the time average 
of the exponential growth rate of the process. 

The change in the logarithm of $\x$ in a time interval $\dt$ is
\bea
\ln \x(\t+\dt) - \ln \x(\t) &=&\ln [\x(1+\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t)] - \ln \x(\t)\\
&=&\ln \x + \ln (1+\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t) - \ln \x(\t)\\
&=&\ln (1+\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t),
\eea
which we Taylor-expand as $\ln(1+ \text{something small})$ because we will let $\dt$ become small.
Expanding to second order,
\be
\ln \x(\t+\dt) - \ln \x(\t) =\gmu \dt+ \gsigma \sqrt{\dt} \gxi_\t - \frac{1}{2} \left(\gmu \gsigma \dt^{3/2}\gxi_\t+
\gsigma^2\dt 
\gxi_\t^2\right)+\go(\dt^2),
\ee
using ``little-o notation'' to denote terms that are of order $\dt^2$ or smaller. Finally, because
$\d \ln \x(\t)$ is ergodic, by taking the expectation value of this equation we find the
time average of $\d \ln \x(\t)$
\be
\ave{\ln \x(\t+\dt) - \ln \x(\t)} =\gmu \dt- \frac{1}{2} \left(\gmu^2\dt^2+\gsigma^2\dt \right)+o(\dt^2).
\ee
Letting $\dt$ become infinitesimal the higher-order terms in $\dt$ vanish, and we find
\be
\ave{\ln \x(\t+\gd\t) - \ln \x(\t)} =\gmu \gd\t- \frac{1}{2} \gsigma^2 \gd\t,
\ee
so that the time-average growth rate is
\be
\gt=\frac{\gd \ave{\ln \x}}{\gd\t}=\gmu - \frac{1}{2} \gsigma^2.
\elabel{time_g}
\ee
The non-ergodicity of \GBM leads to a difference between the behaviour of the expectation value (which grows at $\gm(\ave{\x})$) 
and the long-time behaviour of any given trajectory (which grows at $\gt$). Because people experience their wealth over time (which may be described by \GBM) and have not access to the ensemble of other possible trajectories, they quite reasonably behave closer to optimising $\gm(\ave{\x})$ than to $\gt$.

We could have guessed the result by combining Whitworth's argument on the disadvantage of
gambling with the scaling of \BM. Let's re-write the factor $1-\epsilon$ in \eref{Whitworth} as
$1-\gsigma \sqrt{\dt}$. According to the scaling of the variance in a random walk, \eref{BM_var}, 
this would be a good coarse-graining of some faster
process (with shorter time step) underlying Whitworth's game.
To find out what happens over one single time step we take the square root of \eref{Whitworth},
\be
[(1+\gsigma \sqrt{\dt})(1-\gsigma \sqrt{\dt})]^{1/2}=[1-\gsigma^2 \dt]^{1/2}.
\ee 
Letting $\dt$ become infinitesimally small, we replace $\dt$ by $\gd \t$, and the first-order
term of a Taylor-expansion becomes exact,
\be
[(1+\gsigma \sqrt{\dt})(1-\gsigma \sqrt{\dt})]^{1/2}\to 1-\frac{\gsigma^2}{2} \gd\t,
\ee 
in agreement with \eref{time_g} if the drift term $\mu=0$, as assumed by Whitworth.

\section{\Ito calculus}
\seclabel{Ito}
We have chosen to work with the discrete process here and have arrived at a result that is more
commonly shown using \Ito's formula. We will not discuss \Ito calculus in depth 
but we will use some of its results. The key insight of \Ito was that the non-differentiability
of so-called \Ito processes leads to a new form of calculus, where in particular the chain rule of
ordinary calculus is replaced.
An \Ito process is a \SDE of the following form
\be
d\x = a(\x, \t) d\t + b(\x, \t) \gd\gW.
\elabel{Ito_process}
\ee
If we are interested in the behaviour of some other quantity that is a 
function of $\x$, let's say $\gv(\x)$, then \Ito's formula tells us how to derive
the relevant \SDE as follows:
\be
d\gv =  \left(\frac{\partial \gv}{\partial\t} +a(\x, \t)\frac{\partial \gv}{\partial\x} + \frac{b(\x,\t)^2}{2} \frac{\partial^2 \gv}{\partial\x^2}\right) d\t + b(\x, \t) \frac{\partial \gv}{\partial\x} \gd\gW.
\elabel{Ito}
\ee
Derivations of this formula can be found on Wikipedia. Intuitive derivations, such as \cite{Hull2006}, use the 
scaling of the variance, \eref{BM_var}, and more formal 
derivations, along the lines of \cite{Harrison2013}, rely on integrals.
We simply accept \eref{Ito} as given. It makes it very easy to re-derive \eref{time_g}, which we leave as an exercise:
use \eref{Ito} to find the \SDE for $\ln(\x)$, take its expectation value and differentiate with respect to $\t$. 
We will use \eref{Ito} in \secref{rescaled}. The above computations are intended to 
give the reader intuitive confidence that \Ito calculus can be 
trusted\footnote{\Ito calculus is one way of interpreting the non-differentiability of \gd\gW. Another interpretation
is due to Stratonovich, which is not strictly equivalent. However, the key property of \GBM that we make
extensive use of is the difference between the growth rate of the expectation value, $\gm(\ave{\x})$, and the
time-average growth rate, $\gt$. This difference is the same in the Stratonovich and the \Ito interpretation, and all our results hold in both cases.}. 
We find that, though phrased in different words, our key insight -- that {\it the growth rate of the expectation value is not the time-average
growth rate} -- has appeared in the literature not only in 1870 but also in 1944.
And in 1956 \cite{Kelly1956}, and in 1966 \cite{Thorp1966}, and in 1991 \cite{CoverThomas1991}, and at many other times. 
Yet the depth of this insight remained unprobed.

\Eref{time_g}, which agrees with \Ito calculus, may be surprising.
Consider the case of no noise $\gd\x=\x \gmu \gd\t$. Here we can identify $\gmu=\frac{1}{\x}\frac{\gd\x}{\gd\t}$ 
as the infinitesimal increment in the logarithm, $\frac{\gd \ln(\x)}{\gd\t}$, using the chain rule of ordinary calculus. 
A na\"ive application of the chain rule to \eref{GBM_c} would therefore also yield $\frac{\gd \ave{\ln(\x)}}{\gd\x}=\gmu$, 
but the fluctuations in \GBM have a non-linear effect, and it turns out that the usual chain rule does not apply. \Ito
calculus is a modified chain rule, \eref{Ito}, which leads to the difference $-\frac{\gsigma^2}{2}$ between the 
expectation-value growth rate and the time-average
growth rate. 

This difference is sometimes called the ``spurious drift'', but at the \LML we call it the ``Weltschmerz'' because 
it is the difference between the many worlds of our dreams and fantasies, and the one cruel reality that 
the passage of time imposes on us.

\section*{Summary of \cref{Coins}}
Suppressed.
%In this chapter we have introduced the following key concepts:
%\bi
%\item[\bf Random variable]
%A random variable $Y$ is a set of pairs of possible values and  corresponding probabilities, $Y=\{(y_1, p_1),(y_2, p_2)...\}$.
%The sets may be discrete or continuous. We stressed that a random variable is an a-temporal concept. It's just a bunch of possible values and their weights (probabilities). In real life we often this of generating instances of random variables as time passes, but this is not part of the formal definition of a random variable.
%
%\item[{\bf Expectation value}]
%The expectation value of a random variable is the weighted sum $\ave{Y}=\int y \mathcal{P}_Y(y) dy$, where $\mathcal{P}_Y$ has atomic point masses in the discrete case, which means we can express the integral as $\ave{Y}=\sum_i y_i p_i$.
%
%The expectation value is also called the ensemble average, which reflects a physical interpretation: imagine (infinitely) many possible worlds, identical safe for the value taken by the the random variable $Y$. Those values are represented in the superverse of many worlds in proportion to their probabilties. Averaging $y$ over the ensemble of universes then gives the expectation value.
%
%\item[\bf Logarithms and exponentials]
%The logarithmic function is defined as $\ln \ga :=  \lim_{\T\to\infty}\T \left\{\ga^{1/\T}-1\right\}$. It was motivated by computing interest on loans. Its property of turning products into sums makes it a key function in many branches of science where quantities that combine multiplicatively are to be represented by a quantity that combines additively. 
%
%The exponential function is the inverse of the logarithm.
%
%\item[\bf Stochastic process]
%A stochastic process $Y_Z(z;t)$ is a family of random numbers, indexed by a parameter, $t$, that we call ``time.'' At each moment in time, a realization of a stochastic process takes a random value. We often think of stochastic processes as being generated through stochastic differential equations, where the initial value of some quantity is iteratively transformed -- for example by adding a random value to it.
%Examples include
%\bi
%\item[{\bf Coin-toss game}]
%Start with initial value \$1 and multiply it in each round with an instance of the random variable $[(0.6,1/2),(1.5,1/2)].$
%
%\item[\bf Brownian motion]
%Brownian motion can be defined as a stochastic process with continuous paths (no jumps) whose distribution is Gaussian $\mathcal{N}(\mu t, \sigma^2 t)$. 
%
%\item[\bf Geometric Brownian motion]
%If the logarithm of a quantity is a Brownian motion, the quantity itself is geometric Brownian motion. Wealth in the coin-toss game is a discrete form of geometric Brownian motion.
%\ei
%
%\item[\bf Time average]
%Any function of time, including any stochastic process can time averaged. For an observable $\ga(\t)$, the finite-time average is $\frac{1}{\T}\int_0^\T \ga(\t) \dt$. This quantity is a random variable. One way to get rid of the randomness is to keep averaging, namely to let $\T$ diverge. The time average (without the qualifier ``finite-time'') is the limit  $\lim_{\T\to\infty} \frac{1}{\T}\int_0^\T \ga(\t) \dt$.
%
%\item[\bf Scalars]
%Scalars are just numbers. We say ``scalar'' when we want to emphasize that we're not talking about higher-dimensional mathematical objects, like vectors or random variables. The neat thing about scalars is their transitivity property: if $a>b$ and $b>c$ then $a>c$. This allows us to rank scalars. As a consequence, anything that can be mapped, or collapsed onto a scalar can be ranked. For this reason, much of decision theory consists of coming up with ways of collapsing models of wealth onto scalars. These can then be ranked, yielding transitive preferences.
%
%\item[\bf Dimensional analysis]
%Dimensional analysis is a branch of mathematics that places constraints on what can be a physical quantity. This is done by insisting that the quantity cannot depend on human conventions. It is often used to derive scaling relations and as a sanity check for supposed mathematical relationships between physical quantities.
%
%\item[\bf \Ito process]
%\Ito processes are stochastic processes that can be written down as particularly simple stochastic differential equations. Their form allows a deep mathematical analysis of their properties.
%
%\item[\bf \Ito's formula]
%We will make use of \Ito's formula, which is a tool to find the increment in a function of an \Ito process.
%
%\item[\bf Ergodic observable]
%An ergodic observable is a stochastic process with the following ergodic property: its time average is identical to its expectation value. If the process includes transients, one has to consider the expectation value far away from any transients.
%\ei