%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Markets\clabel{Markets}}

{\it This lecture applies the ideas developed in the preceding lectures to markets. 

We set up a simple portfolio selection problem in a market of two assets: one risky, like shares; and the other riskless, like a bank deposit. We ask how an investor would best allocate his money between the two assets, which we phrase in terms of his leverage. We review the classical approach, which can't answer this question without additional information about the investor's risk preferences. We then use the decision theory we've developed so far to answer the question unambiguously, by deriving the optimal leverage which maximises the investment's time-average growth rate. 

This is a Gedankenexperiment that will lead us to predict and the empirically discover certain regularities in the price statistics of freely traded assets. We consider what this objectively defined optimal leverage might mean for financial markets themselves. If all the participants in a market aim for the same optimal leverage, does this constrain the prices and price fluctuations that emerge from their trading? We argue that it does, and we quantify how. Our treatment of the problem generates a theory of noise in stock prices, resolves the so-called equity premium puzzle (and the price volatility puzzle), provides a natural framework for setting central-bank interest rates, and even suggests a method for fraud detection.
We test our predictions using data collected from the American and German stock markets as well as Bitcoin and Bernie Madoff's Ponzi scheme.
}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimal leverage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A model market}
We consider assets whose values follow multiplicative dynamics, which we will model using \GBM. In general, an amount $\x$ invested in such an asset evolves according to the \SDE,
\be
\gd\x = \x(\gmu \gd\t + \gsigma \gd\gW),
\elabel{sde_gbm}
\ee
where $\gmu$ is the drift and $\gsigma$ is the volatility. By now we are very familiar with this equation and how to solve it.

To keep things simple, we imagine a market of two assets. One asset is riskless: the growth in its value is known deterministically and comes with a cast-iron guarantee.\footnote{Such guarantees are easy to offer in a model. In the real world, one should be very suspicious of anything that comes with a ``cast-iron guarantee''.} This might correspond in reality to a bank deposit. The other asset is risky: there is uncertainty over what its value will be in the future. This might correspond to a share in a company or a collection of shares in different companies. We will think of it simply as stock.

An amount $\xzero$ invested in the riskless asset evolves according to
\be
\gd\xzero = \xzero \mur \gd\t.
\elabel{sde_0}
\ee
$\mur$ is the riskless drift, known in finance as the riskless rate of return.\footnote{In general we will eschew financial terminology for rates. Economics has failed to define them clearly, with the result that different quantities, like $\gt$ and $\gex$, are often conflated. The definitions developed in these lectures are aimed at avoiding such confusion.} There is no volatility term. In effect, we have set $\gsigma=0$. We know with certainty what $\xzero$ will be at any point in the future: 
\be
\xzero(\tn+\Dt) = \xzero(\tn)\exp(\mur\Dt).
\elabel{sde_0_soln}
\ee

An amount $\xone$ invested in the risky asset evolves according to
\be
\gd\xone = \xone ( \mus \gd\t + \sigmas \gd\gW ),
\elabel{sde_1}
\ee
where $\mus>\mur$ is the risky drift and $\sigmas>0$ is the volatility (the subscript s stands for stock). $\mus$ is also known in finance as the expected return.\footnote{Probably because it's the growth rate of the expected value, see \eref{exp_ret}.} This equation has solution
\be
\xone(\tn+\Dt) = \xone(\tn)\exp\left[\left(\mus -\frac{\sigmas^2}{2}\right)\Dt + \sigmas \gW(\Dt)\right],
\elabel{sde_1_soln}
\ee
which is a random variable.

We will refer to the difference
\be
\mue=\mus-\mur
\elabel{def_mue}
\ee
as the excess drift. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Leverage}
Let's turn to the concept of leverage. Imagine a very simple portfolio of value $\xl$, out of which $\l \xl$ is invested in stock and the remainder, $(1-\l)\xl$, is put in the bank. $\l$ is known as the leverage. It is the fraction of the total investment assigned to the risky asset. $\l=0$ corresponds to a portfolio consisting only of bank deposits. $\l=1$ corresponds to a portfolio only of stock.

You would be forgiven for thinking that prudence dictates $0\leq\l\leq1$, \ie that we invest some of our money in stock and keep the rest in the bank. However, the financial markets have found all sorts of exciting ways for us to invest almost any amount in an asset. For example, we can make $\l>1$ by borrowing money from the bank to buy more stock than we could have bought with only our own money.\footnote{This doesn't immediately affect the portfolio's value. The bank loan constitutes a negative investment in the riskless asset, whose value cancels the value of the stock we bought with it. Of course, the change in the portfolio's composition will affect its future value.} We can even make $\l<0$ by borrowing stock (a negative investment in the risky asset), selling it, and putting the money raised in the bank. In the financial world this practice is called short selling.

Each investment in our portfolio experiences the same relative fluctuations as the asset in which it has been made. The overall change in the portfolio's value is, therefore,
\be
\gd\xl = (1-\l)\xl \frac{\gd\xzero}{\xzero} + \l \xl \frac{\gd\xone}{\xone}.
\ee
Substituting in \eref{sde_0} and \eref{sde_1} gives the \SDE for a leveraged investment in the risky asset,
\be
\gd\xl = \xl [ (\mur + \l\mue) \gd\t + \l\sigmas \gd\gW ],
\elabel{sde_l}
\ee
with solution,
\be
\xl(\tn+\Dt) = \xl(\tn)\exp\left[\left(\mur+\l\mue-\frac{\l^2\sigmas^2}{2}\right)\Dt+\l\sigmas \gW(\Dt)\right].
\elabel{sde_l_soln}
\ee
We can now see why we labelled investments in the riskless and risky assets by $\xzero$ and $\xone$: when $\l=0$, $\xl$ follows the same evolution as $\xzero$; and when $\l=1$, it evolves as $\xone$.

In our model $\l$, once chosen, is held constant over time. This means that our model portfolio must be continuously rebalanced to ensure that the ratio of stock to total investment stays fixed at $\l$. For example, imagine our stock investment fluctuates up a lot over a short time-step, while our bank deposit only accrues a little interest. Immediately we have slightly more than $\l$ of the portfolio's value in stock, and slightly less than $1-\l$ of its value in the bank. To return the leverage to $\l$, we need to sell some stock and deposit the proceeds in the bank, \fref{rebalance}. In \eref{sde_l} we are imagining that this happens continuously.\footnote{In reality, of course, that's not possible. We could try to get close by rebalancing frequently. However, every time we buy or sell an asset in a real market, we pay transaction costs, such as broker's fees and transaction taxes. This means that frequent rebalancing in the real world can be costly.}

\begin{figure}
\begin{picture}(200,200)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_markets/figs/rebalance.pdf}}
\end{picture}
\caption{Investment in a risky (blue) and riskless (red) asset. Leverage starts at $\l=0.7$, meaning 70\% is invested in the risky asset, and 30\% in the riskless. The investment then experiences the relative returns of the market. In this case the risky asset goes up quite a bit, and the riskless asset goes up less. Consequently, the leverage changes. To maintain a constant leverage, some risky asset has to be sold in return for some riskless, a step known as re-balancing.
\flabel{rebalance}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Portfolio theory\seclabel{pf_theory}}
Our simple model portfolio parametrised by $\l$ allows us to ask the
\begin{keypts}{Question:}
What is the optimal value of $\l$?
\end{keypts}
This is similar to choosing between gambles, for which we have already developed a decision theory. The main difference is that we are now choosing from a continuum of gambles, each characterised by a value of $\l$, whereas previously we were choosing between a discrete set of gambles. The principle, however, is the same: we will maximise the time-average growth rate of our investment.

\subsection{Markowitz's efficient frontier}

Before we use ergodicity economics, let's review the classical treatment of the problem so that we appreciate the wider context. Intuitively, people understand there is some kind of trade-off between risk and reward. In our model of a generic multiplicative asset, \eref{sde_gbm}, we could use $\gsigma$ as a proxy for risk and $\gmu$ as a proxy for reward. Ideally we want an investment with large $\gmu$ and small $\gsigma$, but we also acknowledge the rule-of-thumb that assets with larger $\gmu$ tend to have larger $\gsigma$.\footnote{A ``no such thing as a free lunch'' type of rule.} This is why we model our risky asset as having a positive excess return, $\mue>0$, over the riskless asset.

Intuition will only take us so far. A rigorous treatment of the portfolio selection problem was first attempted by Markowitz in 1952~\cite{Markowitz1952}. He suggested defining a portfolio with parameters $(\gsigma_i, \gmu_i)$ as efficient if there exists no rival portfolio with parameters $(\gsigma_j, \gmu_j)$ with $\gmu_j\geq\gmu_i$ and $\gsigma_j\leq\gsigma_i$.
% AA: simplified definition of Markwoitz-efficient for clarity at tiny cost of rigour
%for which at least one of the following statements is true:
%\begin{enumerate}
%\item $\gmu_j>\gmu_i$ and $\gsigma_j\leq\gsigma_i$;
%\item $\gsigma_j<\gsigma_i$ and $\gmu_j\geq\gmu_i$.
%\end{enumerate}
%In other words, if the rival portfolio has higher $\gmu$, it had better have higher $\gsigma$; or if it has lower $\gsigma$, it had better have lower $\gmu$.
Markowitz argued that it is unwise to invest in a portfolio which is not efficient.

In our problem, we are comparing portfolios with parameters $(\l\sigmas, \mur+\l\mue)$. These lie on a straight line in the $(\gsigma, \gmu)$-plane,\footnote{Derived by eliminating $\l$ from the equations $\gmu=\mur+\l\mue$ and $\gsigma=\l\sigmas$.}
\be
\gmu = \mur + \left(\frac{\mue}{\sigmas}\right)\gsigma,
\elabel{frontier}
\ee
shown schematically in \fref{markowitz}.
\begin{figure}
\begin{picture}(200,200)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_markets/figs/markowitz.pdf}}
\end{picture}
\caption{The Markowitz portfolio selection picture. The red dots are the locations in the $(\gsigma,\gmu)$-plane of portfolios containing only the riskless (left) or risky (right) asset. The blue dot is one possible leveraged portfolio, in this case with $\l>1$. All possible leveraged portfolios lie on the black line, \eref{frontier}. The grey dots are hypothetical alternative portfolios, containing different assets excluded from our simple portfolio problem. Their location below and to the right of the black line makes them inefficient under the Markowitz scheme.\flabel{markowitz}}
\end{figure}
Under Markowitz's classification, all of the leveraged portfolios on this line are efficient.\footnote{Indeed, in finance this line is called the efficient frontier.} Therefore, any leverage we choose gives a portfolio the classification would recommend. This does not help answer our question. By itself, Markowitz's approach is agnostic to leverage: it requires additional information to select a specific portfolio as the optimum. Markowitz was aware of this limitation and argued that the optimal portfolio could be identified by considering the investor's risk preferences \cite{Markowitz1991}: ``The proper choice among portfolios depends on the willingness and ability of the investor to assume risk.'' The ultimate reliance on personal preferences is a theme that runs through all of economic theory, and we have seen examples of it in previous chapters. Of course, economic theory, or finance theory, is not meant to replace human judgement and imagination, nor is it supposed to force certain products and behaviours on people -- but it is supposed to guide decisions. A statement like ``this is the action that will eventually make your wealth grow fastest, assuming we have chosen models and estimated parameters correctly'' is helpful and leaves plenty of room for judgement calls. As in decision theory in general, risk preferences are typically included in the portfolio selection problem through a utility function, and thereby the theory of finance inherits a major cornerstone of economics, along with all its problems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sharpe ratio\seclabel{sharpe}}
The Sharpe ratio~\cite{Sharpe1966} for an asset with drift $\gmu$ and volatility $\gsigma$ is defined as
\be
\Sharpe\equiv\frac{\gmu-\mur}{\gsigma}
\elabel{def_sharpe}
\ee
It is the gradient of the straight line in the $(\gsigma, \gmu)$-plane which passes through the riskless asset and the asset in question. $\Sharpe$ is often used as a convenient shorthand for applying Markowitz's ideas, since choosing the portfolio with the highest $\Sharpe$ from the set of available portfolios is equivalent to choosing an efficient portfolio. The optimal leverage problem, however, important as it is, cannot be resolved by this procedure. The reason is this: all of our leveraged portfolios lie on the same line, \eref{frontier}, and so all of them have the same Sharpe ratio, which is simply the line's gradient:
\be
\Sharpe_\l = \frac{\mue}{\sigmas}.
\elabel{sharpe_l}
\ee
This is insensitive to the leverage $\l$, resulting in the same non-advice as the Markowitz approach. Sharpe also suggested considering risk preferences to resolve the optimal portfolio: ``The investor's task is to select from among the efficient portfolios the one that he considers most desirable, based on his particular feelings regarding risk and expected return''~\cite{Sharpe1966}. Once more we encounter the theme of reliance on personal preferences in finance and economics. It is no surprise that this switch from formal recommendation (a portfolio should be on the efficient frontier/have a high Sharpe ratio) to personal preference (pick the one whose risks you feel most comfortable with) happens too soon, from our perspective. In general, we use a formal model as long as it's useful, and when we have pushed it to the point where it yields no further reliable answers we switch to personal judgement. The conceptual superiority of our analysis (relying on averages over time not over parallel worlds) allows us to push the model a little further, and the point where we previously had to switch now seems premature. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expected return\seclabel{exp_ret}}
We noted previously that the growth rate of the expectation value of the risky price is the risky drift, $\mus$, also known as the expected return. This is because
\be
\ave{\xone(\tn+\Dt)} = \ave{\xone(\tn)}\exp(\mus \Dt),
\elabel{exp_ret}
\ee
which, as a multiplicative process, has growth rate
\be
\gm(\ave{\xone}) = \frac{\D\ln\ave{\xone}}{\Dt} = \mus.
\ee
It follows immediately from comparison of \eref{sde_1} and \eref{sde_l} that the expected value of the leveraged portfolio grows at
\be
\gm(\ave{\xl}) = \mur+\l\mue.
\elabel{exp_ret_l}
\ee
This illustrates why a portfolio theory which is insensitive to leverage, such as that of Markowitz and Sharpe\footnote{Both recipients of the 1990 Alfred Nobel Memorial Prize in Economic Sciences.}, is potentially dangerous. Because it doesn't alert the investor to the dangers of over-leveraging and considers any leverage optimal, the investor must find an additional criterion. There is nothing in the formalism that would prevent the investor to select as that additional criterion the expected return in \eref{exp_ret_l}. Consequently he would maximise his leverage, $\l\to\infty$. This, as we will shortly see, would almost surely ruin him.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Growth rate maximisation}
We now understand the classical approach, as applied to a very simple portfolio problem, and we are aware of its limitations. What does ergodicity economics have to say?

The time-average growth rate of the leveraged portfolio is
\be
\gtm(\l) \equiv \lim_{\Dt\to\infty} \left\{ \gm(\xl,\Dt) \right\} = \lim_{\Dt\to\infty} \left\{ \frac{\D\ln \xl}{\Dt} \right\}.
\elabel{g_l_def}
\ee
Note that \eref{sde_gbm} fully defines the dynamics -- finance works with stochastic processes that include the mode of repetition. In contrast, classical decision theory works with gambles where the appropriate mode of repetition has to be guessed. This is a key advantage of finance over economics, and it means that while from the utility perspective the problem of portfolio selection is underspecified (we need to guess the utility function), from the perspective of time optimization it is not. We have all the information we need, the problem is well posed.
The time-average growth rate will depend on $\l$. Inserting the expression for $\xl$ from \eref{sde_l_soln} in \eref{g_l_def} gives
\be
\gtm(\l) = \lim_{\Dt\to\infty} \left\{ \frac{1}{\Dt} \left[ \left(\mur + \l\mue - \frac{\l^2\sigmas^2}{2}\right)\Dt + \l\sigmas \gW(\Dt) \right] \right\},
\elabel{g_l_noisy}
\ee
which, since $\gW(\Dt)/\Dt\sim\Dt^{-1/2}\to0$ as $\Dt\to\infty$, converges to
\be
\boxed{\; \gtm(\l) = \mur + \l\mue - \frac{\l^2\sigmas^2}{2}. \;}
\elabel{g_l_quadratic}
\ee
This is a quadratic in $\l$ with an unambiguous maximum\footnote{Derived, for example, by setting $\frac{d\gtm}{d\l}=0$.} at
\be
\boxed{\; \lopt = \frac{\mue}{\sigmas^2}. \;}
\elabel{lopt}
\ee
$\lopt$ is the optimal leverage which defines the portfolio with the highest time-average growth rate. Classical theory is indifferent to where on the line in \fref{markowitz} we choose to be. Our decision theory, however, selects a particular point on that line,\footnote{Subsequently Markowitz became aware of this point, which he called the ``Kelly-Latan\'{e}'' point in~\cite{Markowitz1976}, referring to~\cite{Kelly1956,Latane1959}.} which answers the question we posed at the start of \secref{pf_theory}. This is the key result. We note that all we need to know are the parameters $\mur$, $\mus$, and $\sigmas$ of the two assets in our market. In particular, it is defined objectively, with no reference to idiosyncrasies of the investor (except that we assume him to prefer fast growth to slow growth). Our approach can be useful when it is unknown who the ``investor'' is. Consider, for instance, a pension fund -- it has to position itself in some way, manage its portfolio and the risks it is taking. The portfolio manager often cannot know who the investors are -- there may be thousands of them. Optimizing the time-average growth rate would be a sensible goal that's easily communicated, whereas trying to optimize the investment according to the investors' risk preferences is simply impossible.

\eref{g_l_quadratic} gives the time-average growth rate along the efficient frontier, where all of our leveraged portfolios lie. In fact, it's easy to calculate the growth rate for any point in the $(\gsigma,\gmu)$-plane: it is simply $\gmu-\gsigma^2/2$. Overlaying the Markowitz picture in \fref{markowitz} on the growth rate landscape is illuminating. In effect, it adds the information missing from the classical model, which was needed to distinguish between portfolios. This is shown in \fref{markowitz_peters}.
\begin{figure}
\begin{picture}(200,200)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_markets/figs/markowitz_peters.png}}
\end{picture}
%\centering
%\includegraphics[width=\textwidth]{./chapter_markets/figs/markowitz_peters.png}
\caption{The augmented portfolio selection picture. The Markowitz picture is shown in green, with our model leveraged portfolios on the straight line (the efficient frontier) and hypothetical alternative portfolios within the ellipse (analogous to the grey dots in \fref{markowitz}). This is overlaid on a colour plot of the time-average growth rate, $\gt$. The optimal leverage, $\lopt\approx1.54$, is marked at the location of the highest $\gt$ on the efficient frontier. Portfolios on the white curve have $\gt=0$. Eventually this will intersect the efficient frontier, at which point applying more leverage will produce a portfolio with negative long-run growth. Parameters are $\mur=\mue=0.05$ per unit time and $\sigmas=0.18$ per square root time (denoted by $\gmu_\text{riskless}$, $\gmu_\text{excess}$, and $\gsigma_\text{M}$ in the figure). Adapted from~\cite{Peters2011a}.\flabel{markowitz_peters}}
\end{figure}

That \eref{g_l_quadratic} defines an inverted parabola means that, even on the efficient frontier, there exist portfolios with $\gtm(\l) < 0$. These occur for $\l<\lm$ and $\l>\lp$, where
\be 
\lpm \equiv \lopt \pm \sqrt{\lopt^2 + \frac{2\mur}{\sigmas^2}}.
\ee
This confirms our assertion at the end of \secref{exp_ret}, that an investor maximising his leverage in either direction will, if he is able to apply enough leverage, lose wealth. Indeed, if his leveraging ability is unlimited, he will find to his horror what is easily seen in \eref{g_l_quadratic}, that $\gtm(\l)$ diverges negatively as $\l\to\pm\infty$. He will lose fast.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stochastic market efficiency\seclabel{Stochastic}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A fundamental measure\seclabel{fundamental}}
Aside from being insensitive to leverage, the Sharpe ratio, $\Sharpe=\mue/\sigmas$, is a dimensionful quantity. Its unit is $(\text{time unit})^{-1/2}$. This means that its numerical value is arbitrary (since it depends on the choice of time unit) and tells us nothing fundamental about the system under study. For example, a portfolio with $\Sharpe=5$ per square root of one year has $\Sharpe=5(365)^{-1/2}\approx0.26$ per square root of one day. Same portfolio, different numbers.

The optimal leverage, $\lopt=\mue/\sigmas^2$, which differs from the Sharpe ratio by a factor of $1/\sigmas$, is a dimensionless quantity. Therefore, its numerical value does not depend on choice of units and has the potential to carry fundamental information about the system.\footnote{See Barenblatt's modern classic on scaling~\cite{Barenblatt2003}.} We could view $\lopt$ as the fundamental measure of a portfolio's quality, similarly to how $\Sharpe$ is viewed in the classical picture. A portfolio with a high optimal leverage must represent a good investment opportunity to justify such a large commitment of the investor's funds.

However, the significance of $\lopt$ runs deeper than this. The portfolio to which $\lopt$ refers is that which optimally allocates money between the risky and riskless assets in our model market. Therefore, it tells us much about conditions in that market and, by extension, in the wider model economy. A high $\lopt$ indicates an economic environment in which investors are incentivised to take risks. Conversely, a low or negative $\lopt$ indicates little incentive to take risks. This raises a tantalising
\begin{keypts}{Question:}
Are there any special numerical values of $\lopt$ which describe different market regimes, or to which markets are attracted?
\end{keypts}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Relaxing the model}
Our model market contains assets whose prices follow \GBM with constant drift and volatility parameters.\footnote{A tautology, since \eref{sde_gbm} only describes a \GBM if $\gmu$ and $\gsigma$ are constant in time.} Once specified, $\mur$, $\mus$, and $\sigmas$ are static and, therefore, so is $\lopt$. This limits the extent to which we can explore the question, since we cannot consider changes in $\lopt$. To make progress we need to relax the model. We must consider which parts of the model are relevant to the question, and which parts can be discarded without grave loss.

The \GBM-based model is useful because it motivates the idea of an objectively optimal leverage which maximises the growth of an investment over time. It also provides an expression for $\lopt$ in terms of parameters which, in essence, describe the market conditions under which prices fluctuate. These parameters have correspondences with quantities we can measure in real markets. All of this is useful.

However, we are ultimately interested in real markets, and their asset prices do not strictly follow \GBM (because nothing in nature\footnote{We consider markets to be part of nature and appropriate objects of scientific study.} truly does). In particular, real market conditions are not static. They change over time, albeit on a longer time scale than that of the price fluctuations. In this context, the model assumption of constant $\mur$, $\mus$, and $\sigmas$ is restrictive and unhelpful. We will relax it and imagine a less constrained model market where these parameters, and therefore $\lopt$, are allowed to vary slowly. We will not build a detailed mathematical formulation of this, but instead use the idea to run some simple thought experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Efficiency}
One way of approaching a question like this is to invoke the concept of efficiency. In economics this has a specific meaning in the context of financial markets, which we will mention imminently. In general terms, an efficient system is one which is already well optimised and whose performance cannot be improved upon by simple actions. For example, a refrigerator is efficient if it maintains a cool internal environment while consuming little electrical power and emitting little noise\footnote{This is a subtle reference to a fridge whose presence once graced the offices of \LML. It was not efficient.}. Similarly, Markowitz's portfolios were efficient because the investor could do no better than to choose one of them. 

The ``efficient market hypothesis'' of classical economics treats markets as efficient processors of information. It claims that the price of an asset in an efficient market reflects all of the publicly available information about it. The corollary is that no market participant, without access to privileged information, can consistently beat the market simply by choosing the prices at which he buys and sells assets. We shall refer to this hypothesis as ordinary efficiency.\footnote{This is not the most precise hypothesis ever hypothesised. What does it mean for a price to ``reflect'' information? Presumably this involves some comparison between the observed price of the asset and its true value contingent on that information. But only the former is observable, while the latter evades clear definition. Similarly, what does it mean to ``beat the market''? Presumably something to do with achieving a higher growth rate than a general, na\"{i}ve investment in the overall market. But what investment, exactly? We will leave these legitimate questions unanswered here, since our focus is a different form of market efficiency. The interested reader can consult the comprehensive review in~\cite{Sewell2011}.}

We will consider a different sort of efficiency, where we think not about the price at which assets are bought and sold in our model market, but instead about the leverage that is applied to them. Let's run a thought experiment.

\begin{thoughtex}{efficiency under leverage}
Imagine that $\lopt>1$ in our model market. This would mean that the simple strategy of borrowing money to buy stock will achieve faster long-run growth than buying stock only with our own money. If we associate putting all our money in stock, $\l=1$, with an investment in the market, then it would be a trivial matter for us to beat the market (by doing nothing more sophisticated than investing borrowed money).

Similarly, imagine that $\lopt<1$. In this scenario, the market could again be beaten very easily by leaving some money in the bank (and, if $\lopt<0$, by short selling).
\end{thoughtex}

It would strain language to consider our market efficient if consistent outperformance were so straightforward to achieve. This suggests a different, fluctuations-based notion of market efficiency, which we call stochastic market efficiency: it is impossible for a market participant without privileged information to beat a stochastically efficient market simply by choosing the \textit{amount} he invests in stock, \ie by choosing his leverage.\footnote{This resembles ordinary efficiency except that we have replaced price by amount.} We believe real markets to be stochastically efficient. Therefore, we make the following

\begin{keypts}{Hypothesis: stochastic market efficiency}
Real markets self-organise such that
\be
\boxed{\; \lopt = 1 \;}
\elabel{sme_strong}
\ee
is an attractive point for their stochastic properties.
\end{keypts}
These stochastic properties are represented by $\mur$, $\mus$, and $\sigmas$ in the relaxed model that permits dynamic adjustment of their values. %We call this the strong form of the hypothesis, since it makes a very precise prediction about the attractive value for $\lopt$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stability}
Another approach to the question in \secref{fundamental}, whose style owes more to physics than economics, is to consider the stability of the system under study (here, the market) and how this depends on the value of the observable in question (here, $\lopt$). 

Systems which persist over long time scales tend to be stable. Many of the systems we find in nature therefore include stabilising elements. Unstable systems tend to last for shorter times,\footnote{We use the comparative ``shorter'' here. This does not mean short. It is quite possible for an unstable system to remain in flux for a long time in human terms, perhaps giving the illusion of stability. Indeed, much of classical economic theory is predicated on the idea that economies are at or close to equilibrium, \ie stable. We would argue that economies are fundamentally far-from-equilibrium systems and must be modelled as such, even if their dynamics unfold over time scales much longer than our day-to-day affairs.} so we observe them less frequently. With this in mind, let's think about what different values of $\lopt$ imply for systemic stability.

\begin{thoughtex}{stability under leverage}
Imagine that $\lopt>1$ in our relaxed model. Since it is an objectively optimal leverage which does not depend on investor idiosyncrasies, this means that \textit{everyone} in the market should want to borrow money to buy stock. But, if that's true, who's going to lend the money and who's going to sell the stock?

Similarly, imagine that $\lopt<0$. This means that \textit{everyone} should want to borrow stock and sell it for cash. But, if that's true, who's going to lend the stock and who's going to relinquish their cash to buy it?

Unless there are enough market participants disinterested in their time-average growth rate to take the unfavourable sides of these deals -- which in our model we will assume there aren't -- then neither of these situations is globally stable. It's hard to imagine them persisting for long before trading activity causes one or more of the market parameters to change, returning $\lopt$ to a stable value.
\end{thoughtex}

This thought experiment suggests that we could observe markets with $0\leq\lopt\leq1$. We've defined things so that the optimal leverage is the proportion of one's wealth ideally invested in the risk asset, \ie in shares. $\lopt<1$ probably doesn't correspond to a macroeconomically desirable state, in the sense that people would be incentivised to keep their money in bank accounts rather than to invest it. But it wouldn't be unstable purely from the leverage point of view. This hints at an interesting generalisation: what if both assets are risky? What if we're comparing two share indexes or two shares, eliminating cash from our considerations? The same stability arguments would suggest that the optimal leverage in such a portfolio -- now defined as the proportion ideally held in one of the assets -- should be in the range $0\leq\lopt\leq1$, which reflects the symmetry between two risky assets. Our original hypothesis then describes the special case where one asset does not fluctuate (is the numeraire) and can be created by an agent who aims for full investment.

\begin{keypts}{Hypothesis: stochastic market efficiency for general asset pairs}
On sufficiently long time scales, the range $0\leq\lopt\leq1$ is an attractor for the stochastic properties of pairs of risky assets. 
\end{keypts}
%\OP{Not sure we should leave this in without data analysis. Kind of a big point.}

Whether in our original setup real markets spend significant time in a phase with $\lopt<1$ is difficult to tell. Realistic optimal leverage depends significantly on such things as trading costs, access to credit and risky assets in lending programs, infrastructure and technology. All of these factors differ among market participants. They suggest that measuring optimal leverage in the simplest way -- using market prices only and assuming zero trading costs -- will produce an over-estimate, compared to the value experienced by real market participants who incur realistic costs.

The dynamical adjustment, or self-organisation, of the market parameters takes place through the trading activity of market participants. In particular, this creates feedback loops, which cause prices and fluctuation amplitudes to change, returning $\lopt$ to a stable value whenever it strays. To be truly convincing, we should propose plausible trading mechanisms through which these feedback loops arise. We do this in~\cite{PetersAdamou2011}. Since they involve details about how trading takes place in financial markets (in which we assume the typical attendee of these lectures is disinterested) we shall not rehearse them here. The primary drivers of our hypothesis are the efficiency and stability arguments we've just made.

Furthermore, there are additional reasons why we would favour the strong form of the hypothesis over long time scales. The main one is that an economy in which $\lopt$ is close to, or even less than, zero gives people no incentive to invest in productive business activity. Such an economy would appear paralysed, resembling perhaps those periods in history to which economists refer as depressions. We'd like to think that economies are not systematically attracted to such states. The other reasons are more technical, to do with the different interest rates accrued on deposits and loans, and the costs associated with buying and selling assets. These are described in~\cite{PetersAdamou2011}. 


\subsection{Prediction accuracy\seclabel{Prediction_accuracy}}
When we take our theory to the data, we will make systematic errors. For instance, the actual optimal leverage will depend on what actual real asset we choose to represent the risk-free asset -- government bonds, probably, but at what maturity? A portfolio of maturities? And should we only consider interest payments or also changes in bond prices? Similarly, we have to choose something with observable prices to approximate the risky asset in our model. An index, maybe, but which one? What about survivorship bias, and what about dividend payments? How are they taxed? What about market impact when positions are bought and sold? In practice, we have to choose something, and it will be systematically off in some way. 

But ignoring these systematic effects for the moment, we can predict how close we expect observed optimal leverage to be to its predicted value, 1, when we estimate it from a finite time series, let's say of daily returns. Even assuming ideal conditions -- that $\lopt$ really is attracted to 1 and that any effects that lead to systematic mis-estimation can be neglected, we expect random
deviations from $\loptc=1$ to increase as the time series gets shorter. To take an extreme example, with daily data, the observed optimal leverage over a
window of one day does not exist. Either $\loptc \to+\infty$ if the
risky return beats the deposit rate on that day; or $\loptc \to -\infty$ if
deposits beat the risky asset. Indeed, the magnitude of the observed
optimal leverage will diverge for any window over which the daily
risky returns are either all greater than, or all less than, the daily returns on federal
funds. This is unlikely for windows of months or years but using daily data it happens
commonly over windows of days or weeks. Even without this
divergence, shorter windows are more likely to result in larger
positive and negative optimal leverages because relative fluctuations
are larger over shorter time scales.

To quantify this idea we compute $\gm(\xl,\Dt)$, \ie the time-average growth rate observed in an investment following \eref{sde_l} after a finite
time $\Dt$. That's simply the expression on the \RHS of \eref{g_l_noisy} without taking the $\Dt\to\infty$ limit:
\begin{equation}
\gm(\xl,\Dt) =
\mur + \l\mue - \frac{(\l\sigmas)^2}{2} + \frac{\l\sigmas \gW(\Dt)}{\Dt}
%
%\murm+\lm\muem-\frac{(\lm\sigmam)^2}{2}+\frac{\lm\sigmam W(\Dt)}{\Dt}.
\elabel{g_l_finite}
\end{equation}
Maximizing this expression generates a noisy estimate for the optimal leverage over a window of length $\Dt$:
\begin{equation}
\loptc(\Dt, N=1) = \lopt + \frac{\gW(\Dt)}{\sigmas \Dt}.
\end{equation}
Thus, in the model, optimal leverage for finite time series is
normally distributed with mean $\lopt$ and standard deviation
\begin{equation}
\text{stdev}(\loptc(\Dt)) = \frac{1}{\sigmas \Dt^{1/2}}.
\elabel{variance}
\end{equation}
We will use this quantity as the standard error for the prediction $\loptc\approx1$. 


\section{Applications of stochastic market efficiency\seclabel{applications}}
Our solution of the portfolio selection problem, and the concept of stochastic market efficiency that follows from it are momentous developments in the theory of financial markets. While in a sense the puzzle pieces necessary for these developments have been around for a while, it seems that they haven't so far been put together in quite the way we've presented them. This is evident from many puzzles and questions that exist in finance and economics but which have a straight-forward solution in the conceptual space we have developed in these notes. Before we look at data in \secref{real} to understand how accurately this fundamental theoretical work reflects empirical reality, we apply it to some of these famous puzzles to illustrate the type of solution the theory generates.

\subsection{A theory of noise -- Sisyphus discovers prices\seclabel{a_theory}}
According to stochastic market efficiency, prices of risky assets must fluctuate if an excess drift exists, $\mue>0$, simply because the market would otherwise become unstable. But, if price fluctuations are necessary for stability, then the intellectual basis for price efficiency -- that changes in price are driven by the arrival of new economic information --  cannot be the whole truth. Or  --depending on what is meant by ``information'' -- it may be an empty circular statement. At least some component of observed fluctuations must be driven by the leverage feedbacks described in section~\secref{Stochastic}, which enforce leverage efficiency and which have little to do with the arrival of meaningful economic information.

Black differentiated between information-based and other types of price fluctuation, referring to the latter as ``noise'' and regarding it as a symptom of inaccurate information and market {\it in}-efficiency \cite{Black1986}. However, substituting $\lopt=1$ in \eref{lopt} yields 
\be
\elabel{noise}
\left(\sigmas\right)^2=\mue.
\ee
Purely based on stochastic market efficiency, prices must fluctuate, and we can even quantify by how much. An asset whose expectation value grows faster than the value of the riskless asset must fluctuate; otherwise systemic stability will be undermined.
This is a radical departure from conventional thinking, and it has practical consequences. For instance, prices ``discovered'' at ever higher trading frequencies must necessarily reveal more ups and downs, but this noise is self-generated, imposed by the requirement of leverage stability. That stability is the genesis of volatility constitutes a theory of noise requiring no appeal to the arrival of unspecified information, accurate or not.

\subsection{Solution of the equity premium puzzle\seclabel{solution}}
The term ``equity premium'' has been used to describe the ``premium'' I receive for holding ``equity'' -- meaning the extra growth rate my wealth experiences, compared to riskless interest rates, if I invest it in shares, or some other risky asset. We denote this with the symbol $\pim$ (for premium). 

Puzzles usually arrise because we look at things from the wrong angle, with a mental model that doesn't reflect reality. The equity premium puzzle is no exception. The researchers who first studied the equity premium had a model in mind whereby equity prices are set based on the consumption preferences of the population \cite{MehraPrescott1985}. 
We won't go into these models in detail because they're the wrong angle. But the story is this: consumption-based asset pricing models come to the conclusion that no one should ever hold cash. In these models only people who are pathologically terrified of losing money would hold cash, and such people just don't exist. But if you assume that they don't exist, then the models would predict a very different equity premium. So something is not working.
In 2016 LeRoy summarized the state of the debate as follows \cite{LeRoy2016}: ``Most analysts believe that no single convincing explanation has been provided for the volatility of equity prices. The conclusion that appears to follow from the equity premium and price volatility puzzles is that, for whatever reason, prices of financial assets do not behave as the theory of consumption-based asset pricing predicts.''

Of course the difference between the time-average growth rates of the risky asset ($\l=1$) and the riskless asset ($\l=0$) can be measured. It is
\bea
\pim &\equiv& \gtm(1) - \gtm(0)\elabel{epdef_1}\\
&=& \mue-\frac{(\sigmas)^2}{2}.
\elabel{epdef}
\eea
%If we take the S\&P500 total return index (which properly includes dividends) as an example for a risky asset and overnight federal funds as a riskless asset, then the equity premium comes out as about 6.7\% p.a. over the last 30 years (1988--2018). Of course one can argue which assets should be used and what interest rates make sense -- the S\&P500 suffers from survivorship bias -- only successful companies are included. It's not really an asset at all -- its composition keeps changing, and for (good) reasons we won't go into, it weights share prices with the free float, \ie the part of a company's market capitalisation that is freely traded in the market, not held by insiders. In other words it's full of arbitrariness. And why overnight money? That certainly biases the equity premium to a higher value. Why not 10-year bonds, which would give a lower equity premium? This can change the value one measures for $\pim$ by quite a bit. Some authors have even claimed there is no equity premium at all. But it's generally believed that there is one, and that it's somewhere between 2\%~p.a. and 8\%~p.a.
What value would we expect the equity premium to take in a real market? Substituting \eref{noise} into \eref{epdef}, it follows that the equity premium is attracted to
\be
\pim = \frac{(\sigmas)^2}{2}.
\elabel{epval}
\ee
It is important to remember that any theoretical prediction comes with a band of uncertainty around it. The next step is, therefore, to estimate the uncertainty in the equity premium. We do this by considering finite measurement times and substitute \eref{g_l_finite} into \eref{epdef_1}
\be
\text{stdev}(\pim(\Dt)) = \frac{\sigmas}{\Dt^{1/2}}.
\elabel{pim_error}
\ee
In \secref{real} we will use this as our definition of one statistical standard error. The tension may by now have become unbearable, so here's a sneak preview of what's to come empirically: 
Our estimate of $(\sigmas)^2$ for the \DAX since its inception in 1987 is 5.3\%\pa. Half of that is 2.6\%\pa, and the standard error is 4.2\% \pa, meaning the 2-standard-error range around the predicted value of 2.6\%\pa is $-5.8\%\pa\leq \pim\leq 11.0\%\pa$. We will later see that this range is systematically biased: it underestimates the real equity premium. For now we note that in a review of 150 textbooks discussing the equity premium, Fern\'andez finds estimates for $\pim$ in a range from 3\%\pa to 10\%\pa, broadly consistent with our predictions \cite{Fernandez2009}. We hasten to add that the 150 textbooks are not consistent in their definitions of the equity premium, so these numbers can only be a rough guide. The simplicity of our input is important: stability and the standard \GBM model for price dynamics. 

The equity premium puzzle is an interesting case study of economic science in operation. The literature on the puzzle is large and often takes a psychological and individual-specific perspective. For instance, a more risk-averse individual will demand a higher equity premium. Models of human behaviour enter both into the definition of the equity premium -- which lacks consensus \cite{Fernandez2009} -- and into its analysis. The problem is treated in wordy essays, and it's also treated using impenetrably complicated formal models. But this additional complexity generates less, rather than more, comprehension compared to our treatment: it fails to explain the equity premium. It seems to be a result of science incrementally going in one direction and failing to retrace its steps and try a different angle.

The framework we have developed here takes a psychologically na\"{i}ve perspective, without reference to human behaviour or consumption.
It is analytically embarrassingly simple. Only our one key insight was needed: performance over time is not performance of the ensemble. 
We regard the correct prediction of the equity premium by stochastic market efficiency as the long-sought resolution of the equity premium and price volatility puzzles. It is worth noting that the experts disagree with us: the equity premium puzzle, in their minds, is the fact that observed equity premia cannot be explained with consumption-based asset pricing models. In our view that simply means it's time to throw out consumption-based asset pricing models, whereas clinging on to them can be summarized in the cartoon in \fref{cartoon}.
\begin{figure}
\begin{picture}(200,220)(0,0)
    \put(0,0){\includegraphics[width=.8\textwidth, angle=90]{./chapter_markets/figs/cartoon.pdf}}
\end{picture}
\caption{The equity premium puzzle.\flabel{cartoon}}
\end{figure}


\subsection{Central-bank interest rates\seclabel{central-bank}}
Our observations are also relevant to the issue of setting a central bank's lending rate. The rate setter would view the total drift $\mus$ of an appropriate asset or index as given, and the risk-free drift $\mur$ as the central bank's rate. If the aim is to achieve full investment in productive activity without fuelling an asset bubble, then this rate should be set so that $\lopt=1$. Since $\lopt=\mus/(\sigmas)^2$ and $\mue=\mur-\mus$, this is achieved by setting 
\be
\mur=\mus-(\sigmas)^2.
\ee
Using values for $\mus$ and $\sigmas$ estimated in section~\secref{real}, we can determine the corresponding optimal interest rate, which depends on the assets we choose to represent riskless and risky, of course. For example, using interest rates for ten-year maturity Treasury bills as the return for the riskless asset, and the S\&P500 total return index for the risky, between 1988 and 2020 the optimal interest rate comes out as 8.2\% {\it p.a.}, suggesting that actual interest rates over the analysed period have been in a region where they can inflate leverage bubbles (the geometric average on 10-year maturity Treasury bills was about 4.6\% \pa, meaning lower than this value). The effect is mitigated by trading costs, and our estimate of its magnitude depends on what we think are good assets to use in the analysis. So these numbers should be thought of as a proof of principle -- choose your own assets, and build more realistic models to come to a comprehensive understanding of where we are. It is hard to deny, of course, that asset bubbles were inflated over this period. 

The task of the central banker can be seen as the task of estimating $\mur$ and $\sigmas$ in the relevant way. This will involve choices about data and timescales which are far from trivial. For instance, in our data analyses at any given time there is an estimate for $\mus$ and one for $\sigmas$ for each possible length of lookback window. Operational matters aside, stability with respect to leverage is an important consideration for any central bank. Leverage efficiency provides a simple quantitative basis for a rate setting protocol and may frame qualitative discussions about interest rates in a useful way.


\subsection{Fraud detection\seclabel{fraud}}
We will see in the next section that stochastic market efficiency is real. Prices of real traded assets more or less obey the constraints dictated by systemic stability. Of course, over short time scales fluctuations will be large, there are all manner of difficult-to-model effects in real trading environments. These include transaction costs and other operational costs. Still -- a realistic asset should more or less satisfy stochastic market efficiency.

But what if it doesn't? What if there is an asset that consistently, and at low volatility outperforms bank deposits? 
First of all, we now know that such behavior is a challenge to systemic stability. Because of this, we don't expect it to survive for long. Where it does survive for long, one possible explanation is fraud. A common type of fraud is the so-called Ponzi scheme: the fraudster invents an attractive-looking past performance of a portfolio or asset value and entices new investors to give him their money. Instead of investing the money in the non-existent asset, it is passed on to pay off earlier investors and the fraudster siphons off a handy profit. Since the performance of the asset price is invented, there is no strong reason for it to obey stochastic market efficiency. Below we will test the Bernie-Madoff ponzi scheme and ask: was it too good to be true from the perspective of stochastic market efficiency? We will ask the same question about bitcoin: is it too good to be true from this perspective?

\section{Real markets -- stochastic market efficiency in action\seclabel{real}}
In \secref{applications} we went through some earth-shattering consequences of leverage efficiency. Of course we wouldn't have done that if there wasn't strong evidence of this organizing principle actually working in practice. Let's get some data and try it out. 

We test the stochastic efficiency hypothesis in real markets, quite na\"ively, by backtesting leveraged investments. The simplest thing to check is this: take two time series, one of the returns of a very stable, low-volatility, asset, like bank deposits, and the other of the returns of a more volatile asset, like shares. Then try out all possible fixed-leverage investment strategies and see which one does best. The leverage where that happens is a real-world estimate of $\lopt$, which we'll call $\loptc$.

Of course we can argue about the data until the cows come home -- which precise data set should be used, what biases will it introduce, and so on. But let's set those worries aside for the moment and just try something.


\subsection{Backtests of constant-leverage investments}

An investment of constant leverage is backtested using two time series, $\rs(\t)$ (risky returns) and $\rr(\t)$ (returns on bank deposits) as illustrated in \fref{rebalance}.
\begin{enumerate}
\item We start with equity of $\xl(\tn;\l)=\$1$, consisting of $\$\l$ in the risky asset and  $\$(1-\l)$ in bank deposits. 
\item Each day the values of these holdings are updated according to the historical returns, so that 
\bea
\l \xl(\t;\l) &\to& \rs(\t) \l \xl(\t;\l) \\
(1-\l) \xl(\t;\l) &\to&\rr(\t) (1-\l) \xl(\t;\l).
\eea
%In this step, we could introduce fees for borrowing cash (needed for $\l>1$), and fees for borrowing the risky asset (possibly needed for $\l<0$).
\item The portfolio is then rebalanced, \ie some risky holdings are ``bought'' or ``sold''  (swapped for cash) so that the ratio between risky holdings and equity remains $\l$.\\ 
%In this step transaction costs can be introduced.\footnote{On non-trading days the return of the market is zero, while deposits continue to accrue interest. This leads to an unrealistic but negligible rebalancing on those days.}
\end{enumerate}
In step 1, we could include extra fees for borrowing cash or stock, and in step 3 we could include transaction costs. For the sake of simplicity we leave out these effects and refer the reader to \cite{PetersAdamou2011}. The result of such extra complexity is two-fold: portfolios with leverage $\l<0$ or $\l>1$ are penalized by borrowing costs, and any portfolios except those with $\l=0$ or $\l=1$ are penalized by trading costs. Both effects reinforce stochastic market efficiency, in the sense that they bring $\lopt$ closer to the interval $[0,1]$ and usually closer to $1$. 

But in the simplest instance, steps 1--3 result in the following protocol, which is a discretized version of \eref{sde_l}
\be
\xl(\t) = \underbrace{\rs(\t) \l \xl(\t-\Dt) }_{\text{new risky holdings}} + \underbrace{ \rr(\t)(1-\l) \xl(\t-\Dt)}_{\text{new bank deposits}},
\ee
which we repeat until the final day of the backtest, $\tmax$, when the final equity $\xl(\tmax)$ is recorded. 
\Fref{STR_trajectories} shows a few trajectories of leveraged investments in the \SPT at different leverages $\l$.

\begin{figure}
\begin{picture}(300,400)
\put(-5,225){\includegraphics[width=.95\textwidth]{./chapter_markets/figs/SP500-FED_equity_trajectories}}
\put(0,0){\includegraphics[width=.95\textwidth]{./chapter_markets/figs/SP500-FED_final_equity.pdf}}
\end{picture}
\caption{{\bf Top:} equity $\xl(\t)$ as it evolves over time, in investments of initially \$1 in the \SPT at different constant leverages, rebalanced daily as in \fref{rebalance}. For riskless returns we use interest rates of 10-year T-bills. Initially, as leverage increases $\l=0\to1\to2$ the benefits of investing in the \SPT manifest themselves. But eventually, as $\l=3\to4\to5$, harmful fluctuations become so large that returns diminish. Each leverage produces one value for the final equity.
{\bf Bottom:} Final equity, $\xl(\tmax)$, for 500 different leverages (where $\tmax$ is 1 March 2018 in this case). Each leverage produces a ragged time series of equity (top panel). But the final equity as a function of leverage is a smooth curve. The vertical dotted lines indicate our prediction of the position of the optimum at $\l=1$, and the range up to 2 standard errors away.
\flabel{STR_trajectories}
}
\end{figure}

The optimal leverage is that which maximizes the final equity -- the trajectories $\xl(\t)$ are turned into curves $\xl(\tmax)$ by fixing $\t=\tmax$, see \fref{STR_trajectories}. 
If at any time the equity falls below zero, the investment is declared bankrupt and the backtest is considered invalid for the corresponding leverage. This happens when the leverage is very high and the risky asset drops in value, or when it's very negative and the risky asset rises in value. Between these extremes, a smooth curve emerges, resembling a Gaussian bell-shape (or an upside-down parabola on logarithmic vertical scales).

\FloatBarrier 
\subsection{Data sets}
We want to compare the performance of a riskless asset to that of a risky. Ideally, the value of the riskless asset would grow at a fixed exponential rate, and the value of the risky asset would fluctuates as it grows. But reality doesn't provide us with such assets. Something close to a riskless asset is US Treasury bonds: the US government is unlikely to default on its loans (famous last words), not least because the Federal Reserve can always create money to buy such bonds. Without getting too deep into this hypothetical scenario, the dollar value of a bond is not riskless: when the Fed's interest rates change, not only do coupon payments change but also the price of previously issued bonds with different coupon payments. A realistic bond portfolio would have bonds bought at different times, with different maturities, and its value would change over time by quite a bit more than the compounding interest payments that it generates. For a risky asset, we want something like a stock market index. It will fluctuate as it grows and give a broad overall impression of similar assets. It won't depend too much on the specific fortunes of any given company. But such assets, too, come with conceptual problems: not all indexes can actually be bought and sold; some of them are closer to a statistic than to an asset. Many indexes disregard dividend payments, and if they don't -- how should those payments be treated? Dividends are taxed differently from a (paper) profits derived from some position.

We caution readers to think carefully about the issues involved when interpreting our results or when analyzing other assets. All codes for our analyses are available at XXX, and we encourage readers to try out their own favorite asset pairs. Without further ado, we choose the following assets:

\begin{itemize}
\item[\bf Riskless] 
We use \FED overnight US interest rates as the return for the riskless asset.
This is an underestimate of a realistic portfolio of (low-risk) bonds because such a portfolio would generally contain longer-dated bonds. It also ignores the effect of price changes in bonds, which would contribute to the return of such a portfolio.\\
Data from  01-July-1927 until 01-July-1954, reported monthly, are downloaded from \url{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip}\\ 
Data from 01-July-1954 until 01-March-2020 are downloaded from \url{https://fred.stlouisfed.org/series/FEDFUNDS}
\item[\bf Stock index]
The \SP does not include dividends. This underestimates the return for an investment in its constituents. Derivatives written on the value of the \SP make it a tradable asset.
\item[\bf Successful fund]
\BRK was selected with hindsight as an example of a very successful stock-picking fund.  
\item[\bf Bitcoin]
As any scientific model, leverage efficiency applies within some range of conditions. \BTC is an extremely volatile asset and it will be interesting to see to what extent our model applies to it.  
\item[\bf Fraudulent fund]
We take the idea of finding the limits of the range of applicability one step further. Bernie \MAD has officially been declared bullshit. Monthly returns were included in a now famous complaint filed to the \SEC by Harry Markopolos in 2005 \cite{Markopolos2005}. The complaint was ignored by the \SEC, and Madoff happily continued his scheme until the turbulent financial events of 2008 brought to light what he'd been up to. We digitized the returns in Markopolos's complaint and ran the same backtests for Madoff as for the other assets.
\end{itemize}

Here are some reasons why real optimal leverage in the most straight-forward backtest will be overestimated: 
\begin{enumerate}
\item Whenever trading occurs to rebalance the portfolio, trading costs would be incurred in reality. 
\item Money borrowed for leveraging up (and shares borrowed for shorting) comes at a premium that the investor has to pay. 
\item Stock indexes reflect well diversified portfolios of successful companies -- they are affected by survivorship bias because unsuccessful companies leave the index. 
\end{enumerate}
We could try to correct for this bias, and we have done that in \cite{PetersAdamou2011}. The necessary assumptions make the results more realistic but also more dependent on subjective guesses, so we'll just stick to the simplest case here and note that it will produce an over-estimate of $\lopt$. Codes are publicly available at XXX, including for more complex models. You can change assumptions as you like there.
\FloatBarrier

\subsection{Full time series}
The bottom panel of \fref{STR_trajectories} shows the leverage-return curve for the \SP. The optimal leverage is a little greater than 1, but close and well within 2 standard errors from the prediction. Without further ado, let's add to this figure the same curves for \BTC and \BRK, this time on logarithmic vertical scales to see a bit more structure, \fref{STR_final_all}.  
\begin{figure}
\begin{picture}(200,200)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_markets/figs/growth_vs_leverage_all.pdf}}
\end{picture}
\caption{Final equity $\xl(\tn; \l)$ for investments of initially \$1 in the \SP (blue), \BRK (yellow), and \BTC (red). The maximum for the much more volatile \BTC is also near the predicted optimum of $\lopt=1$ -- within 2 standard errors of the prediction, without any correction for the biases introduced by neglecting trading costs. For \BRK the picture is a little different: for this extremely successfully managed fund it would have been optimal to leverage up over the course of its existence (disregarding transaction costs \etc).
\flabel{STR_final_all}}
\end{figure}

The overall impression that emerges is this: leverage efficiency is reasonably satisfied. The shaded areas in \fref{STR_final_all} show the predicted ranges, 2 statistical standard errors to the left and right from $\lopt=1$, and only \BRK lies outside its range. We also see a convincing bias: all estimates are greater than 1, as expected given the bias in the estimate that we mentioned earlier.

But from a scientific point of view, we want to know the limits of the theory. When does it not apply? Where does it break down? Before we looked at the data we thought that Bitcoin might be an example of an asset that doesn't obey stochastic market efficiency. But it looks just as predicted -- unspectacular. We didn't have a specific reason to suspect that the theory wouldn't apply to Bitcoin, it just seemed like an extreme asset, unlike the assets we had in mind when we developed the theory. 

But let's see what happens where the theory really doesn't apply. In \fref{STR_final_all_MAD} we add to the curves of \fref{STR_final_all} the return-vs.-leverage curve for \MAD. This is instructive because it shows just how different the results could have been. If it weren't for leverage efficiency, optimal leverage for the \SP, for example, could be miles away from our prediction. We have to show the Madoff data in a separate figure 
because the scales are so different that the differences between \BTC, \SP, and the \BRK become barely visible, and we thought they may be of interest.
Apart from showing the limits of the range of applicability of our theory, the \MAD data suggest that leverage-return curves can help detect investments that are too good to be true, \ie fraud. 
\begin{figure}
\begin{picture}(200,200)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_markets/figs/compare_assets.pdf}}
\end{picture}
\caption{Final equity for investments of initially \$1 in the \SP (blue), the \BRK (yellow), \BTC (red), and Bernie Madoff's ponzi scheme (green). 
The maxima for the real assets cluster near the predicted value $\lopt=1$. Madoff's freely invented performance had such low volatility that a leverage of 100 would have been optimal, even if rebalancing could have only occurred once per month. At higher rebalancing frequencies (had the asset actually existed) the green curve would be seen to be the left end of a parabola whose maximum may well be at 200 or 300.
\flabel{STR_final_all_MAD}}
\end{figure}

Of course, Madoff could have chosen the invented returns of his fund in a way that's consistent with stochastic market efficiency, just to have something credible to show to prospective victims of his fraud. But apparently he didn't bother to do that, and people believed that he'd really found a way to print money. As a former CEO of Nasdaq he himself had sufficient credibility, never mind the data.

That leverage efficiency really doesn't apply to Madoff's returns is a good sign -- showing that a theory fails to predict things it shouldn't be able to predict is important. A theory about asset price fluctuations that applies even to fictitious prices like Madoff's would be suspicious -- it would probably not say much about prices at all but just be a statement that's generally true for a much broader class of phenomena. Having said this, introducing realistic transaction costs, leveraging something by a factor of 100 or 200 is extremely expensive, and the Madoff optimal leverage comes down significantly in a more realistic setup. Nonetheless, in \fref{STR_final_all_MAD} Madoff's returns have a statistical signature that is unequivocally different from prices arising through trading that we have investigated.

\subsection{Shorter time scales}
In \secref{Prediction_accuracy} we derived an expression, \eref{variance}, for the variance of optimal leverage that we expect to observe in a finite time series, and we used this to arrive at a statistical error estimate, a scale on which to judge whether leverage efficiency holds or not.
But we can use \eref{variance} as a prediction in itself: is the sample variance of $\lopt$, measured in many windows of size $\Dt$ well described by \eref{variance}? We can't make the time series longer than they are, but we can make them shorter. In \fref{loglog} we collect statistics of $\loptc$ estimated in non-overlapping time windows and find good agreement with the model-specific prediction. 
%\fref{g_l_linear} shows the simulated multiplicative return as a function of leverage for an investment over the entire time series.
%\begin{figure}
%\centering
%\includegraphics[width=\textwidth]{./chapter_markets/figs/sme_fig1.pdf}
%\caption{Total return for a constant-leverage investment in the S\&P500, starting $4^{\text{th}}$ August 1955 and ending $21^{\text{st}}$ May 2013 as a function of the leverage.
%\newline \underline{Red line:} Simulation 1. Interest at federal funds rate on bank deposits and loans. No stock borrowing costs. No transaction costs.
%\newline \underline{Yellow line:} Simulation 2. As 1, but with interest at federal funds rate on borrowed stock. This introduces a kink at $\l=0$, where stock borrowing begins.
%\newline \underline{Green line:}\ Simulation 3. As 2, but with interest charged at the higher bank prime rate on borrowed cash and stock. This introduces a kink at $\l=1$.
%\newline \underline{Blue line:}\ Simulation 4. As 3, but with a transaction cost of 0.2\% of the value of the assets traded on rebalancing.\flabel{g_l_linear}}
%\end{figure}
%The four curves in the figure correspond to four sets of assumptions about interest rates and transaction costs. These are labelled 1--4 in order of increasing complexity and resemblance to actual practices in financial markets. We will not dwell on these here, although brief descriptions are provided in the figure caption.
%
%$\lopt=0.97$ for simulations 1 (which we will refer to as the simple case), 2, and 3. $\lopt=1.00$ for the most realistic simulation, 4 (the complex case). These results appear to lend great support to the stochastic market efficiency hypothesis. Based on simple thought experiments motivated by the theories developed in these lectures, we predicted that $\lopt$ in a real market should be close to unity. 58 years of real market data confirm this prediction. We discuss below the statistical significance of these results.
%
%The kinks\footnote{A non-technical term for discontinuities in the derivative.} in the return-leverage curve can be accompanied by a change in sign of the derivative. When this happens, the kink is a global maximum and $\lopt$ is fixed at that leverage, either 0 or 1. This makes these special leverages sticky, in that $\lopt$ can get trapped there.\footnote{Although typically only when the $\lopt$ of simulation 1 is already in or very close to the range, $0\leq\lopt\leq1$.}. This stickiness will tend to promote the likelihood of the hypothesis being confirmed. However, simulation 1 shows that $\lopt\approx1$ even when these effects are neglected.
%
%\textbf{Parameter estimation}\\
%\fref{g_l_logarithmic} shows the simulated growth rate as a function of leverage for the simple case (simulation 1) over the entire time series.
%\begin{figure}
%\centering
%\includegraphics[width=\textwidth]{./chapter_markets/figs/sme_fig2.pdf}
%\caption{Computed time-average growth rates closely follow a parabola as a function of leverage.\flabel{g_l_logarithmic}}
%\end{figure}
%This is the logarithm of the simulated return of \fref{g_l_linear} divided by the window length. Since the window is long, we might expect to approximate the time-average growth rate, $\gtm(\l)$, which we know from \eref{g_l_quadratic} is a parabola in the original \GBM model. The black dashed line is a fitted parabola, whose parameters can be taken as meaningful definitions of the effective values of $\mur$, $\mue$, and $\sigmas$ for the S\&P500 for the entire time series.\footnote{We say effective because, although we think of these parameters as being time-varying in the relaxed model, their values from the parabolic fit are those of the original constant-parameter model which produce an almost identical outcome for $\gtm(\l)$ as the real data.} A least-squares fit estimates these parameters as $\mur=5.2\%$ \textit{pa}, $\mue=2.4\%$ \textit{pa}, and $\sigmas=16\%$ \textit{p$\sqrt{\mathit{a}}$}.
%
%The deviation from parabolic form for high and low leverages in \fref{g_l_logarithmic} is due to extremely large fluctuations in the index, which are much less rare than would be observed in a true \GBM. These result in large losses and, indeed, bankruptcy for highly leveraged portfolios. That real returns distributions are observed to have fatter tails than those predicted by \GBM is an oft-made criticism of classical theory. In this study it is not especially relevant: indeed, the existence of rogue fluctuations strengthens the stochastic efficiency hypothesis, in that it penalises high leverage strategies.
%
%\textbf{Finite time scales}\\
%In the real world, of course, we can never truly observe the time-average growth rate of a leveraged investment since this would require an infinite observation time. Instead we observe the finite-time growth rate over a window of length $\Dt$. This is a random variable whose distribution broadens as $\Dt\to0$
%
%Likewise, we never truly observe $\lopt$ either, since this is the leverage that maximises a time average-growth rate. Instead we observe simulated optimal leverages which maximise finite-time growth rates. We can guess that these will show larger fluctuations from the underlying $\lopt$ as the simulation window gets shorter, because short periods containing a sequence of almost all positive or negative stock price movements will result in very high (possibly infinite) positive or negative simulated optimal leverages.
%
%More formally, let's denote by $\loptc(\Dt)$ the simulated optimal leverage which maximises the finite-time growth rate, $\gm(\xl,\Dt)$. As $\Dt\to0$, $\gm(\xl,\Dt)$ becomes a worse estimator of the time-average growth rate, $\gtm(\l)$, because its distribution becomes broader. Thus $\loptc(\Dt)$ becomes a similarly worse estimator of $\lopt$. 
%
%This is important because a single observation of $\loptc(\Dt)$ consistent with our hypothesis (such as the one we just made for $\Dt\approx58$ years) is only significant if the uncertainty in $\loptc(\Dt)$ is of the same order of magnitude as $\loptc(\Dt)$ itself. For example, if we knew that $\loptc(\Dt)$ had a distribution that could place it between, say, -10 and 10 with reasonable probability, then we couldn't read much from a single observation either inside or outside the attractive range of our hypothesis. If, on the other hand, most of the distribution's mass were inside the range, then an observation outside would be strong evidence that the hypothesis is flawed, and so an observation inside is significant.
%
%We can quantify these ideas in the original model by discarding the $\Dt\to\infty$ limit in \eref{g_l_noisy}. This gives
%\be
%\gm(\xl,\Dt) = \mur+\l\mue-\frac{\l^2\sigmas^2}{2}+\frac{\l\sigmas \gW(\Dt)}{\Dt},
%\elabel{g_l_finite}
%\ee
%which is maximised at
%\be
%\loptc(\Dt) = \lopt + \frac{\gW(\Dt)}{\sigmas\Dt}.
%\elabel{lopt_finite}
%\ee
%This is normally distributed with mean $\lopt$ and standard deviation
%\be
%\text{stdev}[\loptc(\Dt)] = \frac{1}{\sigmas\sqrt{\Dt}}.
%\elabel{lopt_sd}
%\ee
%Using the computed volatility of $16\%$ per square root of one year, this standard deviation is approximately 0.83. This means that the uncertainty in $\loptc(\Dt)$ is about the same size as the hypothesised value of $\lopt$, and so the single observation we made is a significant corroboration of the hypothesis.
%
%We can also test the validity of the relationship in \eref{lopt_finite} by simulating investments of different window lengths in the market data and compiling histograms of the resulting $\loptc(\Dt)$. \fref{loglog} shows, on double-logarithmic scales, the standard deviation of $\loptc(\Dt)$ as a function of $\Dt$ for the simple simulation.
\begin{figure}
\begin{picture}(200,230)(0,0)
    \put(0,0){\includegraphics[width=\textwidth]{./chapter_markets/figs/lopt_var.pdf}}
\end{picture}
\caption{Dots show the standard deviation of samples of $\loptc(\Dt)$ as a function of window length measured in days. Dashed line shows the prediction of \eref{variance} using the estimate of $\sigmas$ obtained by fitting a parabola to curves like in \fref{STR_final_all} (blue for \SP, yellow for \BTC).\flabel{loglog}}
\end{figure}

The structure we have identified -- leverage efficiency -- operates on all time scales from weeks to decades. This broad range of applicability supports our theory of noise in \secref{a_theory}: it is a requirement of stability that asset prices fluctuate, and the shorter the time scales the less significant any economic information becomes. Thinking of price changes as a reflection of new information may be relevant on time scales of years or decades, where the real economic fortunes of companies become apparent. On time scales of days or minutes, let alone milliseconds, price movements usually have nothing to do with economic information.

%\footnote{For shorter time scales, the standard deviation is slightly higher than predicted. We suspect this is due to discretisation effects: for investments over a small number of days, very high optimal leverages can be observed when the window contains predominantly positive or negative index movements. These would no longer be optimal if fluctuations and rebalancing were simulated on smaller time scales, \ie intra-day. For longer time scales, the standard deviation drops below the prediction. This is because for window lengths approaching the entire period under study, the number of independent windows in the sample is small, and so the standard deviation of the sample is depressed.}
%
%\fref{l_opt_expanding} shows the simulated $\loptc(\Dt)$ for the investment window starting on $4^\text{th}$ August 1955 and ending on the date on the horizontal axis.
%\begin{figure}
%\includegraphics[width=\textwidth]{./chapter_markets/figs/sme_fig4.pdf}
%\caption{Daily simulated optimal leverages for an expanding window with start date $4^\text{th}$ August 1955 and end date on the horizontal axis. Both simple (red line) and complex (blue line) simulations are shown. The broken magenta lines show the one- and two-standard deviation envelopes about $\lopt=1$, based on \eref{lopt_sd} and the estimate of $\sigmas$ from the parabolic fit in \fref{g_l_logarithmic}.\flabel{l_opt_expanding}}
%\end{figure}
%The reduction in fluctuations with increasing window length are broadly consistent with \eref{lopt_sd} and support the hypothesis that $\lopt$ -- as estimated by $\loptc(\Dt)$ -- is attracted to the range $0\leq\lopt\leq1$ and, over long time scales, to $\lopt=1$ in particular.
%
%We can also plot the simulated optimal leverages for investment windows with fixed lengths and moving start date. We do this for the simple and complex simulations in \fref{l_opt_expanding}, with window lengths ranging from 5 to 40 years.
%\begin{figure}
%\includegraphics[width=1.\textwidth]{./chapter_markets/figs/sme_fig5a.pdf}
%\includegraphics[width=1.\textwidth]{./chapter_markets/figs/sme_fig5b.pdf}
%\caption{\newline
%(a) In the simple simulation, observed optimal leverage fluctuates strongly on short time scales but appears to converge to $\loptc(\Dt)=1$ on long time scales. This constitutes the central result of the study.
%\newline
%(b) In the complex simulation, the kinks in \fref{g_l_linear} ensure that $\loptc(\Dt)=0,1$ are often found exactly. The 40-year simulation supports the strong stochastic efficiency hypothesis, that real markets are attracted to $\lopt=1$, with a dip to $\lopt=0$ only during the financial crisis of 2008. \flabel{l_opt_fixed}}
%\end{figure}
%From the strong fluctuations over short time scales emerges attractive behaviour consistent with our refined hypothesis. The effects of the stickiness of the points $\lopt=0$ and $\lopt=1$ in the complex model are clearly visible. In particular, over the last decade or so optimal leverage for the 20- and 40-year windows remained close to unity.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
The primary aim of this final lecture was to demonstrate that the mathematical formalism we have developed to conceptualise randomness in economics, which started with a simple model of wealth evolution by repetition of a gamble, is very powerful. It does more than simply create a rigorous and plausible foundation for economic theory. In particular, because the framework is epistemologically sound, we can make testable predictions, which we can corroborate empirically using real economic data. We could not have guessed from our simple coin-tossing game that we would end up making a prediction about the fluctuations of stock markets or those of Bitcoin. The simple but deep conceptual insight that the ergodicity question has been overlooked ultimately shed a good deal of light on the equity premium puzzle, informed the debate of how to set central-bank interest rates, and provided a new way of thinking about noise and the stabilizing role of price fluctuations. What other surprising predictions can we make in this formalism? That, dear reader, is a question for you.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
