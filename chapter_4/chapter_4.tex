\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Markets}

{\it This lecture applies the ideas developed in the first two lectures to markets. It is based on two recent papers~\cite{Peters2011a,PetersAdamou2013}.

Firstly, we set up a simple portfolio selection problem in a market of two assets: one risky, like a stock; and the other riskless, like a bank deposit. We ask how an investor should best allocate his money between the two assets, which we phrase in terms of his leverage. We review the classical approach, which can't answer this question without additional information about the investor's risk preferences. We then use the decision theory we've developed so far to answer the question unambiguously, by deriving the optimal leverage which maximises the investment's time-average growth rate.

Secondly, we consider what this objectively defined optimal leverage might mean for financial markets themselves. If all the participants in a market aim for the same optimal leverage, does this constrain the prices and price fluctuations that emerge from their trading? We argue that it does, we quantify how, and we confirm our prediction using data collected from the American stock markets over almost sixty years.
}
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimal leverage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{A model market}
We consider assets whose values follow multiplicative dynamics, which we will model using \GBM. In general, an amount $\x$ invested in such an asset evolves according to the \SDE,
\be
d\x = \x(\gmu d\t + \gsigma d\gW),
\elabel{sde_gbm}
\ee
where $\gmu$ is the drift and $\gsigma$ is the volatility. By now we are very familiar with this equation and how to solve it.

To keep things simple, we imagine a market of two assets. One asset is riskless: the growth in its value is known deterministically and comes with a cast-iron guarantee.\footnote{Such guarantees are easy to offer in a model. In the real world, one should be very suspicious of anything that comes with a ``cast-iron guarantee''.} This might correspond in reality to a bank deposit. The other asset is risky: there is uncertainty over what its value will be in the future. This might correspond to a share in a company or a collection of shares in different companies. We will think of it simply as stock.

An amount $\xzero$ invested in the riskless asset evolves according to
\be
d\xzero = \xzero \mur d\t.
\elabel{sde_0}
\ee
$\mur$ is the riskless drift, known in finance as the riskless rate of return.\footnote{In general we will eschew financial terminology for rates. Economics has failed to define them clearly, with the result that different quantities, like $\gt$ and $\gex$, are often conflated. The definitions developed in these lectures are aimed at avoiding such confusion.} There is no volatility term. In effect, we have set $\gsigma=0$. We know with certainty what $\xzero$ will be at any point in the future: 
\be
\xzero(\tn+\Dt) = \xzero(\tn)\exp(\mur\Dt).
\elabel{sde_0_soln}
\ee

An amount $\xone$ invested in the risky asset evolves according to
\be
d\xone = \xone ( \mus d\t + \sigmas d\gW ),
\elabel{sde_1}
\ee
where $\mus>\mur$ is the risky drift and $\sigmas>0$ is the volatility (the subscript s stands for stock). $\mus$ is also known in finance as the expected return.\footnote{Probably because it's the growth rate of the expected value, see \eref{exp_ret}.} This equation has solution
\be
\xone(\tn+\Dt) = \xone(\tn)\exp\left[\left(\mus -\frac{\sigmas^2}{2}\right)\Dt + \sigmas \gW(\Dt)\right],
\elabel{sde_1_soln}
\ee
which is a random variable.

The difference
\be
\mue=\mus-\mur
\elabel{def_mue}
\ee
is known variously as the excess return, the risk premium, and in stock markets as the equity premium. We will refer to it simply as the excess drift. It will be a very important quantity in later discussions. For the moment we can think of it as compensation for accepting the uncertain outcome of \eref{sde_1_soln} instead of the guaranteed result of \eref{sde_0_soln}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Leverage}
Let's turn to the concept of leverage. Imagine a very simple portfolio of value $\xl$, out of which $\l \xl$ is invested in stock and the remainder, $(1-\l)\xl$, is put in the bank. $\l$ is known as the leverage. It is the fraction of the total investment assigned to the risky asset. $\l=0$ corresponds to a portfolio consisting only of bank deposits. $\l=1$ corresponds to a portfolio only of stock.

You would be forgiven for thinking that prudence dictates $0\leq\l\leq1$, \ie that we invest some of our money in stock and keep the rest in the bank. However, the financial markets have found all sorts of exciting ways for us to invest almost any amount in an asset. For example, we can make $\l>1$ by borrowing money from the bank to buy more stock than we could have bought with only our own money.\footnote{This doesn't immediately affect the portfolio's value. The bank loan constitutes a negative investment in the riskless asset, whose value cancels the value of the stock we bought with it. Of course, the change in the portfolio's composition will affect its future value.} We can even make $\l<0$ by borrowing stock (a negative investment in the risky asset), selling it, and putting the money raised in the bank. In the financial world this practice is called short selling.

Each investment in our portfolio experiences the same relative fluctuations as the asset in which it has been made. The overall fluctuation in the portfolio's value is, therefore,
\be
d\xl = (1-\l)\xl \frac{d\xzero}{\xzero} + \l \xl \frac{d\xone}{\xone}.
\ee
Substituting in \eref{sde_0} and \eref{sde_1} gives the \SDE for a leveraged investment in the risky asset,
\be
d\xl = \xl [ (\mur + \l\mue) d\t + \l\sigmas d\gW ],
\elabel{sde_l}
\ee
with solution,
\be
\xl(\tn+\Dt) = \xl(\tn)\exp\left[\left(\mur+\l\mue-\frac{\l^2\sigmas^2}{2}\right)\Dt+\l\sigmas \gW(\Dt)\right].
\elabel{sde_l_soln}
\ee
We can now see why we labelled investments in the riskless and risky assets by $\xzero$ and $\xone$: when $\l=0$, $\xl$ follows the same evolution as $\xzero$; and when $\l=1$, it evolves as $\xone$.

In our model $\l$, once chosen, is held constant over time. This means that our model portfolio must be continuously rebalanced to ensure that the ratio of stock to total investment stays fixed at $\l$. For example, imagine our stock investment fluctuates down over a short time-step, while our bank deposit accrues a little interest. Immediately we have slightly less than $\l$ of the portfolio's value in stock, and slightly more than $1-\l$ of its value in the bank. To return the leverage to $\l$, we need to withdraw some money from the bank and use it to buy more stock. In \eref{sde_l} we are imagining that this happens continuously.\footnote{In reality, of course, that's not possible. We could try to get close by rebalancing frequently. However, every time we buy or sell an asset in a real market, we pay transaction costs, such as broker's fees and transaction taxes. This means that frequent rebalancing in the real world can be costly.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Portfolio theory\seclabel{pf_theory}}
Our simple model portfolio parametrised by $\l$ allows us to ask the
\begin{keypts}{Question:}
What is the optimal value of $\l$?
\end{keypts}
This is similar to choosing between gambles, for which we have already developed a decision theory. The main difference is that we are now choosing from a continuum of gambles, each characterised by a value of $\l$, whereas previously we were choosing between discrete gambles. The principle, however, is the same: we will maximise the time-average growth rate of our investment.

Before we do this, let's review the classical treatment of the problem so that we appreciate the wider context. Intuitively, people understand there is some kind of trade-off between risk and reward. In our model of a generic multiplicative asset, \eref{sde_gbm}, we could use $\gsigma$ as a proxy for risk and $\gmu$ as a proxy for reward. Ideally we want an investment with large $\gmu$ and small $\gsigma$, but we also acknowledge the rule-of-thumb that assets with larger $\gmu$ tend to have larger $\gsigma$.\footnote{A ``no such thing as a free lunch'' type of rule.} This is why we model our risky asset as having a positive excess return, $\mue>0$, over the riskless asset.

Intuition will only take us so far. A rigorous treatment of the portfolio selection problem was first attempted by Markowitz in 1952~\cite{Markowitz1952}. He suggested defining a portfolio with parameters $(\gsigma_i, \gmu_i)$ as efficient if there exists no rival portfolio with parameters $(\gsigma_j, \gmu_j)$ with $\gmu_j\geq\gmu_i$ and $\gsigma_j\leq\gsigma_i$.
% AA: simplified definition of Markwoitz-efficient for clarity at tiny cost of rigour
%for which at least one of the following statements is true:
%\begin{enumerate}
%\item $\gmu_j>\gmu_i$ and $\gsigma_j\leq\gsigma_i$;
%\item $\gsigma_j<\gsigma_i$ and $\gmu_j\geq\gmu_i$.
%\end{enumerate}
%In other words, if the rival portfolio has higher $\gmu$, it had better have higher $\gsigma$; or if it has lower $\gsigma$, it had better have lower $\gmu$.
Markowitz argued that it is unwise to invest in a portfolio which is not efficient.

In our problem, we are comparing portfolios with parameters $(\l\sigmas, \mur+\l\mue)$. These lie on a straight line in the $(\gsigma, \gmu)$-plane,\footnote{Derived by eliminating $\l$ from the equations $\gmu=\mur+\l\mue$ and $\gsigma=\l\sigmas$.}
\be
\gmu = \mur + \left(\frac{\mue}{\sigmas}\right)\gsigma,
\elabel{frontier}
\ee
shown schematically in \fref{markowitz}.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{./chapter_4/figs/markowitz.pdf}
\caption{The Markowitz portfolio selection picture. The red dots are the locations in the $(\gsigma,\gmu)$-plane of portfolios containing only the riskless (left) or risky (right) asset. The blue dot is one possible leveraged portfolio, in this case with $\l>1$. All possible leveraged portfolios lie on the black line, \eref{frontier}. The grey dots are hypothetical alternative portfolios, containing different assets excluded from our simple portfolio problem. Their location below and to the right of the black line makes them inefficient under the Markowitz scheme.\flabel{markowitz}}
\end{figure}
Under Markowitz's classification, all of the leveraged portfolios on this line are efficient.\footnote{Indeed, in finance this line is called the efficient frontier.} Therefore, any leverage we choose gives a portfolio in which we are not counselled against investing. This does not help answer our question. By itself, Markowitz's approach is agnostic to leverage: it requires additional information to distinguish between portfolios. Markowitz was aware of this limitation and argued that the optimal portfolio could be identified by considering the investor's risk preferences.\footnote{``The proper choice among portfolios depends on the willingness and ability of the investor to assume risk.''~\cite{Markowitz1991}.} Typically this is modelled by the introduction of a utility function, with which approach we are now familiar.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sharpe ratio\seclabel{sharpe}}
The Sharpe ratio~\cite{Sharpe1966} for an asset with drift $\gmu$ and volatility $\gsigma$ is defined as
\be
\Sharpe\equiv\frac{\gmu-\mur}{\gsigma}
\elabel{def_sharpe}
\ee
It is the gradient of the straight line in the $(\gsigma, \gmu)$-plane which passes through the riskless asset and the asset in question. $\Sharpe$ is often used as a convenient shorthand for applying Markowitz's ideas, since choosing the portfolio with the highest $\Sharpe$ from the set of available portfolios is equivalent to choosing an efficient portfolio. In our scenario, however, we can immediately see why it sheds no light. All of our leveraged portfolios lie on the same line, \eref{frontier}, and so all of them have the same Sharpe ratio, which is simply the line's gradient:
\be
\Sharpe_\l = \frac{\mue}{\sigmas}.
\elabel{sharpe_l}
\ee
This is insensitive to the leverage $\l$, resulting in the same non-advice as the Markowitz approach. Sharpe also suggested considering risk preferences to resolve the optimal portfolio.\footnote{``The investor's task is to select from among the efficient portfolios the one that he considers most desirable, based on his particular feelings regarding risk and expected return.''~\cite{Sharpe1966}.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Expected return\seclabel{exp_ret}}
We noted previously that the growth rate of the expected value of the risky asset is the risky drift, $\mus$, also known as the expected return. This is because
\be
\ave{\xone(\tn+\Dt)} = \ave{\xone(\tn)}\exp(\mus \Dt),
\elabel{exp_ret}
\ee
which, as a multiplicative process, has growth rate
\be
\gm(\ave{\xone}) = \frac{\D\ln\ave{\xone}}{\Dt} = \mus.
\ee
It follows immediately from comparison of \eref{sde_1} and \eref{sde_l} that the expected value of the leveraged portfolio grows at
\be
\gm(\ave{\xl}) = \mur+\l\mue.
\elabel{exp_ret_l}
\ee
This illustrates why a portfolio theory which is insensitive to leverage, such as that of Markowitz and Sharpe\footnote{Both recipients of the 1990 Alfred Nobel Memorial Prize in Economic Sciences.}, is potentially dangerous. If any leverage is admissible, then an investor misguidedly seeking to maximise his expected return in \eref{exp_ret_l} would maximise his leverage, $\l\to\infty$. This, as we will shortly see, would almost surely ruin him.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Growth rate maximisation}
We now understand the classical approach, as applied to a very simple portfolio problem, and we are aware of its limitations. What does our own decision theory have to say?

The time-average growth rate of the leveraged portfolio is
\be
\gtm(\l) \equiv \lim_{\Dt\to\infty} \left\{ \gm(\xl,\Dt) \right\} = \lim_{\Dt\to\infty} \left\{ \frac{\D\ln \xl}{\Dt} \right\}.
\ee
This will depend on $\l$. Inserting the expression for $\xl$ in \eref{sde_l_soln} gives
\be
\gtm(\l) = \lim_{\Dt\to\infty} \left\{ \frac{1}{\Dt} \left[ \left(\mur + \l\mue - \frac{\l^2\sigmas^2}{2}\right)\Dt + \l\sigmas \gW(\Dt) \right] \right\},
\elabel{g_l_noisy}
\ee
which, since $\gW(\Dt)/\Dt\sim\Dt^{-1/2}\to0$ as $\Dt\to\infty$, converges to
\be
\boxed{\; \gtm(\l) = \mur + \l\mue - \frac{\l^2\sigmas^2}{2}. \;}
\elabel{g_l_quadratic}
\ee
This is a quadratic in $\l$ with an unambiguous maximum\footnote{Derived, for example, by setting $\frac{d\gtm}{d\l}=0$.} at
\be
\boxed{\; \lopt = \frac{\mue}{\sigmas^2}. \;}
\elabel{lopt}
\ee
$\lopt$ is the optimal leverage which defines the portfolio with the highest time-average growth rate. Classical theory is indifferent to where on the line in \fref{markowitz} we choose to be. Our decision theory, however, selects a particular point on that line,\footnote{Subsequently Markowitz became aware of this point, which he called the ``Kelly-Latan\'{e}'' point in~\cite{Markowitz1976}, referring to~\cite{Kelly1956,Latane1959}.} which answers the question we posed at the start of \secref{pf_theory}. This is the key result. We note that it requires no additional knowledge to the parameters $\mur$, $\mus$, and $\sigmas$ of the two assets in our market. In particular, it is defined objectively, with no reference to idiosyncrasies of the investor (other than that we assume him to be a time-average growth rate maximiser).

\eref{g_l_quadratic} gives the time-average growth rate along the efficient frontier, where all of our leveraged portfolios lie. In fact, it's easy to calculate the growth rate for any point in the $(\gsigma,\gmu)$-plane: it is simply $\gmu-\gsigma^2/2$. Overlaying the Markowitz picture in \fref{markowitz} on the growth rate landscape is illuminating. In effect, it adds the information missing from the classical model, which was needed to distinguish between portfolios. This is shown in \fref{markowitz_peters}.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{./chapter_4/figs/markowitz_peters.png}
\caption{The augmented portfolio selection picture. The Markowitz picture is shown in green, with our model leveraged portfolios on the straight line (the efficient frontier) and hypothetical alternative portfolios within the ellipse (analogous to the grey dots in \fref{markowitz}). This is overlaid on a colour plot of the time-average growth rate, $\gt$. The optimal leverage, $\lopt\approx1.54$, is marked at the location of the highest $\gt$ on the efficient frontier. Portfolios on the white curve have $\gt=0$. Eventually this will intersect the efficient frontier, at which point applying more leverage will produce a portfolio with negative long-run growth. Parameters are $\mur=\mue=0.05$ per unit time and $\sigmas=0.18$ per square root time (denoted by $\gmu_\text{riskless}$, $\gmu_\text{excess}$, and $\gsigma_\text{M}$ in the figure). Adapted from~\cite{Peters2011a}.\flabel{markowitz_peters}}
\end{figure}

That \eref{g_l_quadratic} defines an inverted parabola means that, even on the efficient frontier, there exist portfolios with $\gtm(\l) < 0$. These occur for $\l<\lm$ and $\l>\lp$, where
\be 
\lpm \equiv \lopt \pm \sqrt{\lopt^2 + \frac{2\mur}{\sigmas^2}}.
\ee
This confirms our assertion at the end of \secref{exp_ret}, that an investor maximising his leverage in either direction will, if he is able to apply enough leverage, subject his wealth to negative time-average growth. Indeed, if his leveraging ability is unlimited, he will find to his horror what is easily seen in \eref{g_l_quadratic}, that $\gtm(\l)$ diverges negatively as $\l\to\pm\infty$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stochastic market efficiency}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{A fundamental measure\seclabel{fundamental}}
Aside from being insensitive to leverage, the Sharpe ratio, $\Sharpe=\mue/\sigmas$, is a dimensionful quantity. Its unit is $(\text{time unit})^{-1/2}$. This means that its numerical value is arbitrary (since it depends on the choice of time unit) and tells us nothing fundamental about the system under study. For example, a portfolio with $\Sharpe=5$ per square root of one year has $\Sharpe=5(365)^{-1/2}\approx0.26$ per square root of one day. Same portfolio, different numbers.

The optimal leverage, $\lopt=\mue/\sigmas^2$, which differs from the Sharpe ratio by a factor of $1/\sigmas$, is a dimensionless quantity. Therefore, its numerical value does not depend on choice of units and has the potential to carry fundamental information about the system.\footnote{See Barenblatt's modern classic on scaling~\cite{Barenblatt2003}.} We could view $\lopt$ as the fundamental measure of a portfolio's quality, similarly to how $\Sharpe$ is viewed in the classical picture. A portfolio with a high optimal leverage must represent a good investment opportunity to justify such a large commitment of the investor's funds.

However, the significance of $\lopt$ runs deeper than this. The portfolio to which $\lopt$ refers is that which optimally allocates money between the risky and riskless assets in our model market. Therefore, it tells us much about conditions in that market and, by extension, in the wider model economy of which we might imagine it is a part. A high $\lopt$ indicates an economic environment in which investors are incentivised to take risks. A low or negative $\lopt$ indicates the converse. This raises a tantalising
\begin{keypts}{Question:}
Are there any special numerical values of $\lopt$ which describe different market regimes, or to which markets are attracted?
\end{keypts}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Relaxing the model}
Our model market contains assets whose prices follow \GBM with constant drift and volatility parameters.\footnote{A tautology, since \eref{sde_gbm} only describes a \GBM if $\gmu$ and $\gsigma$ are constant in time.} Once specified, $\mur$, $\mus$, and $\sigmas$ are static and, therefore, so is $\lopt$. This limits the extent to which we can explore the question, since we cannot consider changes in $\lopt$. To make progress we need to relax the model. We must consider which parts of the model are relevant to the question, and which parts can be discarded without grave loss.

The \GBM-based model is useful because it motivates the idea of an objectively optimal leverage which maximises the growth of an investment over time. It also provides an expression for $\lopt$ in terms of parameters which, in essence, describe the market conditions under which prices fluctuate. These parameters have correspondences with quantities we can measure in real markets. All of this is useful.

However, the real markets in which we are ultimately interested contain assets whose prices do not follow \GBM (because nothing in nature\footnote{We consider markets to be part of nature and appropriate subjects of scientific study.} truly does). In particular, real market conditions are not static. They change over time, albeit on a longer time scale than that of the price fluctuations. In this context, the model assumption of constant $\mur$, $\mus$, and $\sigmas$ is restrictive and unhelpful. We will relax it and imagine a less constrained model market in which these parameters, and therefore $\lopt$, are allowed to vary slowly. We will not build a detailed mathematical formulation of this, but instead use the idea to run some simple thought experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Efficiency}
One way of approaching a question like this is to invoke the concept of efficiency. In economics this has a specific meaning in the context of financial markets, which we will mention imminently. In general terms, an efficient system is one which is already well optimised and whose performance cannot be improved upon by simple actions. For example, a refrigerator is efficient if it maintains a cool internal environment while consuming little electrical power and emitting little noise. Similarly, Markowitz's portfolios were efficient because the investor could do no better than to choose one of them. 

The ``efficient market hypothesis'' of classical economics treats markets as efficient processors of information. It claims that the price of an asset in an efficient market reflects all of the publicly available information about it. The corollary is that no market participant, without access to privileged information, can consistently beat the market simply by choosing the prices at which he buys and sells assets. We shall refer to this hypothesis as ordinary efficiency.\footnote{This is not the most precise hypothesis ever hypothesised. What does it mean for a price to ``reflect'' information? Presumably this involves some comparison between the observed price of the asset and its true value contingent on that information. But only the former is observable, while the latter evades clear definition. Similarly, what does it mean to ``beat the market''? Presumably something to do with achieving a higher growth rate than a general, na\"{i}ve investment in the overall market. But what investment, exactly? We will leave these legitimate questions unanswered here, since our focus is a different form of market efficiency. The interested reader can consult the comprehensive review in~\cite{Sewell2011}.}

We will consider a different sort of efficiency, where we think not about the price at which assets are bought and sold in our model market, but instead about the leverage that is applied to them. Let's run a thought experiment.

\begin{thoughtex}{efficiency under leverage}
Imagine that $\lopt>1$ in our model market. This would mean that the simple strategy of borrowing money to buy stock will achieve faster long-run growth than buying stock only with our own money. If we associate putting all our money in stock, $\l=1$, with an investment in the market, then it would be a trivial matter for us to beat the market (by doing nothing more sophisticated than investing borrowed money).

Similarly, imagine that $\lopt<1$. In this scenario, the market could again be beaten very easily by leaving some money in the bank (and, if $\lopt<0$, by short selling).
\end{thoughtex}

It would strain language to consider our market efficient if consistent outperformance were so straightforward to achieve. This suggests a different, fluctuations-based notion of market efficiency, which we call stochastic efficiency. We claim that it is impossible for a market participant without privileged information to beat a stochastically efficient market simply by choosing the \textit{amount} he invests in stock, \ie by choosing his leverage.\footnote{This resembles ordinary efficiency except that we have replaced price by amount.} Therefore, we make the following

\begin{keypts}{Hypothesis: stochastic market efficiency (strong form)}
Real markets self-organise such that
\be
\boxed{\; \lopt = 1 \;}
\elabel{sme_strong}
\ee
is an attractive point for their stochastic properties.
\end{keypts}

These stochastic properties are represented by $\mur$, $\mus$, and $\sigmas$ in the relaxed model, in which we permit dynamic adjustment of their values. We call this the strong form of the hypothesis, since it makes a very precise prediction about the attractive value for $\lopt$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Stability}
Another approach to the question in \secref{fundamental}, whose style owes more to physics than economics, is to consider the stability of the system under study (here, the market) and how this depends on the value of the measure in question (here, $\lopt$). Systems which are stable tend to persist over long time scales and are usually what we observe in nature. Unstable systems tend to last for shorter times,\footnote{We use the comparative ``shorter'' here. This does not mean short. It is quite possible for an unstable system to remain in flux for a long time in human terms, perhaps giving the illusion of stability. Indeed, much of classical economic theory is predicated on the idea that economies are at or close to equilibrium, \ie stable. We would argue that economies are fundamentally far-from-equilibrium systems and must be modelled as such, even if their dynamics unfold over time scales much longer than our day-to-day affairs.} so we observe them less frequently. With this in mind, let's think about the logic of different values of $\lopt$.

\begin{thoughtex}{stability under leverage}
Imagine that $\lopt>1$ in our relaxed model. Since it is an objectively optimal leverage which does not depend on investor idiosyncrasies, this means that \textit{everyone} in the market should want to borrow money to buy stock. But, if that's true, who's going to lend the money and who's going to sell the stock?

Similarly, imagine that $\lopt<0$. This means that \textit{everyone} should want to borrow stock and sell it for cash. But, if that's true, who's going to lend the stock and who's going to relinquish their cash to buy it?

Unless there are enough market participants disinterested in their time-average growth rate to take the unfavourable sides of these deals -- which in our model we will assume there aren't -- then neither of these situations is globally stable. It's hard to imagine them persisting for long before trading activity causes one or more of the market parameters to change, returning $\lopt$ to a stable value.
\end{thoughtex}

This thought experiment suggests that we will observe markets where $\lopt$ is within its stable range more often than it is not. This motivates the following
\begin{keypts}{Hypothesis: stochastic market efficiency (weak form)}
Real markets self-organise such that
\be
\boxed{\; 0\leq\lopt\leq1 \;}
\elabel{sme_weak}
\ee
is an attractive range for their stochastic properties.
\end{keypts}
We call this the weak form of the hypothesis, since its prediction about the attractive value for $\lopt$ is less precise than in \eref{sme_strong}.

The dynamical adjustment, or self-organisation, of the market parameters takes place through the trading activity of market participants. In particular, this creates feedback loops, which cause prices and fluctuation amplitudes to change, returning $\lopt$ to a stable value whenever it strays. To be truly convincing, we should propose plausible trading mechanisms through which these feedback loops arise. We do this in~\cite{PetersAdamou2013}. Since they involve details about how trading takes place in financial markets (in which we assume the typical attendee of these lectures is disinterested) we shall not rehearse them here. The primary drivers of our hypothesis are the efficiency and stability arguments we've just made.

Furthermore, there are additional reasons why we would favour the strong form of the hypothesis over long time scales. The main one is that an economy in which $\lopt$ is close to, or even less than, zero gives people no incentive to invest in productive business activity. Such an economy would appear paralysed, resembling perhaps those periods in history to which economists refer as depressions. We'd like to think that economies are not systematically attracted to such states. The other reasons are more technical, to do with the different interest rates accrued on deposits and loans, and the costs associated with buying and selling assets. These are described in~\cite{PetersAdamou2013} and lead to a refined
\begin{keypts}{Hypothesis: stochastic market efficiency (refined)}
On sufficiently long time scales, $\lopt=1$ is a strong attractor for the stochastic properties of real markets. Deviations from this attractor over shorter time scales are likely to be confined to the range $0\leq\lopt\leq1$.
\end{keypts}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Tests of the hypothesis\seclabel{tests}}
We test the stochastic efficiency hypothesis in a real market by simulating leveraged investments in the Standard \& Poor's index of 500 leading U.S. companies (the ``S\&P500'') using historical data of its daily returns over 58 years. This index is usually viewed as a proxy for an investment in large American businesses or, more simply, the American economy. We will treat the S\&P500 as the real-world equivalent of the risky asset in our model. Bank deposits at historical interest rates will be treated as the real-world equivalent of the riskless asset. Therefore, the dichotomy we set up is: invest generally in business; or put money in the bank.

\textbf{Data sets}\\
The data used in this study are publicly available from the Federal Reserve Economic Data (FRED) website.\footnote{\url{http://research.stlouisfed.org/fred2}} We use the daily closing prices of the S\&P500 (FRED time series ``SP500'') from $4^{\text{th}}$ August 1955 to $21^{\text{st}}$ May 2013. Additionally, we use two daily bank interest rates: the effective federal funds rate (``DFF''); and the bank prime loan rate (``DPRIME''). Optimal leverage is likely to be over-estimated from these data because the S\&P500 represents a well diversified portfolio of large and successful companies, which -- since ailing companies are replaced -- is positively affected by survivorship bias.

\textbf{Simulation}\\
An investment of constant leverage over a given time period, or window, is simulated as follows. At the start of the first day we assume an initial net investment, or equity, of $\$1$. This comprises stock holdings of $\$\l$ in the S\&P500 and bank deposits of $\$(1-\l)$. At the end of the day the values of these holdings and deposits are updated according to the historical market returns and interest rates. The portfolio is then rebalanced, \ie the holdings in the risky asset are adjusted so that their ratio to the equity remains $\l$. On non-trading days the return of the market is zero, while deposits continue to accrue interest.\footnote{This leads to an unrealistic but negligible rebalancing on those days.} The simulation proceeds in this fashion until the final day of the window, when the final equity is recorded. If at any time the equity falls below zero, the investment is declared bankrupt and the simulation stops.  The procedure is then repeated for different leverages, and the simulated optimal leverage is the leverage for which the final equity is maximised.

\textbf{Full time series}\\
\fref{g_l_linear} shows the simulated multiplicative return as a function of leverage for an investment over the entire time series.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{./chapter_4/figs/sme_fig1.pdf}
\caption{Total return for a constant-leverage investment in the S\&P500, starting $4^{\text{th}}$ August 1955 and ending $21^{\text{st}}$ May 2013 as a function of the leverage.
\newline \underline{Red line:} Simulation 1. Interest at federal funds rate on bank deposits and loans. No stock borrowing costs. No transaction costs.
\newline \underline{Yellow line:} Simulation 2. As 1, but with interest at federal funds rate on borrowed stock. This introduces a kink at $\l=0$, where stock borrowing begins.
\newline \underline{Green line:}\ Simulation 3. As 2, but with interest charged at the higher bank prime rate on borrowed cash and stock. This introduces a kink at $\l=1$.
\newline \underline{Blue line:}\ Simulation 4. As 3, but with a transaction cost of 0.2\% of the value of the assets traded on rebalancing.\flabel{g_l_linear}}
\end{figure}
The four curves in the figure correspond to four sets of assumptions about interest rates and transaction costs. These are labelled 1--4 in order of increasing complexity and resemblance to actual practices in financial markets. We will not dwell on these here, although brief descriptions are provided in the figure caption.

$\lopt=0.97$ for simulations 1 (which we will refer to as the simple case), 2, and 3. $\lopt=1.00$ for the most realistic simulation, 4 (the complex case). These results appear to lend great support to the stochastic market efficiency hypothesis. Based on simple thought experiments motivated by the theories developed in these lectures, we predicted that $\lopt$ in a real market should be close to unity. 58 years of real market data confirm this prediction. We discuss below the statistical significance of these results.

The kinks\footnote{A non-technical term for discontinuities in the derivative.} in the return-leverage curve can be accompanied by a change in sign of the derivative. When this happens, the kink is a global maximum and $\lopt$ is fixed at that leverage, either 0 or 1. This makes these special leverages sticky, in that $\lopt$ can get trapped there.\footnote{Although typically only when the $\lopt$ of simulation 1 is already in or very close to the range, $0\leq\lopt\leq1$.}. This stickiness will tend to promote the likelihood of the hypothesis being confirmed. However, simulation 1 shows that $\lopt\approx1$ even when these effects are neglected.

\textbf{Parameter estimation}\\
\fref{g_l_logarithmic} shows the simulated growth rate as a function of leverage for the simple case (simulation 1) over the entire time series.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{./chapter_4/figs/sme_fig2.pdf}
\caption{Computed time-average growth rates closely follow a parabola as a function of leverage.\flabel{g_l_logarithmic}}
\end{figure}
This is the logarithm of the simulated return of \fref{g_l_linear} divided by the window length. Since the window is long, we might expect to approximate the time-average growth rate, $\gtm(\l)$, which we know from \eref{g_l_quadratic} is a parabola in the original \GBM model. The black dashed line is a fitted parabola, whose parameters can be taken as meaningful definitions of the effective values of $\mur$, $\mue$, and $\sigmas$ for the S\&P500 for the entire time series.\footnote{We say effective because, although we think of these parameters as being time-varying in the relaxed model, their values from the parabolic fit are those of the original constant-parameter model which produce an almost identical outcome for $\gtm(\l)$ as the real data.} A least-squares fit estimates these parameters as $\mur=5.2\%$ \textit{pa}, $\mue=2.4\%$ \textit{pa}, and $\sigmas=16\%$ \textit{p$\sqrt{\mathit{a}}$}.

The deviation from parabolic form for high and low leverages in \fref{g_l_logarithmic} is due to extremely large fluctuations in the index, which are much less rare than would be observed in a true \GBM. These result in large losses and, indeed, bankruptcy for highly leveraged portfolios. That real returns distributions are observed to have fatter tails than those predicted by \GBM is an oft-made criticism of classical theory. In this study it is not especially relevant: indeed, the existence of rogue fluctuations strengthens the stochastic efficiency hypothesis, in that it penalises high leverage strategies.

\textbf{Equity premium puzzle}\\
These results are relevant to an unexplained phenomenon in economics known as the ``equity premium puzzle''~\cite{MehraPrescott1985}. Proponents of the puzzle\footnote{Which include another prize winner, Edward C.\ Prescott, this time of the 2004 Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel (which by then was on its tenth renaming).} argue that estimates of the equity premium, represented by $\mue$ in our model, are fundamentally incompatible with classical economic theory -- in particular utility theory -- because they imply an implausibly high level of investor risk aversion. A value of 6\% \textit{pa} has become established in the literature, which is markedly higher than the $\mue=2.4\%$ \textit{pa} in our parabolic fit. The source of this discrepancy is unclear because the classical estimates are based on complicated theories of investor behaviour. In our picture, which contains fewer assumptions to question, there is no puzzle: the value of $\mue$ we find is perfectly consistent with a stochastically efficient market.

\textbf{Finite time scales}\\
In the real world, of course, we can never truly observe the time-average growth rate of a leveraged investment since this would require an infinite observation time. Instead we observe the finite-time growth rate over a window of length $\Dt$. This is a random variable whose distribution broadens as $\Dt\to0$

Likewise, we never truly observe $\lopt$ either, since this is the leverage that maximises a time average-growth rate. Instead we observe simulated optimal leverages which maximise finite-time growth rates. We can guess that these will show larger fluctuations from the underlying $\lopt$ as the simulation window gets shorter, because short periods containing a sequence of almost all positive or negative stock price movements will result in very high (possibly infinite) positive or negative simulated optimal leverages.

More formally, let's denote by $\lopts(\Dt)$ the simulated optimal leverage which maximises the finite-time growth rate, $\gm(\xl,\Dt)$. As $\Dt\to0$, $\gm(\xl,\Dt)$ becomes a worse estimator of the time-average growth rate, $\gtm(\l)$, because its distribution becomes broader. Thus $\lopts(\Dt)$ becomes a similarly worse estimator of $\lopt$. 

This is important because a single observation of $\lopts(\Dt)$ consistent with our hypothesis (such as the one we just made for $\Dt\approx58$ years) is only significant if the uncertainty in $\lopts(\Dt)$ is of the same order of magnitude as $\lopts(\Dt)$ itself. For example, if we knew that $\lopts(\Dt)$ had a distribution that could place it between, say, -10 and 10 with reasonable probability, then we couldn't read much from a single observation either inside or outside the attractive range of our hypothesis. If, on the other hand, most of the distribution's mass were inside the range, then an observation outside would be strong evidence that the hypothesis is flawed, and so an observation inside is significant.

We can quantify these ideas in the original model by discarding the $\Dt\to\infty$ limit in \eref{g_l_noisy}. This gives
\be
\gm(\xl,\Dt) = \mur+\l\mue-\frac{\l^2\sigmas^2}{2}+\frac{\l\sigmas \gW(\Dt)}{\Dt},
\elabel{g_l_finite}
\ee
which is maximised at
\be
\lopts(\Dt) = \lopt + \frac{\gW(\Dt)}{\sigmas\Dt}.
\elabel{lopt_finite}
\ee
This is normally distributed with mean $\lopt$ and standard deviation
\be
\text{stdev}[\lopts(\Dt)] = \frac{1}{\sigmas\sqrt{\Dt}}.
\elabel{lopt_sd}
\ee
Using the computed volatility of $16\%$ per square root of one year, this standard deviation is approximately 0.83. This means that the uncertainty in $\lopts(\Dt)$ is about the same size as the hypothesised value of $\lopt$, and so the single observation we made is a significant corroboration of the hypothesis.

We can also test the validity of the relationship in \eref{lopt_finite} by simulating investments of different window lengths in the market data and compiling histograms of the resulting $\lopts(\Dt)$. \fref{loglog} shows, on double-logarithmic scales, the standard deviation of $\lopts(\Dt)$ as a function of $\Dt$ for the simple simulation.
\begin{figure}
\includegraphics[width=\textwidth]{./chapter_4/figs/sme_fig3.pdf}
\caption{The red dots show the standard deviation of samples of $\lopts(\Dt)$ as a function of window length for the simple simulation. The black line shows the prediction of \eref{lopt_finite} using the estimate of $\sigmas$ from the parabolic fit in \fref{g_l_logarithmic}.\flabel{loglog}}
\end{figure}
Good agreement is found with the model-specific prediction.\footnote{For shorter time scales, the standard deviation is slightly higher than predicted. We suspect this is due to discretisation effects: for investments over a small number of days, very high optimal leverages can be observed when the window contains predominantly positive or negative index movements. These would no longer be optimal if fluctuations and rebalancing were simulated on smaller time scales, \ie intra-day. For longer time scales, the standard deviation drops below the prediction. This is because for window lengths approaching the entire period under study, the number of independent windows in the sample is small, and so the standard deviation of the sample is depressed.}

\fref{l_opt_expanding} shows the simulated $\lopts(\Dt)$ for the investment window starting on $4^\text{th}$ August 1955 and ending on the date on the horizontal axis.
\begin{figure}
\includegraphics[width=\textwidth]{./chapter_4/figs/sme_fig4.pdf}
\caption{Daily simulated optimal leverages for an expanding window with start date $4^\text{th}$ August 1955 and end date on the horizontal axis. Both simple (red line) and complex (blue line) simulations are shown. The broken magenta lines show the one- and two-standard deviation envelopes about $\lopt=1$, based on \eref{lopt_sd} and the estimate of $\sigmas$ from the parabolic fit in \fref{g_l_logarithmic}.\flabel{l_opt_expanding}}
\end{figure}
The reduction in fluctuations with increasing window length are broadly consistent with \eref{lopt_sd} and support the hypothesis that $\lopt$ -- as estimated by $\lopts(\Dt)$ -- is attracted to the range $0\leq\lopt\leq1$ and, over long time scales, to $\lopt=1$ in particular.

We can also plot the simulated optimal leverages for investment windows with fixed lengths and moving start date. We do this for the simple and complex simulations in \fref{l_opt_expanding}, with window lengths ranging from 5 to 40 years.
\begin{figure}
\includegraphics[width=1.\textwidth]{./chapter_4/figs/sme_fig5a.pdf}
\includegraphics[width=1.\textwidth]{./chapter_4/figs/sme_fig5b.pdf}
\caption{\newline
(a) In the simple simulation, observed optimal leverage fluctuates strongly on short time scales but appears to converge to $\lopts(\Dt)=1$ on long time scales. This constitutes the central result of the study.
\newline
(b) In the complex simulation, the kinks in \fref{g_l_linear} ensure that $\lopts(\Dt)=0,1$ are often found exactly. The 40-year simulation supports the strong stochastic efficiency hypothesis, that real markets are attracted to $\lopt=1$, with a dip to $\lopt=0$ only during the financial crisis of 2008. \flabel{l_opt_fixed}}
\end{figure}
From the strong fluctuations over short time scales emerges attractive behaviour consistent with our refined hypothesis. The effects of the stickiness of the points $\lopt=0$ and $\lopt=1$ in the complex model are clearly visible. In particular, over the last decade or so optimal leverage for the 20- and 40-year windows remained close to unity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}
The primary aim of this final lecture was to demonstrate that the mathematical formalism we have developed to conceptualise randomness in economics, which started with a simple model of wealth evolution by repetition of a gamble, is very powerful. It does more than simply create a rigorous and plausible foundation for economic theory. In particular, because the framework is epistemologically sound, we can make testable predictions -- such as the stochastic market efficiency hypothesis -- which we can corroborate empirically using real economic data. We could not have guessed from our simple coin-tossing game that we would end up making a prediction about the fluctuations of the American stock market over more than half a century. What other surprising predictions can we make in this formalism? That, dear reader, is a question for you.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
